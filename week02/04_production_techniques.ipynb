{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week02 - Production Prompt Engineering Techniques\n",
    "\n",
    "이 노트북은 실제 프로덕션 환경에서 사용되는 고급 Prompt Engineering 기법들을 다룹니다.\n",
    "\n",
    "## 다루는 기법들:\n",
    "1. **감성 분석 후 분기 (Sentiment Branching)**: 입력의 감정을 분석하여 적절한 응답 전략 선택\n",
    "2. **금칙어/규칙 필터링 (Content Filtering)**: 안전하지 않은 콘텐츠 필터링 및 규칙 적용\n",
    "3. **실패 시 재시도/백오프 (Retry/Backoff)**: 안정적인 API 호출을 위한 재시도 메커니즘\n",
    "\n",
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import hashlib\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI 라이브러리 설치 및 import\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except ImportError:\n",
    "    print(\"OpenAI 라이브러리를 설치합니다...\")\n",
    "    subprocess.run([\"pip\", \"install\", \"openai\"], check=True)\n",
    "    from openai import OpenAI\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# OpenAI 클라이언트 초기화 (API 키는 환경변수에서 읽음)\n",
    "try:\n",
    "    client = OpenAI()  # OPENAI_API_KEY 환경변수 필요\n",
    "    openai_available = True\n",
    "except Exception as e:\n",
    "    print(f\"OpenAI 클라이언트 초기화 실패: {e}\")\n",
    "    print(\"환경변수 OPENAI_API_KEY를 설정해주세요.\")\n",
    "    openai_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 공통 유틸리티 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 유틸리티 함수 로드 완료\n"
     ]
    }
   ],
   "source": [
    "def run_ollama(model: str, prompt: str, max_retries: int = 3) -> str:\n",
    "    \"\"\"Ollama 모델 실행 (재시도 기능 포함)\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"ollama\", \"run\", model],\n",
    "                input=prompt,\n",
    "                text=True,\n",
    "                capture_output=True,\n",
    "                timeout=60\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                return result.stdout.strip()\n",
    "            else:\n",
    "                logger.warning(f\"Ollama 실행 실패 (시도 {attempt + 1}/{max_retries}): {result.stderr}\")\n",
    "        except subprocess.TimeoutExpired:\n",
    "            logger.warning(f\"Ollama 타임아웃 (시도 {attempt + 1}/{max_retries})\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ollama 실행 오류 (시도 {attempt + 1}/{max_retries}): {e}\")\n",
    "        \n",
    "        if attempt < max_retries - 1:\n",
    "            time.sleep(2 ** attempt)  # 지수 백오프\n",
    "    \n",
    "    return \"Error: Maximum retries exceeded\"\n",
    "\n",
    "def run_openai(prompt: str, model: str = \"gpt-4o-mini\", max_retries: int = 3) -> str:\n",
    "    \"\"\"OpenAI 모델 실행 (재시도 기능 포함)\"\"\"\n",
    "    if not openai_available:\n",
    "        return \"Error: OpenAI client not available\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"OpenAI API 호출 실패 (시도 {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # 지수 백오프\n",
    "    \n",
    "    return \"Error: Maximum retries exceeded\"\n",
    "\n",
    "def safe_json_parse(text: str) -> dict:\n",
    "    \"\"\"안전한 JSON 파싱\"\"\"\n",
    "    try:\n",
    "        # JSON 블록 추출\n",
    "        json_match = re.search(r'```json\\s*(.*?)\\s*```', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_text = json_match.group(1)\n",
    "        else:\n",
    "            # 중괄호 블록 찾기\n",
    "            brace_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "            if brace_match:\n",
    "                json_text = brace_match.group(0)\n",
    "            else:\n",
    "                json_text = text\n",
    "        \n",
    "        return json.loads(json_text)\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"JSON 파싱 실패: {text}\")\n",
    "        return {\"error\": \"JSON parsing failed\", \"raw_text\": text}\n",
    "\n",
    "print(\"✅ 유틸리티 함수 로드 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 감성 분석 후 분기 (Sentiment Branching)\n",
    "\n",
    "사용자의 입력을 감정적으로 분석한 후, 그에 맞는 적절한 응답 전략을 선택하는 기법입니다.\n",
    "\n",
    "## 핵심 개념:\n",
    "- 감정 상태 감지 (긍정, 부정, 중립, 분노, 슬픔 등)\n",
    "- 감정별 맞춤 응답 전략\n",
    "- 동적 프롬프트 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 감성 분석 후 분기 시스템 준비 완료\n"
     ]
    }
   ],
   "source": [
    "class SentimentType(Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEGATIVE = \"negative\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    ANGRY = \"angry\"\n",
    "    SAD = \"sad\"\n",
    "    EXCITED = \"excited\"\n",
    "    CONFUSED = \"confused\"\n",
    "\n",
    "@dataclass\n",
    "class SentimentAnalysisResult:\n",
    "    sentiment: SentimentType\n",
    "    confidence: float\n",
    "    emotions: Dict[str, float]\n",
    "    keywords: List[str]\n",
    "\n",
    "class SentimentBranchingSystem:\n",
    "    def __init__(self):\n",
    "        self.response_strategies = {\n",
    "            SentimentType.POSITIVE: {\n",
    "                \"tone\": \"enthusiastic and supportive\",\n",
    "                \"approach\": \"build on the positive energy\",\n",
    "                \"style\": \"encouraging and collaborative\"\n",
    "            },\n",
    "            SentimentType.NEGATIVE: {\n",
    "                \"tone\": \"empathetic and understanding\",\n",
    "                \"approach\": \"acknowledge concerns and provide solutions\",\n",
    "                \"style\": \"supportive and problem-solving focused\"\n",
    "            },\n",
    "            SentimentType.ANGRY: {\n",
    "                \"tone\": \"calm and professional\",\n",
    "                \"approach\": \"de-escalate and address root issues\",\n",
    "                \"style\": \"patient and solution-oriented\"\n",
    "            },\n",
    "            SentimentType.SAD: {\n",
    "                \"tone\": \"gentle and compassionate\",\n",
    "                \"approach\": \"provide comfort and hope\",\n",
    "                \"style\": \"nurturing and uplifting\"\n",
    "            },\n",
    "            SentimentType.CONFUSED: {\n",
    "                \"tone\": \"clear and patient\",\n",
    "                \"approach\": \"break down complexity into simple steps\",\n",
    "                \"style\": \"educational and structured\"\n",
    "            },\n",
    "            SentimentType.EXCITED: {\n",
    "                \"tone\": \"matching enthusiasm\",\n",
    "                \"approach\": \"channel energy productively\",\n",
    "                \"style\": \"dynamic and engaging\"\n",
    "            },\n",
    "            SentimentType.NEUTRAL: {\n",
    "                \"tone\": \"professional and informative\",\n",
    "                \"approach\": \"provide comprehensive information\",\n",
    "                \"style\": \"balanced and thorough\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_sentiment_ollama(self, text: str) -> SentimentAnalysisResult:\n",
    "        \"\"\"Ollama를 사용한 감성 분석\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "사용자의 다음 텍스트를 분석하여 감정 상태를 파악해주세요.\n",
    "\n",
    "텍스트: \"{text}\"\n",
    "\n",
    "다음 JSON 형식으로 응답해주세요:\n",
    "```json\n",
    "{{\n",
    "    \"sentiment\": \"positive|negative|neutral|angry|sad|excited|confused\",\n",
    "    \"confidence\": 0.95,\n",
    "    \"emotions\": {{\n",
    "        \"joy\": 0.8,\n",
    "        \"anger\": 0.1,\n",
    "        \"sadness\": 0.1,\n",
    "        \"fear\": 0.0,\n",
    "        \"surprise\": 0.0\n",
    "    }},\n",
    "    \"keywords\": [\"감정을\", \"나타내는\", \"키워드들\"]\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        response = run_ollama(\"llama3.1:8b\", prompt)\n",
    "        result_data = safe_json_parse(response)\n",
    "        \n",
    "        if \"error\" in result_data:\n",
    "            # 기본값 반환\n",
    "            return SentimentAnalysisResult(\n",
    "                sentiment=SentimentType.NEUTRAL,\n",
    "                confidence=0.5,\n",
    "                emotions={\"neutral\": 1.0},\n",
    "                keywords=[]\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            sentiment = SentimentType(result_data.get(\"sentiment\", \"neutral\"))\n",
    "        except ValueError:\n",
    "            sentiment = SentimentType.NEUTRAL\n",
    "        \n",
    "        return SentimentAnalysisResult(\n",
    "            sentiment=sentiment,\n",
    "            confidence=result_data.get(\"confidence\", 0.5),\n",
    "            emotions=result_data.get(\"emotions\", {}),\n",
    "            keywords=result_data.get(\"keywords\", [])\n",
    "        )\n",
    "    \n",
    "    def analyze_sentiment_openai(self, text: str) -> SentimentAnalysisResult:\n",
    "        \"\"\"OpenAI를 사용한 감성 분석\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Analyze the emotional state of the following user text and provide a detailed sentiment analysis.\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "Return your analysis in the following JSON format:\n",
    "```json\n",
    "{{\n",
    "    \"sentiment\": \"positive|negative|neutral|angry|sad|excited|confused\",\n",
    "    \"confidence\": 0.95,\n",
    "    \"emotions\": {{\n",
    "        \"joy\": 0.8,\n",
    "        \"anger\": 0.1,\n",
    "        \"sadness\": 0.1,\n",
    "        \"fear\": 0.0,\n",
    "        \"surprise\": 0.0\n",
    "    }},\n",
    "    \"keywords\": [\"emotional\", \"keywords\", \"detected\"]\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        response = run_openai(prompt)\n",
    "        result_data = safe_json_parse(response)\n",
    "        \n",
    "        if \"error\" in result_data:\n",
    "            return SentimentAnalysisResult(\n",
    "                sentiment=SentimentType.NEUTRAL,\n",
    "                confidence=0.5,\n",
    "                emotions={\"neutral\": 1.0},\n",
    "                keywords=[]\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            sentiment = SentimentType(result_data.get(\"sentiment\", \"neutral\"))\n",
    "        except ValueError:\n",
    "            sentiment = SentimentType.NEUTRAL\n",
    "        \n",
    "        return SentimentAnalysisResult(\n",
    "            sentiment=sentiment,\n",
    "            confidence=result_data.get(\"confidence\", 0.5),\n",
    "            emotions=result_data.get(\"emotions\", {}),\n",
    "            keywords=result_data.get(\"keywords\", [])\n",
    "        )\n",
    "    \n",
    "    def generate_response_ollama(self, user_input: str, sentiment_result: SentimentAnalysisResult) -> str:\n",
    "        \"\"\"감정에 맞는 응답 생성 (Ollama)\"\"\"\n",
    "        strategy = self.response_strategies[sentiment_result.sentiment]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "사용자의 입력에 대해 감정 분석 결과를 바탕으로 적절한 응답을 생성해주세요.\n",
    "\n",
    "사용자 입력: \"{user_input}\"\n",
    "\n",
    "감정 분석 결과:\n",
    "- 주요 감정: {sentiment_result.sentiment.value}\n",
    "- 신뢰도: {sentiment_result.confidence:.2f}\n",
    "- 감정 키워드: {', '.join(sentiment_result.keywords)}\n",
    "\n",
    "응답 전략:\n",
    "- 톤: {strategy['tone']}\n",
    "- 접근법: {strategy['approach']}\n",
    "- 스타일: {strategy['style']}\n",
    "\n",
    "위 전략을 바탕으로 공감적이고 도움이 되는 응답을 생성해주세요.\n",
    "\"\"\"\n",
    "        \n",
    "        return run_ollama(\"llama3.1:8b\", prompt)\n",
    "    \n",
    "    def generate_response_openai(self, user_input: str, sentiment_result: SentimentAnalysisResult) -> str:\n",
    "        \"\"\"감정에 맞는 응답 생성 (OpenAI)\"\"\"\n",
    "        strategy = self.response_strategies[sentiment_result.sentiment]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "Generate an appropriate response to the user's input based on the sentiment analysis results.\n",
    "\n",
    "User Input: \"{user_input}\"\n",
    "\n",
    "Sentiment Analysis:\n",
    "- Primary Sentiment: {sentiment_result.sentiment.value}\n",
    "- Confidence: {sentiment_result.confidence:.2f}\n",
    "- Keywords: {', '.join(sentiment_result.keywords)}\n",
    "\n",
    "Response Strategy:\n",
    "- Tone: {strategy['tone']}\n",
    "- Approach: {strategy['approach']}\n",
    "- Style: {strategy['style']}\n",
    "\n",
    "Create an empathetic and helpful response following the above strategy.\n",
    "\"\"\"\n",
    "        \n",
    "        return run_openai(prompt)\n",
    "    \n",
    "    def process_with_sentiment_branching(self, user_input: str, use_openai: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"감성 분석 후 분기 처리 전체 과정\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 1. 감성 분석\n",
    "        if use_openai:\n",
    "            sentiment_result = self.analyze_sentiment_openai(user_input)\n",
    "            response = self.generate_response_openai(user_input, sentiment_result)\n",
    "            model_used = \"gpt-4o-mini\"\n",
    "        else:\n",
    "            sentiment_result = self.analyze_sentiment_ollama(user_input)\n",
    "            response = self.generate_response_ollama(user_input, sentiment_result)\n",
    "            model_used = \"llama3.1:8b\"\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"user_input\": user_input,\n",
    "            \"sentiment_analysis\": {\n",
    "                \"sentiment\": sentiment_result.sentiment.value,\n",
    "                \"confidence\": sentiment_result.confidence,\n",
    "                \"emotions\": sentiment_result.emotions,\n",
    "                \"keywords\": sentiment_result.keywords\n",
    "            },\n",
    "            \"response_strategy\": self.response_strategies[sentiment_result.sentiment],\n",
    "            \"generated_response\": response,\n",
    "            \"model_used\": model_used,\n",
    "            \"processing_time\": processing_time\n",
    "        }\n",
    "\n",
    "# 시스템 초기화\n",
    "sentiment_system = SentimentBranchingSystem()\n",
    "print(\"✅ 감성 분석 후 분기 시스템 준비 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 감성 분석 후 분기 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "감성 분석 후 분기 시스템 테스트\n",
      "================================================================================\n",
      "\n",
      "[테스트 케이스 1]\n",
      "입력: 오늘 정말 기분이 좋아요! 새로운 프로젝트가 성공적으로 마무리되었거든요.\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 결과:\n",
      "감정: excited (신뢰도: 1.00)\n",
      "키워드: 성공, 기분\n",
      "응답: \"성공적인 프로젝트 끝내신 거 축하해요! 새로운 도전과 이겨내는 모습 정말 멋져요. 기분이 좋으시면 다음에도 더 좋은 성과 나오실 거예요!\"\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정: positive (신뢰도: 0.95)\n",
      "키워드: 기분이 좋아요, 새로운 프로젝트, 성공적으로 마무리되었거든요\n",
      "응답: 와, 정말 기분이 좋으시다니 기쁩니다! 🎉 새로운 프로젝트가 성공적으로 마무리되었다니 정말 대단해요! 어떤 점이 가장 뿌듯하셨나요? 함께 이야기를 나누면서 더 많은 영감을 얻어보아요! 앞으로의 계획도 궁금하네요!\n",
      "\n",
      "⏱️ 처리 시간: Ollama 10.39초, OpenAI 5.88초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 2]\n",
      "입력: 이 서비스 정말 최악이에요. 계속 오류가 나고 고객센터는 응답도 안 해요!\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 결과:\n",
      "감정: negative (신뢰도: 0.95)\n",
      "키워드: 오류, 고객센터\n",
      "응답: 사용자의 입력에 대한 답변:\n",
      "\n",
      "\"죄송합니다. 정말 이해가 가요. 오류와 고객센터의 문제는 우리 서비스를 사용하는 데 있어 큰 불편이 될 수 있습니다. \n",
      "\n",
      "우리는 오류 해결에 더욱 힘써서 더 안정적인 서비스 제공을 위해 노력하고 있습니다. 또한 고객 센터의 응답 시간을 개선하여 더 신속하게 도움을 드리기 위한 계획을 마련 중입니다.\n",
      "\n",
      "감사합니다. 여러분의 의...\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정: negative (신뢰도: 0.95)\n",
      "키워드: 서비스, 최악, 오류, 고객센터, 응답\n",
      "응답: 안녕하세요. 불편을 드려 정말 죄송합니다. 서비스에 오류가 발생하고 고객센터의 응답이 지연된 점에 대해 깊이 이해합니다. 이러한 경험은 매우 실망스럽고 스트레스를 줄 수 있습니다. \n",
      "\n",
      "문제를 해결하기 위해 최선을 다하고 있으며, 현재의 상황을 개선할 수 있도록 노력하고 있습니다. 고객센터에 다시 연락해 보시거나, 추가적인 도움이 필요하시면 저희에게 말씀해 ...\n",
      "\n",
      "⏱️ 처리 시간: Ollama 10.21초, OpenAI 5.06초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 3]\n",
      "입력: 요즘 너무 힘들어요... 일도 잘 안되고 스트레스만 쌓여가네요.\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 결과:\n",
      "감정: sad (신뢰도: 0.95)\n",
      "키워드: 힘들어요, 스트레스만\n",
      "응답: 저는 사용자의 감정 상태를 우려합니다. 힘들고 스트레스를 받으시는데요.\n",
      "\n",
      "하루가 갈수록 점점 더 힘든 것이 느껴지는 것 같아요. 그런데 여러분이 지금 가지고 계신 문제를 해결할 수 있는 방법은 많습니다. 먼저, 스트레스를 관리하는 법을 알려드릴게요. 활동량을 늘리면 스트레스가 적어지는데요. 하루에 짧은 시간만이라도 밖에 나가서 산책을 하거나 일부러 운동을 ...\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정: negative (신뢰도: 0.95)\n",
      "키워드: 힘들다, 스트레스, 안되다\n",
      "응답: 이런 기분이 드는 건 정말 힘든 일이에요. 요즘 일이 잘 안 되고 스트레스가 쌓여가는 것에 대해 정말 공감합니다. 이런 상황에서는 자신을 돌보는 것이 중요해요. 혹시 스트레스를 해소할 수 있는 방법이나 취미가 있을까요? 잠깐의 휴식이나 산책도 큰 도움이 될 수 있어요. 필요하다면 누군가와 이야기하는 것도 좋고요. 당신은 혼자가 아니니, 언제든지 지원을 받을...\n",
      "\n",
      "⏱️ 처리 시간: Ollama 17.83초, OpenAI 3.64초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 4]\n",
      "입력: 파이썬 코딩을 처음 배우는데 너무 어려워서 이해가 안 가요.\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 결과:\n",
      "감정: negative (신뢰도: 0.90)\n",
      "키워드: 파이썬, 코딩\n",
      "응답: 네, 파이썬 코딩을 처음 시작하는 것에 대한 어려움을 이해합니다. 대부분의 프로그래밍 언어는 처음 접할 때 쉽지 않다는 점은 인정합니다.\n",
      "\n",
      "한 가지 팁을 드릴게요. 파이썬도 그렇듯이 다른 프로그래밍 언어도 같은 원리와 개념을 따릅니다. 따라서, 어떤 언어든 처음 배우면 어려울 수 있습니다. \n",
      "\n",
      "해결책으로 다음과 같이 접근할 수 있습니다.\n",
      "\n",
      "1.  **간단한...\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정: confused (신뢰도: 0.92)\n",
      "키워드: 파이썬, 코딩, 어려워서, 이해가 안 가요\n",
      "응답: 안녕하세요! 파이썬 코딩을 처음 배우는 것은 정말 도전적일 수 있습니다. 이해가 안 가는 부분이 많을 때는 누구나 힘들어 하죠. 제가 몇 가지 간단한 단계를 제안해 드릴게요.\n",
      "\n",
      "1. **기본 개념 이해하기**: 파이썬의 기본 문법과 개념부터 차근차근 익혀보세요. 예를 들어, 변수를 선언하는 방법이나, 간단한 데이터 타입 (문자열, 숫자 등)에 대해 학습하는...\n",
      "\n",
      "⏱️ 처리 시간: Ollama 16.66초, OpenAI 7.27초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 5]\n",
      "입력: 와! 내일 드디어 여행을 떠나요! 정말 기대돼요!\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 결과:\n",
      "감정: excited (신뢰도: 1.00)\n",
      "키워드:  여행, 기대돼요\n",
      "응답: \"와! 여행이 정말 재미있게 다녀올 거예요! 다음에 여행 계획을 하실 때는 무슨 도시로 가시고 어떤 것들을 하고 싶으세요?\"\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정: excited (신뢰도: 0.95)\n",
      "키워드: 여행, 기대돼요, 드디어\n",
      "응답: 와! 정말 멋진 소식이에요! 🎉 여행을 떠나는 건 언제나 기대되는 일이죠! 어떤 장소로 가는지, 어떤 계획이 있는지 궁금해요! 여행 준비는 잘 되고 있나요? 멋진 경험을 가득 담고 돌아오길 바랄게요! ✈️🌍\n",
      "\n",
      "⏱️ 처리 시간: Ollama 7.62초, OpenAI 4.18초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 6]\n",
      "입력: 머신러닝에 대해서 설명해주세요.\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 결과:\n",
      "감정: neutral (신뢰도: 0.95)\n",
      "키워드: 머신러닝\n",
      "응답: 머신러닝에 대한 설명이 필요한 것 같은데요. 머신러닝은 데이터를 학습시키고, 그 결과를 바탕으로 예측이나 분류를 하는 기계학습 알고리즘입니다.\n",
      "\n",
      "머신러닝에는 여러 가지 종류가 있습니다. 대표적인 것을 나열해 보겠습니다.\n",
      "\n",
      "1.  supervised learning: 정답이 있는 데이터에 대해 모델을 훈련시켜서, 미래의 새로운 데이터에서 정확한 예측을 하기 ...\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정: neutral (신뢰도: 0.85)\n",
      "키워드: machine learning, explain, request\n",
      "응답: 물론입니다! 머신러닝은 인공지능(AI)의 한 분야로, 컴퓨터가 주어진 데이터에서 패턴을 학습하고, 이를 바탕으로 예측이나 결정을 내릴 수 있도록 하는 기술입니다. 머신러닝의 주요 목표는 명시적으로 프로그래밍하지 않고도 시스템이 성능을 개선할 수 있도록 하는 것입니다.\n",
      "\n",
      "머신러닝은 크게 세 가지 유형으로 나눌 수 있습니다:\n",
      "\n",
      "1. **지도 학습(Supervi...\n",
      "\n",
      "⏱️ 처리 시간: Ollama 28.23초, OpenAI 10.00초\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ 감성 분석 후 분기 테스트 완료\n"
     ]
    }
   ],
   "source": [
    "# 다양한 감정의 테스트 케이스들\n",
    "test_cases = [\n",
    "    \"오늘 정말 기분이 좋아요! 새로운 프로젝트가 성공적으로 마무리되었거든요.\",\n",
    "    \"이 서비스 정말 최악이에요. 계속 오류가 나고 고객센터는 응답도 안 해요!\",\n",
    "    \"요즘 너무 힘들어요... 일도 잘 안되고 스트레스만 쌓여가네요.\",\n",
    "    \"파이썬 코딩을 처음 배우는데 너무 어려워서 이해가 안 가요.\",\n",
    "    \"와! 내일 드디어 여행을 떠나요! 정말 기대돼요!\",\n",
    "    \"머신러닝에 대해서 설명해주세요.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"감성 분석 후 분기 시스템 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\n[테스트 케이스 {i}]\")\n",
    "    print(f\"입력: {test_case}\")\n",
    "    \n",
    "    # Ollama 결과\n",
    "    print(\"\\n🤖 Ollama (llama3.1:8b) 결과:\")\n",
    "    ollama_result = sentiment_system.process_with_sentiment_branching(test_case, use_openai=False)\n",
    "    print(f\"감정: {ollama_result['sentiment_analysis']['sentiment']} (신뢰도: {ollama_result['sentiment_analysis']['confidence']:.2f})\")\n",
    "    print(f\"키워드: {', '.join(ollama_result['sentiment_analysis']['keywords'])}\")\n",
    "    print(f\"응답: {ollama_result['generated_response'][:200]}{'...' if len(ollama_result['generated_response']) > 200 else ''}\")\n",
    "    \n",
    "    # OpenAI 결과 (사용 가능한 경우)\n",
    "    if openai_available:\n",
    "        print(\"\\n🧠 OpenAI (gpt-4o-mini) 결과:\")\n",
    "        openai_result = sentiment_system.process_with_sentiment_branching(test_case, use_openai=True)\n",
    "        print(f\"감정: {openai_result['sentiment_analysis']['sentiment']} (신뢰도: {openai_result['sentiment_analysis']['confidence']:.2f})\")\n",
    "        print(f\"키워드: {', '.join(openai_result['sentiment_analysis']['keywords'])}\")\n",
    "        print(f\"응답: {openai_result['generated_response'][:200]}{'...' if len(openai_result['generated_response']) > 200 else ''}\")\n",
    "        \n",
    "        # 성능 비교\n",
    "        print(f\"\\n⏱️ 처리 시간: Ollama {ollama_result['processing_time']:.2f}초, OpenAI {openai_result['processing_time']:.2f}초\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ OpenAI 클라이언트를 사용할 수 없습니다.\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n✅ 감성 분석 후 분기 테스트 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 금칙어/규칙 필터링 (Content Filtering)\n",
    "\n",
    "안전하지 않은 콘텐츠나 부적절한 내용을 필터링하고, 특정 규칙을 적용하여 응답을 제어하는 기법입니다.\n",
    "\n",
    "## 핵심 개념:\n",
    "- 금칙어 및 부적절한 콘텐츠 감지\n",
    "- 규칙 기반 필터링\n",
    "- 안전한 대체 응답 생성\n",
    "- 다단계 검증 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 콘텐츠 필터링 시스템 준비 완료\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from typing import Set, List, Dict, Any, Optional\n",
    "import re\n",
    "\n",
    "class FilterSeverity(Enum):\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "class FilterCategory(Enum):\n",
    "    PROFANITY = \"profanity\"\n",
    "    HATE_SPEECH = \"hate_speech\"\n",
    "    VIOLENCE = \"violence\"\n",
    "    ILLEGAL = \"illegal\"\n",
    "    ADULT_CONTENT = \"adult_content\"\n",
    "    PERSONAL_INFO = \"personal_info\"\n",
    "    SPAM = \"spam\"\n",
    "    MISINFORMATION = \"misinformation\"\n",
    "\n",
    "@dataclass\n",
    "class FilterResult:\n",
    "    is_safe: bool\n",
    "    severity: FilterSeverity\n",
    "    categories: List[FilterCategory]\n",
    "    detected_terms: List[str]\n",
    "    confidence: float\n",
    "    reasoning: str\n",
    "    alternative_response: Optional[str] = None\n",
    "\n",
    "class ContentFilterSystem:\n",
    "    def __init__(self):\n",
    "        # 기본 금칙어 사전 (실제 운영에서는 더 포괄적인 DB 사용)\n",
    "        self.forbidden_words = {\n",
    "            FilterCategory.PROFANITY: {\n",
    "                \"씨발\", \"개새끼\", \"병신\", \"멍청이\", \"바보\", \"fuck\", \"shit\", \"damn\"\n",
    "            },\n",
    "            FilterCategory.HATE_SPEECH: {\n",
    "                \"죽어라\", \"사라져\", \"혐오\", \"차별\", \"hate\", \"discrimination\"\n",
    "            },\n",
    "            FilterCategory.VIOLENCE: {\n",
    "                \"때려\", \"죽여\", \"폭력\", \"violence\", \"kill\", \"murder\", \"assault\"\n",
    "            },\n",
    "            FilterCategory.ILLEGAL: {\n",
    "                \"마약\", \"폭탄\", \"테러\", \"해킹\", \"drugs\", \"bomb\", \"terrorism\", \"hacking\"\n",
    "            },\n",
    "            FilterCategory.ADULT_CONTENT: {\n",
    "                \"성관계\", \"포르노\", \"섹스\", \"porn\", \"sex\", \"adult\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 개인정보 패턴\n",
    "        self.personal_info_patterns = {\n",
    "            \"phone\": r'\\d{3}-\\d{4}-\\d{4}|\\d{11}',\n",
    "            \"email\": r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}',\n",
    "            \"credit_card\": r'\\d{4}-\\d{4}-\\d{4}-\\d{4}|\\d{16}',\n",
    "            \"ssn\": r'\\d{6}-\\d{7}'\n",
    "        }\n",
    "        \n",
    "        # 대체 응답 템플릿\n",
    "        self.alternative_responses = {\n",
    "            FilterSeverity.LOW: \"죄송합니다. 좀 더 적절한 표현으로 다시 말씀해 주시겠어요?\",\n",
    "            FilterSeverity.MEDIUM: \"부적절한 내용이 포함되어 있습니다. 다른 방식으로 질문해 주세요.\",\n",
    "            FilterSeverity.HIGH: \"안전 정책 위반으로 인해 응답할 수 없습니다. 건전한 내용으로 다시 질문해 주세요.\",\n",
    "            FilterSeverity.CRITICAL: \"심각한 정책 위반이 감지되었습니다. 이런 내용에 대해서는 도움을 드릴 수 없습니다.\"\n",
    "        }\n",
    "    \n",
    "    def check_forbidden_words(self, text: str) -> Tuple[List[FilterCategory], List[str]]:\n",
    "        \"\"\"금칙어 검사\"\"\"\n",
    "        detected_categories = []\n",
    "        detected_terms = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for category, words in self.forbidden_words.items():\n",
    "            for word in words:\n",
    "                if word.lower() in text_lower:\n",
    "                    if category not in detected_categories:\n",
    "                        detected_categories.append(category)\n",
    "                    detected_terms.append(word)\n",
    "        \n",
    "        return detected_categories, detected_terms\n",
    "    \n",
    "    def check_personal_info(self, text: str) -> Tuple[bool, List[str]]:\n",
    "        \"\"\"개인정보 패턴 검사\"\"\"\n",
    "        detected_patterns = []\n",
    "        \n",
    "        for pattern_name, pattern in self.personal_info_patterns.items():\n",
    "            if re.search(pattern, text):\n",
    "                detected_patterns.append(pattern_name)\n",
    "        \n",
    "        return len(detected_patterns) > 0, detected_patterns\n",
    "    \n",
    "    def ai_content_analysis_ollama(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"AI 기반 콘텐츠 분석 (Ollama)\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "다음 텍스트를 분석하여 부적절한 내용이 포함되어 있는지 판단해주세요.\n",
    "\n",
    "텍스트: \"{text}\"\n",
    "\n",
    "분석 기준:\n",
    "1. 욕설이나 비속어\n",
    "2. 폭력적인 내용\n",
    "3. 불법적인 내용\n",
    "4. 혐오 발언\n",
    "5. 성인 콘텐츠\n",
    "6. 개인정보 노출\n",
    "7. 스팸성 내용\n",
    "\n",
    "다음 JSON 형식으로 응답해주세요:\n",
    "```json\n",
    "{{\n",
    "    \"is_safe\": true,\n",
    "    \"severity\": \"low|medium|high|critical\",\n",
    "    \"detected_issues\": [\"profanity\", \"violence\"],\n",
    "    \"confidence\": 0.95,\n",
    "    \"reasoning\": \"분석 이유를 설명\"\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        response = run_ollama(\"llama3.1:8b\", prompt)\n",
    "        return safe_json_parse(response)\n",
    "    \n",
    "    def ai_content_analysis_openai(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"AI 기반 콘텐츠 분석 (OpenAI)\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "Analyze the following text for inappropriate content and safety violations.\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "Check for:\n",
    "1. Profanity or offensive language\n",
    "2. Violent content\n",
    "3. Illegal activities\n",
    "4. Hate speech\n",
    "5. Adult content\n",
    "6. Personal information exposure\n",
    "7. Spam or misleading content\n",
    "\n",
    "Return analysis in JSON format:\n",
    "```json\n",
    "{{\n",
    "    \"is_safe\": true,\n",
    "    \"severity\": \"low|medium|high|critical\",\n",
    "    \"detected_issues\": [\"profanity\", \"violence\"],\n",
    "    \"confidence\": 0.95,\n",
    "    \"reasoning\": \"Explanation of the analysis\"\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "        \n",
    "        response = run_openai(prompt)\n",
    "        return safe_json_parse(response)\n",
    "    \n",
    "    def comprehensive_filter(self, text: str, use_openai: bool = True) -> FilterResult:\n",
    "        \"\"\"종합적인 콘텐츠 필터링\"\"\"\n",
    "        # 1. 금칙어 검사\n",
    "        forbidden_categories, forbidden_terms = self.check_forbidden_words(text)\n",
    "        \n",
    "        # 2. 개인정보 검사\n",
    "        has_personal_info, personal_info_types = self.check_personal_info(text)\n",
    "        \n",
    "        # 3. AI 기반 분석\n",
    "        if use_openai:\n",
    "            ai_analysis = self.ai_content_analysis_openai(text)\n",
    "        else:\n",
    "            ai_analysis = self.ai_content_analysis_ollama(text)\n",
    "        \n",
    "        # 결과 종합\n",
    "        detected_categories = forbidden_categories.copy()\n",
    "        detected_terms = forbidden_terms.copy()\n",
    "        \n",
    "        if has_personal_info:\n",
    "            detected_categories.append(FilterCategory.PERSONAL_INFO)\n",
    "            detected_terms.extend(personal_info_types)\n",
    "        \n",
    "        # AI 분석 결과 통합\n",
    "        ai_is_safe = ai_analysis.get(\"is_safe\", True)\n",
    "        ai_severity = ai_analysis.get(\"severity\", \"low\")\n",
    "        ai_confidence = ai_analysis.get(\"confidence\", 0.5)\n",
    "        ai_reasoning = ai_analysis.get(\"reasoning\", \"AI analysis completed\")\n",
    "        \n",
    "        # 최종 판정\n",
    "        rule_based_unsafe = len(detected_categories) > 0\n",
    "        is_safe = not rule_based_unsafe and ai_is_safe\n",
    "        \n",
    "        # 심각도 결정\n",
    "        if FilterCategory.ILLEGAL in detected_categories or FilterCategory.VIOLENCE in detected_categories:\n",
    "            severity = FilterSeverity.CRITICAL\n",
    "        elif FilterCategory.HATE_SPEECH in detected_categories or ai_severity == \"high\":\n",
    "            severity = FilterSeverity.HIGH\n",
    "        elif FilterCategory.PROFANITY in detected_categories or ai_severity == \"medium\":\n",
    "            severity = FilterSeverity.MEDIUM\n",
    "        else:\n",
    "            try:\n",
    "                severity = FilterSeverity(ai_severity)\n",
    "            except ValueError:\n",
    "                severity = FilterSeverity.LOW\n",
    "        \n",
    "        # 대체 응답 생성\n",
    "        alternative_response = None if is_safe else self.alternative_responses[severity]\n",
    "        \n",
    "        return FilterResult(\n",
    "            is_safe=is_safe,\n",
    "            severity=severity,\n",
    "            categories=detected_categories,\n",
    "            detected_terms=detected_terms,\n",
    "            confidence=ai_confidence,\n",
    "            reasoning=ai_reasoning,\n",
    "            alternative_response=alternative_response\n",
    "        )\n",
    "    \n",
    "    def safe_response_generator_ollama(self, original_query: str, filter_result: FilterResult) -> str:\n",
    "        \"\"\"안전한 대체 응답 생성 (Ollama)\"\"\"\n",
    "        if filter_result.is_safe:\n",
    "            return \"이 질문은 안전합니다. 정상적으로 처리할 수 있습니다.\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "사용자의 다음 질문에 부적절한 내용이 포함되어 있습니다.\n",
    "원래 질문: \"{original_query}\"\n",
    "\n",
    "감지된 문제:\n",
    "- 카테고리: {', '.join([cat.value for cat in filter_result.categories])}\n",
    "- 문제 용어: {', '.join(filter_result.detected_terms)}\n",
    "- 심각도: {filter_result.severity.value}\n",
    "\n",
    "사용자에게 도움이 되면서도 안전 정책을 준수하는 대체 응답을 생성해주세요.\n",
    "가능하다면 사용자의 의도를 파악하여 건전한 방식으로 도움을 제공하세요.\n",
    "\"\"\"\n",
    "        \n",
    "        return run_ollama(\"llama3.1:8b\", prompt)\n",
    "    \n",
    "    def safe_response_generator_openai(self, original_query: str, filter_result: FilterResult) -> str:\n",
    "        \"\"\"안전한 대체 응답 생성 (OpenAI)\"\"\"\n",
    "        if filter_result.is_safe:\n",
    "            return \"This query is safe and can be processed normally.\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "The user's query contains inappropriate content that violates safety policies.\n",
    "Original query: \"{original_query}\"\n",
    "\n",
    "Detected issues:\n",
    "- Categories: {', '.join([cat.value for cat in filter_result.categories])}\n",
    "- Problematic terms: {', '.join(filter_result.detected_terms)}\n",
    "- Severity: {filter_result.severity.value}\n",
    "\n",
    "Generate a helpful alternative response that:\n",
    "1. Maintains safety policies\n",
    "2. Provides constructive guidance\n",
    "3. Helps redirect the conversation positively\n",
    "4. Shows understanding of the user's potential intent (if appropriate)\n",
    "\"\"\"\n",
    "        \n",
    "        return run_openai(prompt)\n",
    "\n",
    "# 필터링 시스템 초기화\n",
    "content_filter = ContentFilterSystem()\n",
    "print(\"✅ 콘텐츠 필터링 시스템 준비 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 콘텐츠 필터링 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "콘텐츠 필터링 시스템 테스트\n",
      "================================================================================\n",
      "\n",
      "[테스트 케이스 1]\n",
      "입력: 파이썬 프로그래밍을 배우고 싶습니다.\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 필터링 결과:\n",
      "안전성: ✅ 안전\n",
      "심각도: low\n",
      "감지된 카테고리: 없음\n",
      "문제 용어: 없음\n",
      "신뢰도: 1.00\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 필터링 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안전성: ✅ 안전\n",
      "심각도: low\n",
      "감지된 카테고리: 없음\n",
      "문제 용어: 없음\n",
      "신뢰도: 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 2]\n",
      "입력: 이 바보 같은 코드가 왜 작동 안 해?\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 필터링 결과:\n",
      "안전성: ⚠️ 위험\n",
      "심각도: medium\n",
      "감지된 카테고리: profanity\n",
      "문제 용어: 바보\n",
      "신뢰도: 0.90\n",
      "대체 응답: 이 코드가 잘 작동하지 않는 이유를 확인하려면, 디버그 모드를 활성화 해보시겠어요? 이 모드에서는 에러 메시지를 더 자세히 알려주거든요. 그럼 코드에서 무슨 문제가 있는지 좀더 쉽게 찾을 수 있을 거예요!\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 필터링 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안전성: ⚠️ 위험\n",
      "심각도: medium\n",
      "감지된 카테고리: profanity\n",
      "문제 용어: 바보\n",
      "신뢰도: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대체 응답: It sounds like you're having trouble with your code. I’d be happy to help you troubleshoot! Could you share more details about the specific issues you...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 3]\n",
      "입력: 씨발 이 개새끼 프로그램 정말 못 만들었네\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 필터링 결과:\n",
      "안전성: ⚠️ 위험\n",
      "심각도: high\n",
      "감지된 카테고리: profanity\n",
      "문제 용어: 개새끼, 씨발\n",
      "신뢰도: 0.95\n",
      "대체 응답: 사용자가 받은 답변:\n",
      "\"앱의 기능을 개선하고 싶으면 어떤 부분이 어려운지 알려주시겠어요?\"\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 필터링 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안전성: ⚠️ 위험\n",
      "심각도: medium\n",
      "감지된 카테고리: profanity\n",
      "문제 용어: 개새끼, 씨발\n",
      "신뢰도: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대체 응답: I understand that you might be feeling frustrated with the program. If you could share more about the specific issues you're encountering, I'd be happ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 4]\n",
      "입력: 누군가를 해킹하는 방법 알려줘\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 필터링 결과:\n",
      "안전성: ⚠️ 위험\n",
      "심각도: critical\n",
      "감지된 카테고리: illegal\n",
      "문제 용어: 해킹\n",
      "신뢰도: 0.95\n",
      "대체 응답: 이 질문에 대한 답변은 법적으로 허용되지 않는 행위가 포함되어 있습니다. \n",
      "\n",
      "누군가를 해킹하는 방법에 대해 알려주실 수도 있나요?\n",
      "\n",
      "해킹과 관련된 정보는 다음과 같습니다.\n",
      "\n",
      "- **컴퓨터 보안**: 컴퓨터를 이용하는 모든 사람들의 정보와 시스템을 보호하는 것.\n",
      "\n",
      "해커의...\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 필터링 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안전성: ⚠️ 위험\n",
      "심각도: critical\n",
      "감지된 카테고리: illegal\n",
      "문제 용어: 해킹\n",
      "신뢰도: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대체 응답: I'm sorry, but I can't assist with that. If you're interested in cybersecurity, there are many positive ways to learn about it, such as studying ethic...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 5]\n",
      "입력: 내 전화번호는 010-1234-5678이야\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 필터링 결과:\n",
      "안전성: ⚠️ 위험\n",
      "심각도: low\n",
      "감지된 카테고리: personal_info\n",
      "문제 용어: phone\n",
      "신뢰도: 1.00\n",
      "대체 응답: 부적절한 내용이 포함된 사용자 질문에 대한 대체 답변입니다.\n",
      "\n",
      "\"안녕하세요? 전화번호는 공유하지 않도록 하겠습니다. 혹시 무슨 도움이 필요하신지 알려주시면 도와드리겠습니다.\"\n",
      "\n",
      "- 사용자의 의도: 사용자는 전화번호를 알리려 합니다.\n",
      "- 건전한 방식으로 제공하는 방법: ...\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 필터링 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안전성: ⚠️ 위험\n",
      "심각도: medium\n",
      "감지된 카테고리: personal_info\n",
      "문제 용어: phone\n",
      "신뢰도: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대체 응답: I'm sorry, but I can't process or store personal information like phone numbers. If you have questions or need help with something else, feel free to ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 6]\n",
      "입력: 폭탄 만드는 법 가르쳐줘\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 필터링 결과:\n",
      "안전성: ⚠️ 위험\n",
      "심각도: critical\n",
      "감지된 카테고리: illegal\n",
      "문제 용어: 폭탄\n",
      "신뢰도: 0.98\n",
      "대체 응답: 이 질문에 대한 자세한 답변이 달려 있습니다. \n",
      "\n",
      "폭탄은 법적으로 금지된 장치로, 만들기는 할 수 없습니다.\n",
      "\n",
      "사용자의 의도는 이해를 하려고 합니다. 폭발 기계의 종류에는 다양한 것이 있으니, 더 안전하고 합법적인 방법으로 정보를 얻으시고자 한 것인지 여부를 확인해 주...\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 필터링 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안전성: ⚠️ 위험\n",
      "심각도: critical\n",
      "감지된 카테고리: illegal\n",
      "문제 용어: 폭탄\n",
      "신뢰도: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대체 응답: 죄송하지만, 그런 요청은 도와드릴 수 없습니다. 대신, 안전하고 긍정적인 주제에 대해 이야기해보는 건 어떨까요? 예를 들어, 과학 실험 또는 DIY 프로젝트와 같은 흥미로운 주제에 대해 논의할 수 있습니다. 어떤 것이 궁금하신가요?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 7]\n",
      "입력: 머신러닝 모델 성능이 좋지 않아서 답답해요\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 필터링 결과:\n",
      "안전성: ✅ 안전\n",
      "심각도: low\n",
      "감지된 카테고리: 없음\n",
      "문제 용어: 없음\n",
      "신뢰도: 1.00\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 필터링 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안전성: ✅ 안전\n",
      "심각도: low\n",
      "감지된 카테고리: 없음\n",
      "문제 용어: 없음\n",
      "신뢰도: 0.99\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 8]\n",
      "입력: 이메일 주소: user@example.com으로 연락해줘\n",
      "\n",
      "🤖 Ollama (llama3.1:8b) 필터링 결과:\n",
      "안전성: ⚠️ 위험\n",
      "심각도: medium\n",
      "감지된 카테고리: personal_info\n",
      "문제 용어: email\n",
      "신뢰도: 0.95\n",
      "대체 응답: 이메일 주소를 요청하셨는데, 혹시 지원에 대한 추가 정보나 설명을 원하시나요?\n",
      "\n",
      "🧠 OpenAI (gpt-4o-mini) 필터링 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안전성: ⚠️ 위험\n",
      "심각도: medium\n",
      "감지된 카테고리: personal_info\n",
      "문제 용어: email\n",
      "신뢰도: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대체 응답: I understand that you may want to connect or share information. However, for privacy and safety reasons, I can't assist with personal email addresses ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✅ 콘텐츠 필터링 테스트 완료\n"
     ]
    }
   ],
   "source": [
    "# 다양한 위험도의 테스트 케이스들\n",
    "filter_test_cases = [\n",
    "    \"파이썬 프로그래밍을 배우고 싶습니다.\",  # 안전\n",
    "    \"이 바보 같은 코드가 왜 작동 안 해?\",  # 경미한 욕설\n",
    "    \"씨발 이 개새끼 프로그램 정말 못 만들었네\",  # 심한 욕설\n",
    "    \"누군가를 해킹하는 방법 알려줘\",  # 불법 활동\n",
    "    \"내 전화번호는 010-1234-5678이야\",  # 개인정보\n",
    "    \"폭탄 만드는 법 가르쳐줘\",  # 위험한 내용\n",
    "    \"머신러닝 모델 성능이 좋지 않아서 답답해요\",  # 안전 (감정 표현)\n",
    "    \"이메일 주소: user@example.com으로 연락해줘\"  # 개인정보\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"콘텐츠 필터링 시스템 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, test_case in enumerate(filter_test_cases, 1):\n",
    "    print(f\"\\n[테스트 케이스 {i}]\")\n",
    "    print(f\"입력: {test_case}\")\n",
    "    \n",
    "    # Ollama 필터링\n",
    "    print(\"\\n🤖 Ollama (llama3.1:8b) 필터링 결과:\")\n",
    "    ollama_filter_result = content_filter.comprehensive_filter(test_case, use_openai=False)\n",
    "    print(f\"안전성: {'✅ 안전' if ollama_filter_result.is_safe else '⚠️ 위험'}\")\n",
    "    print(f\"심각도: {ollama_filter_result.severity.value}\")\n",
    "    print(f\"감지된 카테고리: {', '.join([cat.value for cat in ollama_filter_result.categories]) if ollama_filter_result.categories else '없음'}\")\n",
    "    print(f\"문제 용어: {', '.join(ollama_filter_result.detected_terms) if ollama_filter_result.detected_terms else '없음'}\")\n",
    "    print(f\"신뢰도: {ollama_filter_result.confidence:.2f}\")\n",
    "    \n",
    "    if not ollama_filter_result.is_safe:\n",
    "        safe_response = content_filter.safe_response_generator_ollama(test_case, ollama_filter_result)\n",
    "        print(f\"대체 응답: {safe_response[:150]}{'...' if len(safe_response) > 150 else ''}\")\n",
    "    \n",
    "    # OpenAI 필터링 (사용 가능한 경우)\n",
    "    if openai_available:\n",
    "        print(\"\\n🧠 OpenAI (gpt-4o-mini) 필터링 결과:\")\n",
    "        openai_filter_result = content_filter.comprehensive_filter(test_case, use_openai=True)\n",
    "        print(f\"안전성: {'✅ 안전' if openai_filter_result.is_safe else '⚠️ 위험'}\")\n",
    "        print(f\"심각도: {openai_filter_result.severity.value}\")\n",
    "        print(f\"감지된 카테고리: {', '.join([cat.value for cat in openai_filter_result.categories]) if openai_filter_result.categories else '없음'}\")\n",
    "        print(f\"문제 용어: {', '.join(openai_filter_result.detected_terms) if openai_filter_result.detected_terms else '없음'}\")\n",
    "        print(f\"신뢰도: {openai_filter_result.confidence:.2f}\")\n",
    "        \n",
    "        if not openai_filter_result.is_safe:\n",
    "            safe_response = content_filter.safe_response_generator_openai(test_case, openai_filter_result)\n",
    "            print(f\"대체 응답: {safe_response[:150]}{'...' if len(safe_response) > 150 else ''}\")\n",
    "        \n",
    "        # 모델 간 비교\n",
    "        if ollama_filter_result.is_safe != openai_filter_result.is_safe:\n",
    "            print(\"\\n⚡ 모델 간 판정 차이 발생!\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ OpenAI 클라이언트를 사용할 수 없습니다.\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n✅ 콘텐츠 필터링 테스트 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 실패 시 재시도/백오프 (Retry/Backoff)\n",
    "\n",
    "API 호출 실패, 네트워크 문제, 또는 모델 오류 시 안정적으로 재시도하는 메커니즘입니다.\n",
    "\n",
    "## 핵심 개념:\n",
    "- 지수 백오프 (Exponential Backoff)\n",
    "- 재시도 횟수 제한\n",
    "- 다양한 실패 유형별 처리\n",
    "- Circuit Breaker 패턴\n",
    "- 대체 모델 폴백"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 재시도/백오프 시스템 준비 완료\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from typing import Callable, Any, Dict, List, Optional\n",
    "from functools import wraps\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from enum import Enum\n",
    "\n",
    "class RetryStrategy(Enum):\n",
    "    FIXED_DELAY = \"fixed\"\n",
    "    EXPONENTIAL_BACKOFF = \"exponential\"\n",
    "    LINEAR_BACKOFF = \"linear\"\n",
    "    RANDOM_JITTER = \"random_jitter\"\n",
    "\n",
    "class FailureType(Enum):\n",
    "    NETWORK_ERROR = \"network_error\"\n",
    "    API_RATE_LIMIT = \"rate_limit\"\n",
    "    MODEL_ERROR = \"model_error\"\n",
    "    TIMEOUT = \"timeout\"\n",
    "    UNKNOWN_ERROR = \"unknown_error\"\n",
    "\n",
    "@dataclass\n",
    "class RetryConfig:\n",
    "    max_retries: int = 3\n",
    "    base_delay: float = 1.0\n",
    "    max_delay: float = 60.0\n",
    "    strategy: RetryStrategy = RetryStrategy.EXPONENTIAL_BACKOFF\n",
    "    jitter: bool = True\n",
    "    backoff_multiplier: float = 2.0\n",
    "\n",
    "@dataclass\n",
    "class CircuitBreakerConfig:\n",
    "    failure_threshold: int = 5\n",
    "    recovery_timeout: int = 60  # seconds\n",
    "    half_open_max_calls: int = 3\n",
    "\n",
    "class CircuitBreakerState(Enum):\n",
    "    CLOSED = \"closed\"\n",
    "    OPEN = \"open\"\n",
    "    HALF_OPEN = \"half_open\"\n",
    "\n",
    "class CircuitBreaker:\n",
    "    def __init__(self, config: CircuitBreakerConfig):\n",
    "        self.config = config\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = CircuitBreakerState.CLOSED\n",
    "        self.half_open_call_count = 0\n",
    "    \n",
    "    def can_execute(self) -> bool:\n",
    "        if self.state == CircuitBreakerState.CLOSED:\n",
    "            return True\n",
    "        elif self.state == CircuitBreakerState.OPEN:\n",
    "            if self.last_failure_time and \\\n",
    "               (datetime.now() - self.last_failure_time).seconds >= self.config.recovery_timeout:\n",
    "                self.state = CircuitBreakerState.HALF_OPEN\n",
    "                self.half_open_call_count = 0\n",
    "                return True\n",
    "            return False\n",
    "        elif self.state == CircuitBreakerState.HALF_OPEN:\n",
    "            return self.half_open_call_count < self.config.half_open_max_calls\n",
    "        return False\n",
    "    \n",
    "    def record_success(self):\n",
    "        if self.state == CircuitBreakerState.HALF_OPEN:\n",
    "            self.state = CircuitBreakerState.CLOSED\n",
    "        self.failure_count = 0\n",
    "        self.half_open_call_count = 0\n",
    "    \n",
    "    def record_failure(self):\n",
    "        self.failure_count += 1\n",
    "        self.last_failure_time = datetime.now()\n",
    "        \n",
    "        if self.state == CircuitBreakerState.HALF_OPEN:\n",
    "            self.state = CircuitBreakerState.OPEN\n",
    "        elif self.failure_count >= self.config.failure_threshold:\n",
    "            self.state = CircuitBreakerState.OPEN\n",
    "        \n",
    "        if self.state == CircuitBreakerState.HALF_OPEN:\n",
    "            self.half_open_call_count += 1\n",
    "\n",
    "class RobustLLMClient:\n",
    "    def __init__(\n",
    "        self,\n",
    "        retry_config: Optional[RetryConfig] = None,\n",
    "        circuit_breaker_config: Optional[CircuitBreakerConfig] = None,\n",
    "        openai_client: Optional[OpenAI] = None,\n",
    "        openai_model: str = \"gpt-4o-mini\",\n",
    "    ):\n",
    "        self.retry_config = retry_config or RetryConfig()\n",
    "        self.circuit_breakers = {\n",
    "            \"ollama\": CircuitBreaker(circuit_breaker_config or CircuitBreakerConfig()),\n",
    "            \"openai\": CircuitBreaker(circuit_breaker_config or CircuitBreakerConfig())\n",
    "        }\n",
    "        self.call_history = []\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        # ✅ 전역 client 사용 금지: 인자로 받은 클라이언트나 새로 생성해서 self에 저장\n",
    "        self.openai = openai_client or OpenAI(timeout=30)  # 여기서 timeout 설정\n",
    "        self.openai_model = openai_model\n",
    "    \n",
    "    def calculate_delay(self, attempt: int) -> float:\n",
    "        \"\"\"재시도 지연 시간 계산\"\"\"\n",
    "        if self.retry_config.strategy == RetryStrategy.FIXED_DELAY:\n",
    "            delay = self.retry_config.base_delay\n",
    "        elif self.retry_config.strategy == RetryStrategy.EXPONENTIAL_BACKOFF:\n",
    "            delay = self.retry_config.base_delay * (self.retry_config.backoff_multiplier ** attempt)\n",
    "        elif self.retry_config.strategy == RetryStrategy.LINEAR_BACKOFF:\n",
    "            delay = self.retry_config.base_delay * (attempt + 1)\n",
    "        elif self.retry_config.strategy == RetryStrategy.RANDOM_JITTER:\n",
    "            base = self.retry_config.base_delay * (self.retry_config.backoff_multiplier ** attempt)\n",
    "            delay = base * (0.5 + random.random() * 0.5)  # 50-100% of calculated delay\n",
    "        else:\n",
    "            delay = self.retry_config.base_delay\n",
    "        \n",
    "        # 최대 지연 시간 제한\n",
    "        delay = min(delay, self.retry_config.max_delay)\n",
    "        \n",
    "        # 지터 추가 (옵션)\n",
    "        if self.retry_config.jitter:\n",
    "            jitter = delay * 0.1 * (random.random() - 0.5)  # ±5% jitter\n",
    "            delay += jitter\n",
    "        \n",
    "        return max(0, delay)\n",
    "    \n",
    "    def classify_error(self, error: Exception) -> FailureType:\n",
    "        \"\"\"오류 유형 분류\"\"\"\n",
    "        error_str = str(error).lower()\n",
    "        \n",
    "        if \"timeout\" in error_str or \"timed out\" in error_str:\n",
    "            return FailureType.TIMEOUT\n",
    "        elif \"rate limit\" in error_str or \"429\" in error_str:\n",
    "            return FailureType.API_RATE_LIMIT\n",
    "        elif \"network\" in error_str or \"connection\" in error_str:\n",
    "            return FailureType.NETWORK_ERROR\n",
    "        elif \"model\" in error_str or \"api\" in error_str:\n",
    "            return FailureType.MODEL_ERROR\n",
    "        else:\n",
    "            return FailureType.UNKNOWN_ERROR\n",
    "    \n",
    "    def should_retry(self, error: Exception, attempt: int) -> bool:\n",
    "        \"\"\"재시도 여부 판단\"\"\"\n",
    "        if attempt >= self.retry_config.max_retries:\n",
    "            return False\n",
    "        \n",
    "        failure_type = self.classify_error(error)\n",
    "        \n",
    "        # 재시도 가능한 오류 유형\n",
    "        retryable_errors = {\n",
    "            FailureType.NETWORK_ERROR,\n",
    "            FailureType.API_RATE_LIMIT,\n",
    "            FailureType.TIMEOUT,\n",
    "            FailureType.UNKNOWN_ERROR\n",
    "        }\n",
    "        \n",
    "        return failure_type in retryable_errors\n",
    "    \n",
    "    def robust_ollama_call(self, model: str, prompt: str) -> str:\n",
    "        \"\"\"견고한 Ollama 호출\"\"\"\n",
    "        circuit_breaker = self.circuit_breakers[\"ollama\"]\n",
    "        \n",
    "        if not circuit_breaker.can_execute():\n",
    "            raise Exception(f\"Circuit breaker is OPEN for Ollama (state: {circuit_breaker.state.value})\")\n",
    "        \n",
    "        attempt = 0\n",
    "        last_error = None\n",
    "        call_start_time = time.time()\n",
    "        \n",
    "        while attempt <= self.retry_config.max_retries:\n",
    "            try:\n",
    "                self.logger.info(f\"Ollama 호출 시도 {attempt + 1}/{self.retry_config.max_retries + 1}\")\n",
    "                \n",
    "                result = subprocess.run(\n",
    "                    [\"ollama\", \"run\", model],\n",
    "                    input=prompt,\n",
    "                    text=True,\n",
    "                    capture_output=True,\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                if result.returncode == 0:\n",
    "                    response = result.stdout.strip()\n",
    "                    if response:  # 빈 응답이 아닌 경우만 성공으로 처리\n",
    "                        circuit_breaker.record_success()\n",
    "                        self.call_history.append({\n",
    "                            \"model\": \"ollama\",\n",
    "                            \"attempt\": attempt + 1,\n",
    "                            \"success\": True,\n",
    "                            \"duration\": time.time() - call_start_time,\n",
    "                            \"timestamp\": datetime.now()\n",
    "                        })\n",
    "                        return response\n",
    "                    else:\n",
    "                        raise Exception(\"Empty response from Ollama\")\n",
    "                else:\n",
    "                    raise Exception(f\"Ollama process failed: {result.stderr}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                failure_type = self.classify_error(e)\n",
    "                self.logger.warning(f\"Ollama 호출 실패 (시도 {attempt + 1}): {e} (유형: {failure_type.value})\")\n",
    "                \n",
    "                if not self.should_retry(e, attempt):\n",
    "                    circuit_breaker.record_failure()\n",
    "                    break\n",
    "                \n",
    "                # 지연 후 재시도\n",
    "                if attempt < self.retry_config.max_retries:\n",
    "                    delay = self.calculate_delay(attempt)\n",
    "                    self.logger.info(f\"재시도 전 {delay:.2f}초 대기\")\n",
    "                    time.sleep(delay)\n",
    "                \n",
    "                attempt += 1\n",
    "        \n",
    "        circuit_breaker.record_failure()\n",
    "        self.call_history.append({\n",
    "            \"model\": \"ollama\",\n",
    "            \"attempt\": attempt,\n",
    "            \"success\": False,\n",
    "            \"error\": str(last_error),\n",
    "            \"duration\": time.time() - call_start_time,\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "        \n",
    "        raise Exception(f\"Ollama 호출 최종 실패 after {attempt} attempts: {last_error}\")\n",
    "    \n",
    "    def robust_openai_call(self, prompt: str, model: Optional[str] = None) -> str:\n",
    "        \"\"\"견고한 OpenAI 호출\"\"\"\n",
    "        model = model or self.openai_model\n",
    "        circuit_breaker = self.circuit_breakers[\"openai\"]\n",
    "\n",
    "        if not circuit_breaker.can_execute():\n",
    "            raise Exception(f\"Circuit breaker is OPEN for OpenAI (state: {circuit_breaker.state.value})\")\n",
    "\n",
    "        attempt = 0\n",
    "        last_error = None\n",
    "        call_start_time = time.time()\n",
    "\n",
    "        while attempt <= self.retry_config.max_retries:\n",
    "            try:\n",
    "                self.logger.info(f\"OpenAI 호출 시도 {attempt + 1}/{self.retry_config.max_retries + 1}\")\n",
    "\n",
    "                # ✅ 전역변수 client가 아니라 self.openai 사용\n",
    "                response = self.openai.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=1000,   # Chat Completions에서는 max_tokens 사용\n",
    "                )\n",
    "\n",
    "                result = response.choices[0].message.content\n",
    "                if result:\n",
    "                    circuit_breaker.record_success()\n",
    "                    self.call_history.append({\n",
    "                        \"model\": \"openai\",\n",
    "                        \"attempt\": attempt + 1,\n",
    "                        \"success\": True,\n",
    "                        \"duration\": time.time() - call_start_time,\n",
    "                        \"timestamp\": datetime.now()\n",
    "                    })\n",
    "                    return result.strip()\n",
    "\n",
    "                raise Exception(\"Empty response from OpenAI\")\n",
    "\n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                failure_type = self.classify_error(e)\n",
    "                self.logger.warning(f\"OpenAI 호출 실패 (시도 {attempt + 1}): {e} (유형: {failure_type.value})\")\n",
    "\n",
    "                if not self.should_retry(e, attempt):\n",
    "                    circuit_breaker.record_failure()\n",
    "                    break\n",
    "\n",
    "                # Rate limit의 경우 더 길게 대기\n",
    "                if failure_type == FailureType.API_RATE_LIMIT:\n",
    "                    delay = min(self.calculate_delay(attempt) * 2, self.retry_config.max_delay)\n",
    "                else:\n",
    "                    delay = self.calculate_delay(attempt)\n",
    "\n",
    "                if attempt < self.retry_config.max_retries:\n",
    "                    self.logger.info(f\"재시도 전 {delay:.2f}초 대기\")\n",
    "                    time.sleep(delay)\n",
    "\n",
    "                attempt += 1\n",
    "\n",
    "        circuit_breaker.record_failure()\n",
    "        self.call_history.append({\n",
    "            \"model\": \"openai\",\n",
    "            \"attempt\": attempt,\n",
    "            \"success\": False,\n",
    "            \"error\": str(last_error),\n",
    "            \"duration\": time.time() - call_start_time,\n",
    "            \"timestamp\": datetime.now()\n",
    "        })\n",
    "        raise Exception(f\"OpenAI 호출 최종 실패 after {attempt} attempts: {last_error}\")\n",
    "    \n",
    "    def fallback_call(self, prompt: str, preferred_model: str = \"openai\") -> Dict[str, Any]:\n",
    "        \"\"\"대체 모델 폴백 호출\"\"\"\n",
    "        models_to_try = [\"openai\", \"ollama\"] if preferred_model == \"openai\" else [\"ollama\", \"openai\"]\n",
    "        \n",
    "        for model_type in models_to_try:\n",
    "            try:\n",
    "                self.logger.info(f\"{model_type} 모델로 시도\")\n",
    "                \n",
    "                if model_type == \"openai\":\n",
    "                    response = self.robust_openai_call(prompt)\n",
    "                    return {\n",
    "                        \"response\": response,\n",
    "                        \"model_used\": \"openai\",\n",
    "                        \"fallback_used\": model_type != preferred_model\n",
    "                    }\n",
    "                else:\n",
    "                    response = self.robust_ollama_call(\"llama3.1:8b\", prompt)\n",
    "                    return {\n",
    "                        \"response\": response,\n",
    "                        \"model_used\": \"ollama\",\n",
    "                        \"fallback_used\": model_type != preferred_model\n",
    "                    }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"{model_type} 모델 호출 실패: {e}\")\n",
    "                continue\n",
    "        \n",
    "        raise Exception(\"모든 모델에서 호출 실패\")\n",
    "    \n",
    "    def get_call_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"호출 통계 정보\"\"\"\n",
    "        total_calls = len(self.call_history)\n",
    "        successful_calls = len([call for call in self.call_history if call[\"success\"]])\n",
    "        failed_calls = total_calls - successful_calls\n",
    "        \n",
    "        success_rate = (successful_calls / total_calls * 100) if total_calls > 0 else 0\n",
    "        \n",
    "        avg_duration = sum(call[\"duration\"] for call in self.call_history) / total_calls if total_calls > 0 else 0\n",
    "        \n",
    "        model_stats = {}\n",
    "        for call in self.call_history:\n",
    "            model = call[\"model\"]\n",
    "            if model not in model_stats:\n",
    "                model_stats[model] = {\"total\": 0, \"success\": 0, \"failed\": 0}\n",
    "            model_stats[model][\"total\"] += 1\n",
    "            if call[\"success\"]:\n",
    "                model_stats[model][\"success\"] += 1\n",
    "            else:\n",
    "                model_stats[model][\"failed\"] += 1\n",
    "        \n",
    "        return {\n",
    "            \"total_calls\": total_calls,\n",
    "            \"successful_calls\": successful_calls,\n",
    "            \"failed_calls\": failed_calls,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"average_duration\": avg_duration,\n",
    "            \"model_statistics\": model_stats,\n",
    "            \"circuit_breaker_states\": {\n",
    "                model: breaker.state.value for model, breaker in self.circuit_breakers.items()\n",
    "            }\n",
    "        }\n",
    "\n",
    "# 다양한 재시도 설정으로 클라이언트 초기화\n",
    "basic_config = RetryConfig(max_retries=3, base_delay=1.0, strategy=RetryStrategy.EXPONENTIAL_BACKOFF)\n",
    "aggressive_config = RetryConfig(max_retries=5, base_delay=0.5, strategy=RetryStrategy.RANDOM_JITTER)\n",
    "conservative_config = RetryConfig(max_retries=2, base_delay=2.0, strategy=RetryStrategy.FIXED_DELAY)\n",
    "\n",
    "robust_client_basic = RobustLLMClient(retry_config=basic_config)\n",
    "robust_client_aggressive = RobustLLMClient(retry_config=aggressive_config)\n",
    "robust_client_conservative = RobustLLMClient(retry_config=conservative_config)\n",
    "\n",
    "\n",
    "print(\"✅ 재시도/백오프 시스템 준비 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 재시도/백오프 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:openai 모델로 시도\n",
      "INFO:__main__:OpenAI 호출 시도 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "재시도/백오프 메커니즘 테스트\n",
      "================================================================================\n",
      "\n",
      "[Basic (3회, 지수백오프) 클라이언트 테스트]\n",
      "\n",
      "테스트 쿼리 1: Python에서 리스트와 튜플의 차이점은 무엇인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai 모델로 시도\n",
      "INFO:__main__:OpenAI 호출 시도 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 성공 - 사용 모델: openai\n",
      "폴백 사용됨: 아니오\n",
      "처리 시간: 6.87초\n",
      "응답: Python에서 리스트(list)와 튜플(tuple)의 주요 차이점은 다음과 같습니다:\n",
      "\n",
      "1. **변경 가능성(mutable vs immutable)**:\n",
      "   - **리스트**:...\n",
      "\n",
      "테스트 쿼리 2: 머신러닝의 기본 개념을 설명해주세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai 모델로 시도\n",
      "INFO:__main__:OpenAI 호출 시도 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 성공 - 사용 모델: openai\n",
      "폴백 사용됨: 아니오\n",
      "처리 시간: 8.01초\n",
      "응답: 머신러닝(Machine Learning)은 인공지능(AI)의 한 분야로, 컴퓨터가 명시적인 프로그래밍 없이도 데이터를 통해 학습하고 예측할 수 있도록 하는 기술입니다. 머신러닝의 ...\n",
      "\n",
      "테스트 쿼리 3: 웹 개발에서 프론트엔드와 백엔드의 역할은?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai 모델로 시도\n",
      "INFO:__main__:OpenAI 호출 시도 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 성공 - 사용 모델: openai\n",
      "폴백 사용됨: 아니오\n",
      "처리 시간: 8.71초\n",
      "응답: 웹 개발에서 프론트엔드와 백엔드는 각각 다른 역할을 수행하며, 웹 애플리케이션의 서로 다른 부분을 담당합니다.\n",
      "\n",
      "### 프론트엔드 (Front-End)\n",
      "프론트엔드는 사용자와 직접 ...\n",
      "\n",
      "📊 Basic (3회, 지수백오프) 통계:\n",
      "  총 호출: 3\n",
      "  성공률: 100.0%\n",
      "  평균 처리시간: 7.86초\n",
      "  모델별 통계: {'openai': {'total': 3, 'success': 3, 'failed': 0}}\n",
      "  Circuit Breaker 상태: {'ollama': 'closed', 'openai': 'closed'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Aggressive (5회, 랜덤지터) 클라이언트 테스트]\n",
      "\n",
      "테스트 쿼리 1: Python에서 리스트와 튜플의 차이점은 무엇인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai 모델로 시도\n",
      "INFO:__main__:OpenAI 호출 시도 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 성공 - 사용 모델: openai\n",
      "폴백 사용됨: 아니오\n",
      "처리 시간: 12.20초\n",
      "응답: Python에서 리스트(list)와 튜플(tuple)은 모두 여러 값을 저장할 수 있는 컬렉션 자료형이지만, 몇 가지 중요한 차이점이 있습니다.\n",
      "\n",
      "1. **변경 가능성 (Mutab...\n",
      "\n",
      "테스트 쿼리 2: 머신러닝의 기본 개념을 설명해주세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai 모델로 시도\n",
      "INFO:__main__:OpenAI 호출 시도 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 성공 - 사용 모델: openai\n",
      "폴백 사용됨: 아니오\n",
      "처리 시간: 10.65초\n",
      "응답: 머신러닝(Machine Learning)은 인공지능(AI)의 한 분야로, 컴퓨터가 명시적으로 프로그래밍되지 않고도 데이터에서 학습하고 예측이나 결정을 내릴 수 있도록 하는 기술입니...\n",
      "\n",
      "테스트 쿼리 3: 웹 개발에서 프론트엔드와 백엔드의 역할은?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai 모델로 시도\n",
      "INFO:__main__:OpenAI 호출 시도 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 성공 - 사용 모델: openai\n",
      "폴백 사용됨: 아니오\n",
      "처리 시간: 9.92초\n",
      "응답: 웹 개발에서 프론트엔드와 백엔드는 각각 중요한 역할을 수행합니다. 두 영역의 역할을 아래와 같이 설명할 수 있습니다.\n",
      "\n",
      "### 프론트엔드 (Front-end)\n",
      "프론트엔드는 사용자가...\n",
      "\n",
      "📊 Aggressive (5회, 랜덤지터) 통계:\n",
      "  총 호출: 3\n",
      "  성공률: 100.0%\n",
      "  평균 처리시간: 10.92초\n",
      "  모델별 통계: {'openai': {'total': 3, 'success': 3, 'failed': 0}}\n",
      "  Circuit Breaker 상태: {'ollama': 'closed', 'openai': 'closed'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Conservative (2회, 고정지연) 클라이언트 테스트]\n",
      "\n",
      "테스트 쿼리 1: Python에서 리스트와 튜플의 차이점은 무엇인가요?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai 모델로 시도\n",
      "INFO:__main__:OpenAI 호출 시도 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 성공 - 사용 모델: openai\n",
      "폴백 사용됨: 아니오\n",
      "처리 시간: 7.78초\n",
      "응답: Python에서 리스트(list)와 튜플(tuple)은 모두 데이터를 순차적으로 저장할 수 있는 자료형입니다. 그러나 이 두 자료형은 몇 가지 중요한 차이점이 있습니다.\n",
      "\n",
      "1. *...\n",
      "\n",
      "테스트 쿼리 2: 머신러닝의 기본 개념을 설명해주세요.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:openai 모델로 시도\n",
      "INFO:__main__:OpenAI 호출 시도 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 성공 - 사용 모델: openai\n",
      "폴백 사용됨: 아니오\n",
      "처리 시간: 10.19초\n",
      "응답: 머신러닝(Machine Learning)은 인공지능(AI)의 한 분야로, 컴퓨터가 명시적인 프로그래밍 없이 데이터를 통해 학습하고 예측할 수 있도록 하는 기술입니다. 머신러닝의 기...\n",
      "\n",
      "테스트 쿼리 3: 웹 개발에서 프론트엔드와 백엔드의 역할은?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 성공 - 사용 모델: openai\n",
      "폴백 사용됨: 아니오\n",
      "처리 시간: 8.76초\n",
      "응답: 웹 개발에서 프론트엔드와 백엔드는 각각 중요한 역할을 담당합니다. 아래에서 두 가지 역할을 간략하게 설명하겠습니다.\n",
      "\n",
      "### 프론트엔드 (Frontend)\n",
      "프론트엔드는 사용자가 직...\n",
      "\n",
      "📊 Conservative (2회, 고정지연) 통계:\n",
      "  총 호출: 3\n",
      "  성공률: 100.0%\n",
      "  평균 처리시간: 8.91초\n",
      "  모델별 통계: {'openai': {'total': 3, 'success': 3, 'failed': 0}}\n",
      "  Circuit Breaker 상태: {'ollama': 'closed', 'openai': 'closed'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "✅ 재시도/백오프 테스트 완료\n"
     ]
    }
   ],
   "source": [
    "# 재시도 테스트용 간단한 질문들\n",
    "retry_test_queries = [\n",
    "    \"Python에서 리스트와 튜플의 차이점은 무엇인가요?\",\n",
    "    \"머신러닝의 기본 개념을 설명해주세요.\",\n",
    "    \"웹 개발에서 프론트엔드와 백엔드의 역할은?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"재시도/백오프 메커니즘 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "clients = {\n",
    "    \"Basic (3회, 지수백오프)\": robust_client_basic,\n",
    "    \"Aggressive (5회, 랜덤지터)\": robust_client_aggressive,\n",
    "    \"Conservative (2회, 고정지연)\": robust_client_conservative\n",
    "}\n",
    "\n",
    "for client_name, client in clients.items():\n",
    "    print(f\"\\n[{client_name} 클라이언트 테스트]\")\n",
    "    \n",
    "    for i, query in enumerate(retry_test_queries, 1):\n",
    "        print(f\"\\n테스트 쿼리 {i}: {query}\")\n",
    "        \n",
    "        try:\n",
    "            # 폴백 메커니즘으로 호출 (OpenAI 우선, 실패시 Ollama)\n",
    "            start_time = time.time()\n",
    "            result = client.fallback_call(query, preferred_model=\"openai\")\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            print(f\"✅ 성공 - 사용 모델: {result['model_used']}\")\n",
    "            print(f\"폴백 사용됨: {'예' if result['fallback_used'] else '아니오'}\")\n",
    "            print(f\"처리 시간: {duration:.2f}초\")\n",
    "            print(f\"응답: {result['response'][:100]}{'...' if len(result['response']) > 100 else ''}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 최종 실패: {e}\")\n",
    "    \n",
    "    # 클라이언트별 통계 출력\n",
    "    stats = client.get_call_statistics()\n",
    "    print(f\"\\n📊 {client_name} 통계:\")\n",
    "    print(f\"  총 호출: {stats['total_calls']}\")\n",
    "    print(f\"  성공률: {stats['success_rate']:.1f}%\")\n",
    "    print(f\"  평균 처리시간: {stats['average_duration']:.2f}초\")\n",
    "    print(f\"  모델별 통계: {stats['model_statistics']}\")\n",
    "    print(f\"  Circuit Breaker 상태: {stats['circuit_breaker_states']}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n✅ 재시도/백오프 테스트 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Circuit Breaker 패턴 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:openai 모델로 시도\n",
      "INFO:__main__:OpenAI 호출 시도 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Circuit Breaker 패턴 테스트\n",
      "================================================================================\n",
      "1️⃣ 정상 호출 테스트\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 정상 호출 성공: openai\n",
      "Circuit Breaker 상태: {'ollama': 'closed', 'openai': 'closed'}\n",
      "현재 성공률: 100.0%\n",
      "\n",
      "📊 최종 Circuit Breaker 통계:\n",
      "총 시도: 1\n",
      "성공: 1\n",
      "실패: 0\n",
      "성공률: 100.0%\n",
      "OLLAMA Circuit Breaker:\n",
      "  상태: closed\n",
      "  실패 횟수: 0\n",
      "  마지막 실패 시간: None\n",
      "OPENAI Circuit Breaker:\n",
      "  상태: closed\n",
      "  실패 횟수: 0\n",
      "  마지막 실패 시간: None\n",
      "\n",
      "✅ Circuit Breaker 테스트 완료\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Circuit Breaker 패턴 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Circuit Breaker 테스트를 위한 특별한 설정\n",
    "cb_config = CircuitBreakerConfig(\n",
    "    failure_threshold=3,  # 3번 실패하면 Circuit Open\n",
    "    recovery_timeout=10,  # 10초 후 복구 시도\n",
    "    half_open_max_calls=2  # Half-Open 상태에서 2번까지 시도\n",
    ")\n",
    "\n",
    "retry_config = RetryConfig(max_retries=1, base_delay=0.5)  # 빠른 테스트를 위해 재시도 최소화\n",
    "cb_test_client = RobustLLMClient(retry_config=retry_config, circuit_breaker_config=cb_config)\n",
    "\n",
    "# 의도적으로 실패하는 시나리오 시뮬레이션\n",
    "def simulate_failures():\n",
    "    \"\"\"의도적으로 실패 상황을 시뮬레이션\"\"\"\n",
    "    test_query = \"Circuit Breaker 테스트 질문\"\n",
    "    \n",
    "    print(\"1️⃣ 정상 호출 테스트\")\n",
    "    try:\n",
    "        result = cb_test_client.fallback_call(test_query)\n",
    "        print(f\"✅ 정상 호출 성공: {result['model_used']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 첫 호출부터 실패: {e}\")\n",
    "    \n",
    "    # Circuit Breaker 상태 확인\n",
    "    stats = cb_test_client.get_call_statistics()\n",
    "    print(f\"Circuit Breaker 상태: {stats['circuit_breaker_states']}\")\n",
    "    print(f\"현재 성공률: {stats['success_rate']:.1f}%\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Circuit Breaker 시뮬레이션 실행\n",
    "final_stats = simulate_failures()\n",
    "\n",
    "print(\"\\n📊 최종 Circuit Breaker 통계:\")\n",
    "print(f\"총 시도: {final_stats['total_calls']}\")\n",
    "print(f\"성공: {final_stats['successful_calls']}\")\n",
    "print(f\"실패: {final_stats['failed_calls']}\")\n",
    "print(f\"성공률: {final_stats['success_rate']:.1f}%\")\n",
    "\n",
    "for model, state in final_stats['circuit_breaker_states'].items():\n",
    "    breaker = cb_test_client.circuit_breakers[model]\n",
    "    print(f\"{model.upper()} Circuit Breaker:\")\n",
    "    print(f\"  상태: {state}\")\n",
    "    print(f\"  실패 횟수: {breaker.failure_count}\")\n",
    "    print(f\"  마지막 실패 시간: {breaker.last_failure_time}\")\n",
    "\n",
    "print(\"\\n✅ Circuit Breaker 테스트 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 통합 Production 시스템\n",
    "\n",
    "앞서 구현한 모든 기법들을 통합한 종합적인 프로덕션 시스템입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 통합 Production 시스템 준비 완료\n"
     ]
    }
   ],
   "source": [
    "class ProductionLLMSystem:\n",
    "    def __init__(self):\n",
    "        # 각 서브시스템 초기화\n",
    "        self.sentiment_system = SentimentBranchingSystem()\n",
    "        self.content_filter = ContentFilterSystem()\n",
    "        self.robust_client = RobustLLMClient(\n",
    "            retry_config=RetryConfig(max_retries=3, strategy=RetryStrategy.EXPONENTIAL_BACKOFF),\n",
    "            circuit_breaker_config=CircuitBreakerConfig(failure_threshold=5)\n",
    "        )\n",
    "        \n",
    "        self.processing_history = []\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def process_query(self, user_input: str, use_openai: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"종합적인 쿼리 처리 파이프라인\"\"\"\n",
    "        start_time = time.time()\n",
    "        processing_id = hashlib.md5(f\"{user_input}{time.time()}\".encode()).hexdigest()[:8]\n",
    "        \n",
    "        result = {\n",
    "            \"processing_id\": processing_id,\n",
    "            \"user_input\": user_input,\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"steps\": [],\n",
    "            \"final_response\": None,\n",
    "            \"model_used\": None,\n",
    "            \"processing_time\": 0,\n",
    "            \"safety_passed\": False,\n",
    "            \"sentiment_detected\": None\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Step 1: 콘텐츠 안전성 검사\n",
    "            self.logger.info(f\"[{processing_id}] Step 1: 콘텐츠 필터링\")\n",
    "            filter_result = self.content_filter.comprehensive_filter(user_input, use_openai=use_openai)\n",
    "            \n",
    "            result[\"steps\"].append({\n",
    "                \"step\": \"content_filtering\",\n",
    "                \"passed\": filter_result.is_safe,\n",
    "                \"details\": {\n",
    "                    \"severity\": filter_result.severity.value,\n",
    "                    \"categories\": [cat.value for cat in filter_result.categories],\n",
    "                    \"confidence\": filter_result.confidence\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            if not filter_result.is_safe:\n",
    "                # 안전하지 않은 콘텐츠의 경우 대체 응답 반환\n",
    "                if use_openai:\n",
    "                    safe_response = self.content_filter.safe_response_generator_openai(user_input, filter_result)\n",
    "                else:\n",
    "                    safe_response = self.content_filter.safe_response_generator_ollama(user_input, filter_result)\n",
    "                \n",
    "                result[\"final_response\"] = safe_response\n",
    "                result[\"safety_passed\"] = False\n",
    "                result[\"processing_time\"] = time.time() - start_time\n",
    "                \n",
    "                self.processing_history.append(result)\n",
    "                return result\n",
    "            \n",
    "            result[\"safety_passed\"] = True\n",
    "            \n",
    "            # Step 2: 감성 분석\n",
    "            self.logger.info(f\"[{processing_id}] Step 2: 감성 분석\")\n",
    "            sentiment_result = self.sentiment_system.process_with_sentiment_branching(user_input, use_openai=use_openai)\n",
    "            \n",
    "            result[\"steps\"].append({\n",
    "                \"step\": \"sentiment_analysis\",\n",
    "                \"passed\": True,\n",
    "                \"details\": sentiment_result[\"sentiment_analysis\"]\n",
    "            })\n",
    "            \n",
    "            result[\"sentiment_detected\"] = sentiment_result[\"sentiment_analysis\"][\"sentiment\"]\n",
    "            \n",
    "            # Step 3: 견고한 응답 생성\n",
    "            self.logger.info(f\"[{processing_id}] Step 3: 견고한 응답 생성\")\n",
    "            \n",
    "            # 감성 분석 결과를 바탕으로 프롬프트 조정\n",
    "            enhanced_prompt = self.create_enhanced_prompt(user_input, sentiment_result)\n",
    "            \n",
    "            try:\n",
    "                if use_openai:\n",
    "                    response = self.robust_client.robust_openai_call(enhanced_prompt)\n",
    "                    model_used = \"gpt-4o-mini\"\n",
    "                else:\n",
    "                    response = self.robust_client.robust_ollama_call(\"llama3.1:8b\", enhanced_prompt)\n",
    "                    model_used = \"llama3.1:8b\"\n",
    "                \n",
    "                result[\"steps\"].append({\n",
    "                    \"step\": \"response_generation\",\n",
    "                    \"passed\": True,\n",
    "                    \"details\": {\"model\": model_used, \"fallback_used\": False}\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                # 폴백 메커니즘 사용\n",
    "                self.logger.warning(f\"[{processing_id}] 기본 모델 실패, 폴백 사용: {e}\")\n",
    "                fallback_result = self.robust_client.fallback_call(enhanced_prompt, preferred_model=\"openai\" if use_openai else \"ollama\")\n",
    "                response = fallback_result[\"response\"]\n",
    "                model_used = fallback_result[\"model_used\"]\n",
    "                \n",
    "                result[\"steps\"].append({\n",
    "                    \"step\": \"response_generation\",\n",
    "                    \"passed\": True,\n",
    "                    \"details\": {\"model\": model_used, \"fallback_used\": True}\n",
    "                })\n",
    "            \n",
    "            result[\"final_response\"] = response\n",
    "            result[\"model_used\"] = model_used\n",
    "            result[\"processing_time\"] = time.time() - start_time\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"[{processing_id}] 처리 중 오류: {e}\")\n",
    "            result[\"final_response\"] = \"죄송합니다. 처리 중 오류가 발생했습니다. 다시 시도해 주세요.\"\n",
    "            result[\"error\"] = str(e)\n",
    "            result[\"processing_time\"] = time.time() - start_time\n",
    "        \n",
    "        self.processing_history.append(result)\n",
    "        return result\n",
    "    \n",
    "    def create_enhanced_prompt(self, user_input: str, sentiment_result: Dict[str, Any]) -> str:\n",
    "        \"\"\"감성 분석 결과를 바탕으로 향상된 프롬프트 생성\"\"\"\n",
    "        sentiment = sentiment_result[\"sentiment_analysis\"][\"sentiment\"]\n",
    "        strategy = sentiment_result[\"response_strategy\"]\n",
    "        \n",
    "        enhanced_prompt = f\"\"\"\n",
    "사용자의 질문에 다음 가이드라인에 따라 응답해주세요:\n",
    "\n",
    "사용자 질문: \"{user_input}\"\n",
    "\n",
    "감지된 감정: {sentiment}\n",
    "응답 톤: {strategy['tone']}\n",
    "접근 방식: {strategy['approach']}\n",
    "스타일: {strategy['style']}\n",
    "\n",
    "위 가이드라인을 따르면서 도움이 되고 정확한 답변을 제공해주세요.\n",
    "\"\"\"\n",
    "        \n",
    "        return enhanced_prompt\n",
    "    \n",
    "    def get_system_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"시스템 전체 통계\"\"\"\n",
    "        total_queries = len(self.processing_history)\n",
    "        if total_queries == 0:\n",
    "            return {\"message\": \"No queries processed yet\"}\n",
    "        \n",
    "        safety_passed = len([h for h in self.processing_history if h[\"safety_passed\"]])\n",
    "        avg_processing_time = sum(h[\"processing_time\"] for h in self.processing_history) / total_queries\n",
    "        \n",
    "        sentiment_distribution = {}\n",
    "        for history in self.processing_history:\n",
    "            sentiment = history.get(\"sentiment_detected\")\n",
    "            if sentiment:\n",
    "                sentiment_distribution[sentiment] = sentiment_distribution.get(sentiment, 0) + 1\n",
    "        \n",
    "        model_usage = {}\n",
    "        for history in self.processing_history:\n",
    "            model = history.get(\"model_used\")\n",
    "            if model:\n",
    "                model_usage[model] = model_usage.get(model, 0) + 1\n",
    "        \n",
    "        robust_client_stats = self.robust_client.get_call_statistics()\n",
    "        \n",
    "        return {\n",
    "            \"total_queries\": total_queries,\n",
    "            \"safety_pass_rate\": (safety_passed / total_queries * 100) if total_queries > 0 else 0,\n",
    "            \"average_processing_time\": avg_processing_time,\n",
    "            \"sentiment_distribution\": sentiment_distribution,\n",
    "            \"model_usage\": model_usage,\n",
    "            \"robust_client_stats\": robust_client_stats\n",
    "        }\n",
    "\n",
    "# 통합 시스템 초기화\n",
    "production_system = ProductionLLMSystem()\n",
    "print(\"✅ 통합 Production 시스템 준비 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 통합 시스템 종합 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[a975073f] Step 1: 콘텐츠 필터링\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Production 시스템 종합 테스트\n",
      "================================================================================\n",
      "\n",
      "[테스트 케이스 1]\n",
      "입력: 파이썬 웹 개발을 시작하려고 하는데 어떤 프레임워크를 추천하시나요?\n",
      "\n",
      "🧠 OpenAI 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "INFO:__main__:[a975073f] Step 2: 감성 분석\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[a975073f] Step 3: 견고한 응답 생성\n",
      "INFO:__main__:OpenAI 호출 시도 1/4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[600d21f2] Step 1: 콘텐츠 필터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: a975073f\n",
      "안전성 통과: ✅\n",
      "감지된 감정: neutral\n",
      "사용 모델: gpt-4o-mini\n",
      "처리 시간: 18.75초\n",
      "\n",
      "처리 단계:\n",
      "  ✅ content_filtering: {'severity': 'low', 'categories': [], 'confidence': 0.5}\n",
      "  ✅ sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  ✅ response_generation: {'model': 'gpt-4o-mini', 'fallback_used': False}\n",
      "\n",
      "최종 응답: 파이썬 웹 개발을 시작하려는 것은 훌륭한 선택입니다! 파이썬은 다양한 웹 프레임워크를 지원하여 개발자들이 필요에 따라 적합한 도구를 선택할 수 있도록 합니다. 다음은 인기 있는 몇 가지 프레임워크와 그 특징을 소개합니다.\n",
      "\n",
      "1. **Django**:\n",
      "   - **특징**: Django는 강력하고 고급 기능을 갖춘 프레임워크로, \"배터리가 포함되어 있다\"는 ...\n",
      "\n",
      "🤖 Ollama 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[600d21f2] Step 2: 감성 분석\n",
      "INFO:__main__:[600d21f2] Step 3: 견고한 응답 생성\n",
      "INFO:__main__:Ollama 호출 시도 1/4\n",
      "INFO:__main__:[211bef66] Step 1: 콘텐츠 필터링\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: 600d21f2\n",
      "안전성 통과: ✅\n",
      "감지된 감정: neutral\n",
      "사용 모델: llama3.1:8b\n",
      "처리 시간: 40.94초\n",
      "\n",
      "처리 단계:\n",
      "  ✅ content_filtering: {'severity': 'low', 'categories': [], 'confidence': 1.0}\n",
      "  ✅ sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.85, 'emotions': {'joy': 0.6, 'fear': 0.2, 'surprise': 0.1, 'anger': 0.05, 'sadness': 0.05}, 'keywords': ['파이썬 웹 개발', '프레임워크']}\n",
      "  ✅ response_generation: {'model': 'llama3.1:8b', 'fallback_used': False}\n",
      "\n",
      "최종 응답: 파이썬 웹 개발의 세계에서 다양한 프레임워크가 있습니다. 각 프레임워크의 강력한 점과 약간의 단점에 대해 살펴보겠습니다.\n",
      "\n",
      "1. **Flask**:\n",
      "   - 장점: 가볍고 유연합니다. \n",
      "   - 단점: 복잡한 프로젝트에는 적합하지 않습니다.\n",
      "   - 추천: 작은 크기의 웹 앱이나 API 개발을 위해 ideal합니다.\n",
      "\n",
      "2. **Django**:\n",
      "   - 장...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 2]\n",
      "입력: 이 바보 같은 에러가 계속 나는데 어떻게 해결해야 하나요?\n",
      "\n",
      "🧠 OpenAI 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[c9cc1de9] Step 1: 콘텐츠 필터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: 211bef66\n",
      "안전성 통과: ❌\n",
      "감지된 감정: None\n",
      "사용 모델: None\n",
      "처리 시간: 6.02초\n",
      "\n",
      "처리 단계:\n",
      "  ❌ content_filtering: {'severity': 'medium', 'categories': ['profanity'], 'confidence': 0.5}\n",
      "\n",
      "최종 응답: Error: Maximum retries exceeded\n",
      "\n",
      "🤖 Ollama 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[a4fcd720] Step 1: 콘텐츠 필터링\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: c9cc1de9\n",
      "안전성 통과: ❌\n",
      "감지된 감정: None\n",
      "사용 모델: None\n",
      "처리 시간: 10.37초\n",
      "\n",
      "처리 단계:\n",
      "  ❌ content_filtering: {'severity': 'medium', 'categories': ['profanity'], 'confidence': 0.95}\n",
      "\n",
      "최종 응답: 부적절한 언어를 피하기 위해, \"에러가 계속 발생한다면 어떻게 해야 하나요?\"와 같은 대안적인 질문을 제시하는 것에서부터 시작해, 에러 해결을 위한 일반적인 방법들을 소개하는 데 초점을 맞출 수 있습니다.\n",
      "\n",
      "이러한 대체 응답은 사용자의 의도된 목적인 에러를 해결하는 방법에 대한 정보를 제공하며, 동시에 안전하고 전문적인 환경을 유지하기 위한 노력을 강조합니...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 3]\n",
      "입력: 머신러닝 공부가 너무 어려워서 포기하고 싶어요...\n",
      "\n",
      "🧠 OpenAI 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "INFO:__main__:[a4fcd720] Step 2: 감성 분석\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[a4fcd720] Step 3: 견고한 응답 생성\n",
      "INFO:__main__:OpenAI 호출 시도 1/4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[6eb991b9] Step 1: 콘텐츠 필터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: a4fcd720\n",
      "안전성 통과: ✅\n",
      "감지된 감정: neutral\n",
      "사용 모델: gpt-4o-mini\n",
      "처리 시간: 16.36초\n",
      "\n",
      "처리 단계:\n",
      "  ✅ content_filtering: {'severity': 'low', 'categories': [], 'confidence': 0.5}\n",
      "  ✅ sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  ✅ response_generation: {'model': 'gpt-4o-mini', 'fallback_used': False}\n",
      "\n",
      "최종 응답: 머신러닝 공부가 어렵게 느껴지는 것은 매우 일반적인 경험입니다. 이 분야는 수학, 통계, 프로그래밍 등 다양한 지식이 요구되기 때문에 처음 접할 때는 특히 힘들 수 있습니다. 그러나 몇 가지 접근 방식을 통해 이 과정을 더 수월하게 할 수 있습니다.\n",
      "\n",
      "1. **기초 다지기**: 머신러닝의 기본 개념을 이해하는 것이 중요합니다. 선형 대수, 미적분학, 확률 ...\n",
      "\n",
      "🤖 Ollama 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[6eb991b9] Step 2: 감성 분석\n",
      "INFO:__main__:[6eb991b9] Step 3: 견고한 응답 생성\n",
      "INFO:__main__:Ollama 호출 시도 1/4\n",
      "INFO:__main__:[2956ea5e] Step 1: 콘텐츠 필터링\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: 6eb991b9\n",
      "안전성 통과: ✅\n",
      "감지된 감정: sad\n",
      "사용 모델: llama3.1:8b\n",
      "처리 시간: 29.16초\n",
      "\n",
      "처리 단계:\n",
      "  ✅ content_filtering: {'severity': 'low', 'categories': [], 'confidence': 1.0}\n",
      "  ✅ sentiment_analysis: {'sentiment': 'sad', 'confidence': 0.95, 'emotions': {'joy': 0.2, 'anger': 0.5, 'sadness': 0.3, 'fear': 0.0, 'surprise': 0.0}, 'keywords': ['공부', '포기']}\n",
      "  ✅ response_generation: {'model': 'llama3.1:8b', 'fallback_used': False}\n",
      "\n",
      "최종 응답: 머신러닝 공부가 너무 어려워서 포기하고 싶으시다면, 먼저 걱정하지 마세요. 많은 사람들은 처음 시작할 때 당황하거나 막막하게 느껴질 수 있습니다. 하지만 도전을 멈추지 않고 계속 해나가는 것이 중요합니다.\n",
      "\n",
      "먼저, 머신러닝은 어려울 수도 있지만, 일종의 문제를 해결하려는 과정이라는 것을 기억해 주세요. 문제가 해결되지 않으면 더 열심히 노력하는 것처럼요!\n",
      "...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 4]\n",
      "입력: 와! 첫 번째 앱을 성공적으로 배포했어요! 다음 단계는 뭔가요?\n",
      "\n",
      "🧠 OpenAI 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "INFO:__main__:[2956ea5e] Step 2: 감성 분석\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[2956ea5e] Step 3: 견고한 응답 생성\n",
      "INFO:__main__:OpenAI 호출 시도 1/4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[dc03772a] Step 1: 콘텐츠 필터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: 2956ea5e\n",
      "안전성 통과: ✅\n",
      "감지된 감정: neutral\n",
      "사용 모델: gpt-4o-mini\n",
      "처리 시간: 14.56초\n",
      "\n",
      "처리 단계:\n",
      "  ✅ content_filtering: {'severity': 'low', 'categories': [], 'confidence': 0.5}\n",
      "  ✅ sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  ✅ response_generation: {'model': 'gpt-4o-mini', 'fallback_used': False}\n",
      "\n",
      "최종 응답: 축하합니다! 첫 번째 앱을 성공적으로 배포한 것은 정말 큰 성과입니다. 이제 다음 단계는 앱의 지속적인 발전과 성공적인 운영을 위해 매우 중요합니다. 다음과 같은 단계들을 고려해 보세요:\n",
      "\n",
      "1. **사용자 피드백 수집**: 사용자 리뷰와 피드백을 통해 앱의 강점과 개선할 점을 파악하세요. 이를 통해 사용자 경험을 향상시킬 수 있습니다.\n",
      "\n",
      "2. **버그 수정...\n",
      "\n",
      "🤖 Ollama 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[dc03772a] Step 2: 감성 분석\n",
      "INFO:__main__:[dc03772a] Step 3: 견고한 응답 생성\n",
      "INFO:__main__:Ollama 호출 시도 1/4\n",
      "INFO:__main__:[1694a818] Step 1: 콘텐츠 필터링\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: dc03772a\n",
      "안전성 통과: ✅\n",
      "감지된 감정: positive\n",
      "사용 모델: llama3.1:8b\n",
      "처리 시간: 23.34초\n",
      "\n",
      "처리 단계:\n",
      "  ✅ content_filtering: {'severity': 'low', 'categories': [], 'confidence': 1.0}\n",
      "  ✅ sentiment_analysis: {'sentiment': 'positive', 'confidence': 0.98, 'emotions': {'joy': 0.95, 'surprise': 0.03}, 'keywords': ['성공', '배포']}\n",
      "  ✅ response_generation: {'model': 'llama3.1:8b', 'fallback_used': False}\n",
      "\n",
      "최종 응답: 축하합니다! 첫 번째 앱을 성공적으로 배포했는 건 대단한 성취입니다!\n",
      "\n",
      "이제 다음 단계로 넘어가보겠습니다. 첫 번째 앱의 성공을 지속시킬 수 있도록 도와드리겠습니다.\n",
      "\n",
      "1.  **사용자 피드백 수집**: 사용자의 의견과 후기를 수집하여 앱을 개선하고 업데이트 할 예정입니다.\n",
      "2.  **앱 분석**: 앱의 성능과 사용자 경험을 분석하여 더 좋은 기능과 콘텐츠...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 5]\n",
      "입력: 씨발 이 코드 진짜 개똥같네, 누가 이딴 걸 만들었어?\n",
      "\n",
      "🧠 OpenAI 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[a887e6c6] Step 1: 콘텐츠 필터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: 1694a818\n",
      "안전성 통과: ❌\n",
      "감지된 감정: None\n",
      "사용 모델: None\n",
      "처리 시간: 6.03초\n",
      "\n",
      "처리 단계:\n",
      "  ❌ content_filtering: {'severity': 'medium', 'categories': ['profanity'], 'confidence': 0.5}\n",
      "\n",
      "최종 응답: Error: Maximum retries exceeded\n",
      "\n",
      "🤖 Ollama 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[e80d0999] Step 1: 콘텐츠 필터링\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: a887e6c6\n",
      "안전성 통과: ❌\n",
      "감지된 감정: None\n",
      "사용 모델: None\n",
      "처리 시간: 7.69초\n",
      "\n",
      "처리 단계:\n",
      "  ❌ content_filtering: {'severity': 'high', 'categories': ['profanity'], 'confidence': 1.0}\n",
      "\n",
      "최종 응답: 대안:\n",
      "\"이 코드는 확실히 개선할 필요가 있습니다. 누가 작성했든 상관없이, 우리 모두 이를 보다 효율적이고 명확하게 만드는 데 참여하실 수 있습니다.\"\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 6]\n",
      "입력: 딥러닝 모델의 성능 최적화 방법에 대해 알려주세요.\n",
      "\n",
      "🧠 OpenAI 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "INFO:__main__:[e80d0999] Step 2: 감성 분석\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[e80d0999] Step 3: 견고한 응답 생성\n",
      "INFO:__main__:OpenAI 호출 시도 1/4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[2736e3e6] Step 1: 콘텐츠 필터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: e80d0999\n",
      "안전성 통과: ✅\n",
      "감지된 감정: neutral\n",
      "사용 모델: gpt-4o-mini\n",
      "처리 시간: 20.75초\n",
      "\n",
      "처리 단계:\n",
      "  ✅ content_filtering: {'severity': 'low', 'categories': [], 'confidence': 0.5}\n",
      "  ✅ sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  ✅ response_generation: {'model': 'gpt-4o-mini', 'fallback_used': False}\n",
      "\n",
      "최종 응답: 딥러닝 모델의 성능을 최적화하는 방법은 여러 가지가 있으며, 각 방법은 문제의 특성과 데이터에 따라 다르게 적용될 수 있습니다. 아래에 몇 가지 주요 방법을 설명하겠습니다.\n",
      "\n",
      "### 1. 데이터 전처리\n",
      "- **정규화 및 표준화**: 입력 데이터를 정규화하거나 표준화하여 모델이 더 빠르고 안정적으로 학습할 수 있도록 합니다.\n",
      "- **데이터 증강**: 이미지나...\n",
      "\n",
      "🤖 Ollama 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[2736e3e6] Step 2: 감성 분석\n",
      "ERROR:__main__:JSON 파싱 실패: 텍스트를 분석한 결과 다음과 같이 요약할 수 있습니다.\n",
      "\n",
      "- 감정 상태: neutral (중립적)\n",
      "- 신뢰도: 1.0\n",
      "- 감성 분석:\n",
      "  - 기쁨: 0.0\n",
      "  - 분노: 0.0\n",
      "  - 슬픔: 0.0\n",
      "\n",
      "키워드는 '딥러닝', '모델의 성능 최적화 방법'입니다. \n",
      "\n",
      "이 텍스트는 일반적으로 중립적인 성격을 가지고 있기 때문에, 감정상태를 neutral로 분석했습니다.\n",
      "INFO:__main__:[2736e3e6] Step 3: 견고한 응답 생성\n",
      "INFO:__main__:Ollama 호출 시도 1/4\n",
      "INFO:__main__:[51a71f36] Step 1: 콘텐츠 필터링\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: 2736e3e6\n",
      "안전성 통과: ✅\n",
      "감지된 감정: neutral\n",
      "사용 모델: llama3.1:8b\n",
      "처리 시간: 42.21초\n",
      "\n",
      "처리 단계:\n",
      "  ✅ content_filtering: {'severity': 'low', 'categories': [], 'confidence': 1.0}\n",
      "  ✅ sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  ✅ response_generation: {'model': 'llama3.1:8b', 'fallback_used': False}\n",
      "\n",
      "최종 응답: 딥러닝 모델의 성능 최적화는 모델이 데이터를 학습하고 예측하는 능력을 향상시키기 위한 과정입니다. 다음과 같은 방법들을 통해 모델의 성능을 최적화할 수 있습니다.\n",
      "\n",
      "1. **모델 아키텍처 조정**: 모델의 구조와 파라미터를 조정하여 모델의 성능을 개선할 수 있습니다. 예를 들어, 레이어의 수도 늘려서 더 많은 정보를 학습하거나, 활성화 함수를 바꾸어 학습 ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 7]\n",
      "입력: 해킹 툴 만드는 방법 가르쳐줘\n",
      "\n",
      "🧠 OpenAI 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[f0c4c2a1] Step 1: 콘텐츠 필터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: 51a71f36\n",
      "안전성 통과: ❌\n",
      "감지된 감정: None\n",
      "사용 모델: None\n",
      "처리 시간: 6.02초\n",
      "\n",
      "처리 단계:\n",
      "  ❌ content_filtering: {'severity': 'critical', 'categories': ['illegal'], 'confidence': 0.5}\n",
      "\n",
      "최종 응답: Error: Maximum retries exceeded\n",
      "\n",
      "🤖 Ollama 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[c1e5380f] Step 1: 콘텐츠 필터링\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: f0c4c2a1\n",
      "안전성 통과: ❌\n",
      "감지된 감정: None\n",
      "사용 모델: None\n",
      "처리 시간: 14.78초\n",
      "\n",
      "처리 단계:\n",
      "  ❌ content_filtering: {'severity': 'critical', 'categories': ['illegal'], 'confidence': 1.0}\n",
      "\n",
      "최종 응답: 사용자는 \"해킹 툴 만드는 방법\"에 대해 질문했습니다. 이 질문은 해킹이라는 범죄적 활동과 관련이 있습니다. 사용자에게 안전하고 법적인 정보를 제공하는 것을 목표로 합니다.\n",
      "\n",
      "대안:\n",
      "- 카테고리: legal\n",
      "- 문제 용어: 보안 강화\n",
      "\n",
      "대체 응답:\n",
      "\n",
      "*   **보안 강화 방법에 대해 알려드리겠습니다**. 보안은 컴퓨터나 네트워크에 대한 방어가 필요합니다. ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[테스트 케이스 8]\n",
      "입력: 데이터베이스 설계할 때 정규화는 왜 필요한가요?\n",
      "\n",
      "🧠 OpenAI 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "INFO:__main__:[c1e5380f] Step 2: 감성 분석\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "ERROR:__main__:JSON 파싱 실패: Error: Maximum retries exceeded\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 1/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 2/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "WARNING:__main__:OpenAI API 호출 실패 (시도 3/3): 'RobustLLMClient' object has no attribute 'chat'\n",
      "INFO:__main__:[c1e5380f] Step 3: 견고한 응답 생성\n",
      "INFO:__main__:OpenAI 호출 시도 1/4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:[ac2aaa67] Step 1: 콘텐츠 필터링\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: c1e5380f\n",
      "안전성 통과: ✅\n",
      "감지된 감정: neutral\n",
      "사용 모델: gpt-4o-mini\n",
      "처리 시간: 16.75초\n",
      "\n",
      "처리 단계:\n",
      "  ✅ content_filtering: {'severity': 'low', 'categories': [], 'confidence': 0.5}\n",
      "  ✅ sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.5, 'emotions': {'neutral': 1.0}, 'keywords': []}\n",
      "  ✅ response_generation: {'model': 'gpt-4o-mini', 'fallback_used': False}\n",
      "\n",
      "최종 응답: 데이터베이스 설계에서 정규화는 매우 중요한 과정으로, 여러 가지 이유로 필요합니다. 정규화는 데이터 중복을 줄이고, 데이터 무결성을 높이며, 업데이트 anomalies를 방지하는 데 기여합니다. 다음은 정규화의 주요 필요성과 이점에 대해 설명합니다.\n",
      "\n",
      "1. **데이터 중복 최소화**: 정규화를 통해 데이터베이스 내에서 동일한 데이터가 여러 번 저장되는 것을...\n",
      "\n",
      "🤖 Ollama 처리 결과:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[ac2aaa67] Step 2: 감성 분석\n",
      "INFO:__main__:[ac2aaa67] Step 3: 견고한 응답 생성\n",
      "INFO:__main__:Ollama 호출 시도 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리 ID: ac2aaa67\n",
      "안전성 통과: ✅\n",
      "감지된 감정: neutral\n",
      "사용 모델: llama3.1:8b\n",
      "처리 시간: 30.50초\n",
      "\n",
      "처리 단계:\n",
      "  ✅ content_filtering: {'severity': 'low', 'categories': [], 'confidence': 1.0}\n",
      "  ✅ sentiment_analysis: {'sentiment': 'neutral', 'confidence': 0.95, 'emotions': {'joy': 0.4, 'anger': 0.2, 'sadness': 0.2, 'fear': 0.1, 'surprise': 0.1}, 'keywords': ['정규화', '데이터베이스 설계']}\n",
      "  ✅ response_generation: {'model': 'llama3.1:8b', 'fallback_used': False}\n",
      "\n",
      "최종 응답: 정규화는 데이터베이스 설계의 중요한 측면 중 하나로, 데이터가 적절하게 저장되고 관리될 수 있도록 돕습니다. 정규화를 통해 다음과 같은 이점이 발생합니다.\n",
      "\n",
      "1.  **데이터 중복 최소화**: 동일한 정보가 여러번 저장되는 것을 피하여 데이터의 일관성을 유지할 수 있습니다.\n",
      "2.  **데이터 종속성 구분**: 관련된 데이터를 독립적으로 관리할 수 있도록 도...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📊 Production 시스템 전체 통계\n",
      "총 처리된 쿼리: 16\n",
      "안전성 통과율: 62.5%\n",
      "평균 처리 시간: 19.01초\n",
      "\n",
      "감정 분포:\n",
      "  neutral: 8회 (50.0%)\n",
      "  sad: 1회 (6.2%)\n",
      "  positive: 1회 (6.2%)\n",
      "\n",
      "모델 사용 분포:\n",
      "  gpt-4o-mini: 5회 (31.2%)\n",
      "  llama3.1:8b: 5회 (31.2%)\n",
      "\n",
      "견고한 클라이언트 통계:\n",
      "  총 호출: 10\n",
      "  성공률: 100.0%\n",
      "  평균 호출 시간: 11.26초\n",
      "  Circuit Breaker 상태: {'ollama': 'closed', 'openai': 'closed'}\n",
      "\n",
      "✅ Production 시스템 종합 테스트 완료\n"
     ]
    }
   ],
   "source": [
    "# 다양한 시나리오의 통합 테스트\n",
    "comprehensive_test_cases = [\n",
    "    \"파이썬 웹 개발을 시작하려고 하는데 어떤 프레임워크를 추천하시나요?\",  # 긍정적 + 안전\n",
    "    \"이 바보 같은 에러가 계속 나는데 어떻게 해결해야 하나요?\",  # 부정적 + 경미한 욕설\n",
    "    \"머신러닝 공부가 너무 어려워서 포기하고 싶어요...\",  # 슬픔 + 안전\n",
    "    \"와! 첫 번째 앱을 성공적으로 배포했어요! 다음 단계는 뭔가요?\",  # 기쁨 + 안전\n",
    "    \"씨발 이 코드 진짜 개똥같네, 누가 이딴 걸 만들었어?\",  # 분노 + 심한 욕설\n",
    "    \"딥러닝 모델의 성능 최적화 방법에 대해 알려주세요.\",  # 중립 + 안전\n",
    "    \"해킹 툴 만드는 방법 가르쳐줘\",  # 불법적 내용\n",
    "    \"데이터베이스 설계할 때 정규화는 왜 필요한가요?\",  # 중립 + 안전\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Production 시스템 종합 테스트\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, test_case in enumerate(comprehensive_test_cases, 1):\n",
    "    print(f\"\\n[테스트 케이스 {i}]\")\n",
    "    print(f\"입력: {test_case}\")\n",
    "    \n",
    "    # OpenAI 우선 처리\n",
    "    if openai_available:\n",
    "        print(\"\\n🧠 OpenAI 처리 결과:\")\n",
    "        openai_result = production_system.process_query(test_case, use_openai=True)\n",
    "        \n",
    "        print(f\"처리 ID: {openai_result['processing_id']}\")\n",
    "        print(f\"안전성 통과: {'✅' if openai_result['safety_passed'] else '❌'}\")\n",
    "        print(f\"감지된 감정: {openai_result.get('sentiment_detected', 'N/A')}\")\n",
    "        print(f\"사용 모델: {openai_result.get('model_used', 'N/A')}\")\n",
    "        print(f\"처리 시간: {openai_result['processing_time']:.2f}초\")\n",
    "        \n",
    "        print(\"\\n처리 단계:\")\n",
    "        for step in openai_result[\"steps\"]:\n",
    "            status = \"✅\" if step[\"passed\"] else \"❌\"\n",
    "            print(f\"  {status} {step['step']}: {step.get('details', {})}\")\n",
    "        \n",
    "        print(f\"\\n최종 응답: {openai_result['final_response'][:200]}{'...' if len(openai_result['final_response']) > 200 else ''}\")\n",
    "    \n",
    "    # Ollama 처리\n",
    "    print(\"\\n🤖 Ollama 처리 결과:\")\n",
    "    ollama_result = production_system.process_query(test_case, use_openai=False)\n",
    "    \n",
    "    print(f\"처리 ID: {ollama_result['processing_id']}\")\n",
    "    print(f\"안전성 통과: {'✅' if ollama_result['safety_passed'] else '❌'}\")\n",
    "    print(f\"감지된 감정: {ollama_result.get('sentiment_detected', 'N/A')}\")\n",
    "    print(f\"사용 모델: {ollama_result.get('model_used', 'N/A')}\")\n",
    "    print(f\"처리 시간: {ollama_result['processing_time']:.2f}초\")\n",
    "    \n",
    "    print(\"\\n처리 단계:\")\n",
    "    for step in ollama_result[\"steps\"]:\n",
    "        status = \"✅\" if step[\"passed\"] else \"❌\"\n",
    "        print(f\"  {status} {step['step']}: {step.get('details', {})}\")\n",
    "    \n",
    "    print(f\"\\n최종 응답: {ollama_result['final_response'][:200]}{'...' if len(ollama_result['final_response']) > 200 else ''}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# 시스템 전체 통계 출력\n",
    "print(\"\\n📊 Production 시스템 전체 통계\")\n",
    "stats = production_system.get_system_statistics()\n",
    "\n",
    "print(f\"총 처리된 쿼리: {stats['total_queries']}\")\n",
    "print(f\"안전성 통과율: {stats['safety_pass_rate']:.1f}%\")\n",
    "print(f\"평균 처리 시간: {stats['average_processing_time']:.2f}초\")\n",
    "\n",
    "print(\"\\n감정 분포:\")\n",
    "for sentiment, count in stats['sentiment_distribution'].items():\n",
    "    percentage = (count / stats['total_queries'] * 100)\n",
    "    print(f\"  {sentiment}: {count}회 ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n모델 사용 분포:\")\n",
    "for model, count in stats['model_usage'].items():\n",
    "    percentage = (count / stats['total_queries'] * 100)\n",
    "    print(f\"  {model}: {count}회 ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n견고한 클라이언트 통계:\")\n",
    "robust_stats = stats['robust_client_stats']\n",
    "print(f\"  총 호출: {robust_stats['total_calls']}\")\n",
    "print(f\"  성공률: {robust_stats['success_rate']:.1f}%\")\n",
    "print(f\"  평균 호출 시간: {robust_stats['average_duration']:.2f}초\")\n",
    "print(f\"  Circuit Breaker 상태: {robust_stats['circuit_breaker_states']}\")\n",
    "\n",
    "print(\"\\n✅ Production 시스템 종합 테스트 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 결론 및 베스트 프랙티스\n",
    "\n",
    "## 주요 학습 내용\n",
    "\n",
    "### 1. 감성 분석 후 분기 (Sentiment Branching)\n",
    "- 사용자의 감정 상태를 분석하여 적절한 응답 전략 선택\n",
    "- 감정별 맞춤형 톤과 접근 방식 적용\n",
    "- 사용자 경험 향상과 만족도 증대\n",
    "\n",
    "### 2. 콘텐츠 필터링 (Content Filtering)\n",
    "- 다단계 안전성 검증 (규칙 기반 + AI 기반)\n",
    "- 금칙어, 개인정보, 위험 콘텐츠 자동 감지\n",
    "- 안전한 대체 응답 자동 생성\n",
    "\n",
    "### 3. 재시도/백오프 (Retry/Backoff)\n",
    "- 다양한 백오프 전략 (지수, 선형, 랜덤 지터)\n",
    "- Circuit Breaker 패턴으로 시스템 보호\n",
    "- 폴백 메커니즘으로 가용성 보장\n",
    "\n",
    "## 프로덕션 환경 적용 시 고려사항\n",
    "\n",
    "### 성능 최적화\n",
    "- 캐싱 메커니즘 구현\n",
    "- 비동기 처리 도입\n",
    "- 데이터베이스 기반 설정 관리\n",
    "\n",
    "### 모니터링 및 로깅\n",
    "- 상세한 메트릭 수집\n",
    "- 알림 시스템 구축\n",
    "- 성능 대시보드 구성\n",
    "\n",
    "### 보안 강화\n",
    "- API 키 보안 관리\n",
    "- 사용자 인증/인가\n",
    "- 데이터 암호화\n",
    "\n",
    "이러한 기법들을 조합하여 안정적이고 사용자 친화적인 AI 시스템을 구축할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajou-llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
