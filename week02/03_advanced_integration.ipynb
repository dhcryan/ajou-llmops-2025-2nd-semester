{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week02 - Advanced Integration Techniques\n",
    "\n",
    "본 노트북은 고급 프롬프트 엔지니어링 통합 기법들을 실습합니다.\n",
    "\n",
    "## 다룰 기법들\n",
    "1. **Function/Tool Calling**: 외부 함수/도구 호출 및 결과 활용\n",
    "2. **Multiple Chains**: 파이프라인 연결 및 단계별 처리\n",
    "3. **Meta-Prompting**: 프롬프트 자체를 진단하고 개선\n",
    "4. **APE (Automatic Prompt Engineering)**: 프롬프트 자동 생성 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라이브러리 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치 및 import\n",
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "from io import StringIO\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI 라이브러리\n",
    "try:\n",
    "    from openai import OpenAI, APIError, RateLimitError\n",
    "except ImportError:\n",
    "    !pip install openai\n",
    "    from openai import OpenAI, APIError, RateLimitError\n",
    "\n",
    "# 설정\n",
    "client = OpenAI()  # 환경변수에서 API 키 자동 로드\n",
    "\n",
    "# 헬퍼 함수들\n",
    "def run_ollama(model: str, prompt: str) -> str:\n",
    "    \"\"\"Ollama 모델 실행\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", model],\n",
    "            input=prompt,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        return result.stdout.strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: Timeout\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def run_openai(prompt: str, model: str = \"gpt-4o-mini\", **kwargs) -> str:\n",
    "    \"\"\"OpenAI 모델 실행\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            **kwargs\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def safe_json_parse(text: str) -> Union[dict, None]:\n",
    "    \"\"\"안전한 JSON 파싱\"\"\"\n",
    "    try:\n",
    "        # 코드 블록 제거\n",
    "        clean_text = re.sub(r'```json\\s*|```\\s*', '', text).strip()\n",
    "        return json.loads(clean_text)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print(\"라이브러리 로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Function/Tool Calling\n",
    "\n",
    "**핵심 아이디어**: LLM이 외부 함수/도구를 \"이름+파라미터\"로 호출 → 실제 결과를 받아 최종 답변에 반영\n",
    "\n",
    "**설계 팁**:\n",
    "- 스키마 명세: `name`, `description`, `parameters(JSON Schema)` 명확히\n",
    "- Idempotency: 재시도 시 중복 실행 방지\n",
    "- 타임아웃/백오프: 툴 실패·지연 대응\n",
    "- 보안: 파라미터 화이트리스트, 출력 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama 툴 호출 시뮬레이션 ===\n",
      "생성된 툴 호출: `{\"name\": \"get_weather\", \"args\": {\"city\": \"Seoul\", \"date\": \"tomorrow\"}}`\n",
      "JSON 파싱 실패\n"
     ]
    }
   ],
   "source": [
    "# 1-1. Ollama 툴 시뮬레이션\n",
    "def simulate_tool_calling_ollama():\n",
    "    \"\"\"Ollama로 툴 호출 시뮬레이션\"\"\"\n",
    "    \n",
    "    # 1) 모델에게 툴 호출 JSON 생성하도록 유도\n",
    "    tool_request_prompt = \"\"\"Return ONLY JSON tool_call: \n",
    "    {\"name\": \"get_weather\", \"args\": {\"city\": \"Seoul\", \"date\": \"tomorrow\"}}\"\"\"\n",
    "    \n",
    "    print(\"=== Ollama 툴 호출 시뮬레이션 ===\")\n",
    "    tool_call_json = run_ollama(\"llama3.1:8b\", tool_request_prompt)\n",
    "    print(f\"생성된 툴 호출: {tool_call_json}\")\n",
    "    \n",
    "    # 2) 더미 툴 함수들 정의\n",
    "    def fake_tools_handler(tool_call: dict) -> dict:\n",
    "        \"\"\"더미 툴 핸들러\"\"\"\n",
    "        name = tool_call.get(\"name\", \"\")\n",
    "        args = tool_call.get(\"args\", {})\n",
    "        \n",
    "        if name == \"get_weather\":\n",
    "            return {\"forecast\": \"Sunny\", \"temp_c\": 27, \"humidity\": \"60%\"}\n",
    "        elif name == \"fx_usd_krw\":\n",
    "            return {\"rate\": 1385.2, \"trend\": \"stable\"}\n",
    "        else:\n",
    "            return {\"error\": \"unknown tool\"}\n",
    "    \n",
    "    # 3) JSON 파싱 후 툴 실행\n",
    "    try:\n",
    "        tool_data = json.loads(tool_call_json)\n",
    "        result = fake_tools_handler(tool_data)\n",
    "        print(f\"툴 실행 결과: {result}\")\n",
    "        \n",
    "        # 4) 결과를 바탕으로 최종 응답 생성\n",
    "        final_prompt = f\"\"\"Based on this tool result: {result}\n",
    "        Write a one-line weather summary in Korean.\"\"\"\n",
    "        \n",
    "        final_response = run_ollama(\"llama3.1:8b\", final_prompt)\n",
    "        print(f\"최종 응답: {final_response}\")\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"JSON 파싱 실패\")\n",
    "\n",
    "simulate_tool_calling_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI Function Calling 데모 ===\n",
      "\n",
      "--- 테스트 1: 서울 날씨 확인해줘 ---\n",
      "도구 호출: get_weather({'city': '서울', 'date': '오늘'})\n",
      "도구 결과: {'forecast': '맑음', 'temp_c': 27, 'humidity': '65%'}\n",
      "최종 응답: 오늘 서울 날씨는 맑음입니다. 기온은 약 27°C, 습도는 65%입니다.  \n",
      "야외 활동하기 좋은 날이니 가벼운 옷차림을 추천드리고, 자외선 차단제는 챙기세요. 우산은 필요하지 않을 것 같습니다.  \n",
      "다른 시간대 예보나 내일 날씨도 알려드릴까요?\n",
      "\n",
      "--- 테스트 2: USD-KRW 환율이랑 25 * 47 계산 결과 알려줘 ---\n",
      "도구 호출: fx_usd_krw({})\n",
      "도구 결과: {'rate': 1385.2, 'change': '+0.5%', 'timestamp': '2024-01-15 09:00'}\n",
      "도구 호출: calculate_math({'expression': '25 * 47'})\n",
      "도구 결과: {'result': 1175, 'expression': '25 * 47'}\n",
      "최종 응답: USD-KRW 환율: 1,385.2원 (변동 +0.5%, 기준 시각 2024-01-15 09:00)  \n",
      "25 × 47 = 1,175\n",
      "\n",
      "더 최신 시세가 필요하시면 알려주세요.\n",
      "\n",
      "--- 테스트 3: 부산 날씨와 현재 환율로 여행 예산 조언해줘 ---\n",
      "도구 호출: get_weather({'city': '부산', 'date': '오늘'})\n",
      "도구 결과: {'error': 'City not found'}\n",
      "도구 호출: fx_usd_krw({})\n",
      "도구 결과: {'rate': 1385.2, 'change': '+0.5%', 'timestamp': '2024-01-15 09:00'}\n",
      "최종 응답: 먼저 알려드린 환율(및 날씨 조회) 상태에 대해 안내드립니다.\n",
      "- 환율(제가 조회한 값): 1 USD = 1,385.2 KRW (출처 타임스탬프: 2024-01-15 09:00). 이 값은 최신이 아닐 수 있으니 실환전 전에 실시간 시세·환전 수수료를 꼭 확인하세요.\n",
      "- 부산 날씨 조회는 시도했지만 현재 도시 조회에 실패했습니다. (다시 시도해도 되고, 여행 날짜를 알려주시면 정확한 예보로 재조회해 드릴게요.)\n",
      "\n",
      "우선 현재 가진 환율로 예산 샘플을 만들어 드립니다. 원하시는 여행 기간·인원·여행 스타일(저예산/중간/고급)을 알려주시면 더 정확히 맞춰 드릴게요.\n",
      "\n",
      "환율 기준: 1 USD = 1,385.2 KRW\n",
      "\n",
      "예산 가이드 (1인 기준, 환율 적용 예시)\n",
      "- 하루당 예상(대략)\n",
      "  - 저예산: 40,000 KRW ≒ 29 USD\n",
      "  - 중간형: 100,000 KRW ≒ 72 USD\n",
      "  - 고급형: 250,000 KRW ≒ 180 USD\n",
      "\n",
      "- 총액 예시\n",
      "  - 3일\n",
      "    - 저예산: 120,000 KRW ≒ 86.6 USD\n",
      "    - 중간형: 300,000 KRW ≒ 216.5 USD\n",
      "    - 고급형: 750,000 KRW ≒ 541.4 USD\n",
      "  - 5일\n",
      "    - 저예산: 200,000 KRW ≒ 144.3 USD\n",
      "    - 중간형: 500,000 KRW ≒ 360.9 USD\n",
      "    - 고급형: 1,250,000 KRW ≒ 901.8 USD\n",
      "\n",
      "예산 구성(권장 배분)\n",
      "- 숙박: 전체 예산의 40~50% (게스트하우스/비즈니스호텔/리조트에 따라 다름)\n",
      "- 식비: 하루 10,000~40,000 KRW (길거리/현지식 vs 레스토랑)\n",
      "- 교통: 시내교통(지하철·버스) 싸고, 공항-도심 이동 비용 별도\n",
      "- 관광·체험: 입장료·투어 비용 예산에 포함\n",
      "- 여유비: 총 예산의 10~20% (환전 수수료·기념품·비상)\n",
      "\n",
      "환전·결제 팁\n",
      "- 카드 사용이 편리(대부분 매장·택시에서 사용 가능). 소액은 현금 권장.\n",
      "- 환전은 공항·시내 환전소·은행 수수료 확인. 해외카드 수수료(먹통 수수료) 고려.\n",
      "- 큰 금액은 카드로 결제하고, 현금은 교통·시장·작은 가게용으로 약간 준비.\n",
      "\n",
      "날씨 관련\n",
      "- 현재(실시간) 부산 날씨를 다시 조회하려면 여행 날짜(예: “이번주 토요일”)를 알려 주세요.\n",
      "- 일반 참고: 9월 초(현재 시스템 날짜 기준 9월)는 부산이 여전히 따뜻하고 습함(낮 23–30°C 범위), 태풍/비가 올 수 있으니 우산·간편한 방수 대비 권장합니다. (정확한 예보는 날짜 지정 후 재조회 필요)\n",
      "\n",
      "원하시면 지금 바로:\n",
      "- 정확한 여행 날짜/기간과 인원 및 여행 스타일을 알려 주세요 → 최신 환율로 다시 확인하고, 해당 날짜의 부산 일기예보(또는 7일간 예보) 조회해 맞춤 예산 다시 계산해 드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# 1-2. OpenAI 실제 Function Calling\n",
    "def openai_function_calling_demo():\n",
    "    \"\"\"OpenAI Function Calling 실제 구현\"\"\"\n",
    "    \n",
    "    # 실제 도구 함수들\n",
    "    def get_weather(city: str, date: str = \"today\") -> dict:\n",
    "        \"\"\"날씨 정보 조회 (더미 데이터)\"\"\"\n",
    "        weather_db = {\n",
    "            \"Seoul\": {\"forecast\": \"Sunny\", \"temp_c\": 27, \"humidity\": \"65%\"},\n",
    "            \"Busan\": {\"forecast\": \"Cloudy\", \"temp_c\": 24, \"humidity\": \"70%\"},\n",
    "            \"서울\": {\"forecast\": \"맑음\", \"temp_c\": 27, \"humidity\": \"65%\"}\n",
    "        }\n",
    "        return weather_db.get(city, {\"error\": \"City not found\"})\n",
    "    \n",
    "    def fx_usd_krw() -> dict:\n",
    "        \"\"\"USD->KRW 환율 조회 (더미 데이터)\"\"\"\n",
    "        return {\"rate\": 1385.2, \"change\": \"+0.5%\", \"timestamp\": \"2024-01-15 09:00\"}\n",
    "    \n",
    "    def calculate_math(expression: str) -> dict:\n",
    "        \"\"\"안전한 수학 계산\"\"\"\n",
    "        try:\n",
    "            if re.match(r'^[0-9+\\-*/().\\s]+$', expression):\n",
    "                result = eval(expression)  # 실제로는 더 안전한 파서 사용 권장\n",
    "                return {\"result\": result, \"expression\": expression}\n",
    "            else:\n",
    "                return {\"error\": \"Unsupported expression\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    # Function Calling 도구 스키마 정의\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"지정된 도시의 날씨 정보를 조회합니다\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"city\": {\"type\": \"string\", \"description\": \"도시 이름\"},\n",
    "                        \"date\": {\"type\": \"string\", \"description\": \"날짜 (오늘/내일/yyyy-mm-dd)\"}\n",
    "                    },\n",
    "                    \"required\": [\"city\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"fx_usd_krw\",\n",
    "                \"description\": \"USD에서 KRW로의 현재 환율을 조회합니다\",\n",
    "                \"parameters\": {\"type\": \"object\", \"properties\": {}}\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\", \n",
    "            \"function\": {\n",
    "                \"name\": \"calculate_math\",\n",
    "                \"description\": \"수학 계산을 수행합니다\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\"type\": \"string\", \"description\": \"계산할 수식\"}\n",
    "                    },\n",
    "                    \"required\": [\"expression\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"=== OpenAI Function Calling 데모 ===\")\n",
    "    \n",
    "    # 테스트 케이스들\n",
    "    test_queries = [\n",
    "        \"서울 날씨 확인해줘\",\n",
    "        \"USD-KRW 환율이랑 25 * 47 계산 결과 알려줘\",\n",
    "        \"부산 날씨와 현재 환율로 여행 예산 조언해줘\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n--- 테스트 {i}: {query} ---\")\n",
    "        \n",
    "        # 첫 번째 요청\n",
    "        messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "        \n",
    "        # 도구 호출 처리\n",
    "        if response.choices[0].message.tool_calls:\n",
    "            messages.append(response.choices[0].message)\n",
    "            \n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                try:\n",
    "                    arguments = json.loads(tool_call.function.arguments)\n",
    "                except:\n",
    "                    arguments = {}\n",
    "                \n",
    "                print(f\"도구 호출: {function_name}({arguments})\")\n",
    "                \n",
    "                # 실제 함수 호출\n",
    "                if function_name == \"get_weather\":\n",
    "                    result = get_weather(**arguments)\n",
    "                elif function_name == \"fx_usd_krw\":\n",
    "                    result = fx_usd_krw()\n",
    "                elif function_name == \"calculate_math\":\n",
    "                    result = calculate_math(arguments.get(\"expression\", \"\"))\n",
    "                else:\n",
    "                    result = {\"error\": \"Unknown function\"}\n",
    "                \n",
    "                print(f\"도구 결과: {result}\")\n",
    "                \n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": json.dumps(result, ensure_ascii=False)\n",
    "                })\n",
    "            \n",
    "            # 최종 응답 생성\n",
    "            final_response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "            )\n",
    "            \n",
    "            print(f\"최종 응답: {final_response.choices[0].message.content}\")\n",
    "        else:\n",
    "            print(f\"도구 호출 없음: {response.choices[0].message.content}\")\n",
    "\n",
    "openai_function_calling_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple Chains\n",
    "\n",
    "**핵심 아이디어**: 파이프라인 연결 - 정제→요약→분류→행동추천\n",
    "\n",
    "**패턴**:\n",
    "- 팬아웃: 동일 입력→여러 모델/프롬프트 병렬\n",
    "- 팬인: 여러 결과→집계/투표\n",
    "\n",
    "**설계 포인트**:\n",
    "- 각 단계의 입출력 스키마 고정(JSON)\n",
    "- 에러 분기(누락 필드→재질문/기본값)\n",
    "- 캐시/재사용: 반복 호출 절감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama Multiple Chains 처리 ===\n",
      "입력: 배송이 너무 늦었고, 포장도 찢어져 왔습니다. 환불 절차 알려주세요. 정말 화가 나네요.\n",
      "\n",
      "[Stage 1] 요약: 배송이 매우늦게 도착했으며 제품의 포장이 찢어져있었습니다. 환불을 처리하려면 어떻게 해야할까요? 정말 기대가 안된 일이었다는 생각에 큰 불편함을 느끼고 있습니다.\n",
      "\n",
      "[Stage 2] 의도 분류: Based on the text, I would classify the intent as:\n",
      "\n",
      "{\"intent\": \"refund\", \"confidence\": 0.9}\n",
      "\n",
      "Here's my reasoning:\n",
      "\n",
      "* The user mentions that the delivery was delayed and the packaging was damaged (\"배송이 너무 늦었고, 포장도 찢어져 왔습니다.\"), which suggests a problem with the product or service.\n",
      "* The user explicitly asks for the refund procedure (\"환불 절차 알려주세요.\"), indicating a clear intent to request a refund.\n",
      "* The tone of the text is negative and frustrated (\"정말 화가 나네요\"), but this doesn't necessarily affect the classification, as it's still focused on requesting a refund.\n",
      "\n",
      "Note that the confidence level is 0.9 because there's no explicit mention of an exchange or any other specific issue, so while it's likely that the user wants a refund, it's not a certainty. However, based on the context and language used, refund is the most likely intent.\n",
      "\n",
      "[Stage 3] 감정 분석: After analyzing the text, I conclude that:\n",
      "\n",
      "* Emotion: Anger\n",
      "* Intensity: High\n",
      "* Keywords: [\"배송\", \"늦었고\", \"포장\", \"찢어져\", \"환불\", \"절차\", \"화가\"]\n",
      "\n",
      "Here's a breakdown of why I chose these results:\n",
      "\n",
      "* The text contains several words and phrases that indicate strong negative emotions, such as:\n",
      "\t+ \"화가 나네요\" (I'm really angry)\n",
      "\t+ \"배송이 너무 늦었고\" (The delivery was too slow)\n",
      "\t+ \"포장도 찢어져 왔습니다\" (The packaging was torn apart)\n",
      "* The intensity of the emotion is High because of the strong language used and the explicit statement of being angry.\n",
      "* The keywords I extracted are related to the issues mentioned in the text, such as delayed delivery, damaged packaging, and a request for a refund.\n",
      "\n",
      "Here's the JSON output:\n",
      "```\n",
      "{\n",
      "  \"emotion\": \"Anger\",\n",
      "  \"intensity\": \"High\",\n",
      "  \"keywords\": [\"배송\", \"늦었고\", \"포장\", \"찢어져\", \"환불\", \"절차\", \"화가\"]\n",
      "}\n",
      "```\n",
      "\n",
      "[Stage 4] 최종 대응: Here's a polite 3-sentence response in Korean:\n",
      "\n",
      "\"죄송합니다. 배송 및 포장에 문제가 발생한 것을 확인했습니다. 환불을 처리하기 위해, 주문 내역과 함께 고객센터로 연락해 주시면 친절하게 안내드리겠습니다.\" (Sorry about the inconvenience. We've confirmed that there was an issue with delivery and packaging. Please contact our customer service with your order details so we can assist you with the refund process.)\n"
     ]
    }
   ],
   "source": [
    "# 2-1. Ollama Multiple Chains (쉘 파이프라인 시뮬레이션)\n",
    "def ollama_multiple_chains():\n",
    "    \"\"\"Ollama를 사용한 다단계 체인 처리\"\"\"\n",
    "    \n",
    "    input_text = \"배송이 너무 늦었고, 포장도 찢어져 왔습니다. 환불 절차 알려주세요. 정말 화가 나네요.\"\n",
    "    \n",
    "    print(\"=== Ollama Multiple Chains 처리 ===\")\n",
    "    print(f\"입력: {input_text}\")\n",
    "    \n",
    "    # Stage 1: 요약 (3문장)\n",
    "    stage1_prompt = f\"Summarize in Korean in 3 sentences: --- {input_text} ---\"\n",
    "    summary = run_ollama(\"llama3.1:8b\", stage1_prompt)\n",
    "    print(f\"\\n[Stage 1] 요약: {summary}\")\n",
    "    \n",
    "    # Stage 2: 의도 분류 (JSON)\n",
    "    stage2_prompt = f\"\"\"Classify intent -> refund|exchange|question. Return JSON {{\"intent\": \"...\", \"confidence\": 0.0-1.0}}.\n",
    "Text: --- {input_text} ---\"\"\"\n",
    "    classification = run_ollama(\"llama3.1:8b\", stage2_prompt)\n",
    "    print(f\"\\n[Stage 2] 의도 분류: {classification}\")\n",
    "    \n",
    "    # Stage 3: 감정 분석\n",
    "    stage3_prompt = f\"\"\"Analyze emotion intensity -> low|medium|high. Return JSON {{\"emotion\": \"...\", \"intensity\": \"...\", \"keywords\": [...]}}.\n",
    "Text: --- {input_text} ---\"\"\"\n",
    "    emotion = run_ollama(\"llama3.1:8b\", stage3_prompt)\n",
    "    print(f\"\\n[Stage 3] 감정 분석: {emotion}\")\n",
    "    \n",
    "    # Stage 4: 통합 대응문 생성\n",
    "    stage4_prompt = f\"\"\"You are a CS agent. Create a response based on:\n",
    "Summary: {summary}\n",
    "Intent: {classification}\n",
    "Emotion: {emotion}\n",
    "\n",
    "Write a polite 3-sentence response in Korean:\n",
    "1) Acknowledge the emotion\n",
    "2) Apologize if needed\n",
    "3) Provide clear next step\"\"\"\n",
    "    \n",
    "    final_response = run_ollama(\"llama3.1:8b\", stage4_prompt)\n",
    "    print(f\"\\n[Stage 4] 최종 대응: {final_response}\")\n",
    "    \n",
    "    return {\n",
    "        \"original\": input_text,\n",
    "        \"summary\": summary,\n",
    "        \"classification\": classification,\n",
    "        \"emotion\": emotion,\n",
    "        \"response\": final_response\n",
    "    }\n",
    "\n",
    "ollama_result = ollama_multiple_chains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI Multiple Chains 처리 ===\n",
      "\n",
      "--- 케이스 1: 배송이 너무 늦었고, 포장도 찢어져 왔습니다. 환불 절차 알려주세요. ---\n",
      "Stage 1 (정규화): ```json\n",
      "{\n",
      "  \"language\": \"Korean\",\n",
      "  \"normalized\": \"배송이 너무 늦었고, 포장도 찢어져 왔습니다. 환불 절차 알려주세요.\",\n",
      "  \"length\": 45\n",
      "}\n",
      "```\n",
      "Stage 2a (감성): ```json\n",
      "{\n",
      "  \"sentiment\": \"negative\",\n",
      "  \"confidence\": 0.95,\n",
      "  \"keywords\": [\"배송\", \"늦었다\", \"포장\", \"찢어짐\", \"환불\", \"절차\"]\n",
      "}\n",
      "```\n",
      "Stage 2b (의도): ```json\n",
      "{\n",
      "  \"intent\": \"refund\",\n",
      "  \"urgency\": \"high\"\n",
      "}\n",
      "```\n",
      "Stage 3 (전략): ```json\n",
      "{\n",
      "  \"strategy\": \"empathize|inform|escalate\",\n",
      "  \"tone\": \"apologetic|urgent\",\n",
      "  \"actions\": [\n",
      "    {\n",
      "      \"action\": \"apologize\",\n",
      "      \"message\": \"고객님, 불편을 드려 정말 죄송합니다. 배송 지연과 포장 손상에 대해 사과드립니다.\"\n",
      "    },\n",
      "    {\n",
      "      \"action\": \"provide_info\",\n",
      "      \"message\": \"환불 절차에 대한 자세한 정보를 드리겠습니다. 환불 요청을 위해 고객님께서는 아래 링크를 통해 양식을 작성해 주시기 바랍니다.\"\n",
      "    },\n",
      "    {\n",
      "      \"action\": \"escalate\",\n",
      "      \"message\": \"이 문제를 신속하게 해결하기 위해 고객 서비스 팀에 이 사례를 전달하겠습니다.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Stage 4 (최종응답): 고객님, 불편을 드려 정말 죄송합니다. 배송 지연과 포장 손상에 대해 사과드립니다. 환불 요청을 위해 아래 링크를 통해 양식을 작성해 주시면, 이 문제를 신속하게 해결하기 위해 고객 서비스 팀에 전달하겠습니다. 감사합니다.\n",
      "\n",
      "--- 케이스 2: 제품이 정말 좋네요! 다음에도 주문하고 싶어요. 할인 혜택 있나요? ---\n",
      "Stage 1 (정규화): ```json\n",
      "{\n",
      "  \"language\": \"Korean\",\n",
      "  \"normalized\": \"제품이 정말 좋네요! 다음에도 주문하고 싶어요. 할인 혜택 있나요?\",\n",
      "  \"length\": 51\n",
      "}\n",
      "```\n",
      "Stage 2a (감성): {\n",
      "  \"sentiment\": \"positive\",\n",
      "  \"confidence\": 0.95,\n",
      "  \"keywords\": [\"제품\", \"좋네요\", \"주문\", \"할인\", \"혜택\"]\n",
      "}\n",
      "Stage 2b (의도): ```json\n",
      "{\n",
      "  \"intent\": \"compliment\",\n",
      "  \"urgency\": \"low\"\n",
      "}\n",
      "```\n",
      "Stage 3 (전략): ```json\n",
      "{\n",
      "  \"strategy\": \"empathize|celebrate|inform\",\n",
      "  \"tone\": \"friendly\",\n",
      "  \"actions\": [\n",
      "    {\n",
      "      \"action\": \"thank_customer\",\n",
      "      \"message\": \"제품에 대한 긍정적인 피드백 감사합니다! 저희 제품을 좋아해 주셔서 매우 기쁩니다.\"\n",
      "    },\n",
      "    {\n",
      "      \"action\": \"provide_discount_info\",\n",
      "      \"message\": \"현재 진행 중인 할인 혜택에 대해 안내해 드릴게요. 다음 주문 시 적용할 수 있는 할인 정보는 여기에서 확인하실 수 있습니다.\"\n",
      "    },\n",
      "    {\n",
      "      \"action\": \"invite_next_order\",\n",
      "      \"message\": \"주문해 주셔서 감사합니다! 다음 주문도 기대하고 있습니다.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Stage 4 (최종응답): 제품에 대한 긍정적인 피드백 감사합니다! 저희 제품을 좋아해 주셔서 매우 기쁩니다. 현재 진행 중인 할인 혜택에 대해서는 다음 주문 시 적용할 수 있는 정보를 확인하실 수 있으니, 꼭 참고해 주세요. 다음 주문도 기대하고 있습니다!\n",
      "\n",
      "--- 케이스 3: 사이즈가 안 맞아서 교환하고 싶은데, 어떻게 해야 하나요? ---\n",
      "Stage 1 (정규화): ```json\n",
      "{\n",
      "  \"language\": \"Korean\",\n",
      "  \"normalized\": \"사이즈가 안 맞아서 교환하고 싶은데, 어떻게 해야 하나요?\",\n",
      "  \"length\": 44\n",
      "}\n",
      "```\n",
      "Stage 2a (감성): {\n",
      "  \"sentiment\": \"neutral\",\n",
      "  \"confidence\": 0.85,\n",
      "  \"keywords\": [\"사이즈\", \"교환\", \"어떻게\"]\n",
      "}\n",
      "Stage 2b (의도): ```json\n",
      "{\n",
      "  \"intent\": \"exchange\",\n",
      "  \"urgency\": \"medium\"\n",
      "}\n",
      "```\n",
      "Stage 3 (전략): ```json\n",
      "{\n",
      "  \"strategy\": \"empathize|inform\",\n",
      "  \"tone\": \"friendly|apologetic\",\n",
      "  \"actions\": [\n",
      "    \"Acknowledge the customer's concern about the size issue.\",\n",
      "    \"Provide clear instructions on how to initiate the exchange process.\",\n",
      "    \"Offer assistance in selecting the right size if needed.\",\n",
      "    \"Thank the customer for reaching out and encourage them to ask any further questions.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Stage 4 (최종응답): 안녕하세요, 고객님. 사이즈 문제로 불편을 드려 정말 죄송합니다. 교환을 원하신다면, 저희 웹사이트에서 '교환 요청'을 클릭하신 후, 필요한 사이즈를 선택해주시면 됩니다. 추가적으로 올바른 사이즈 선택에 도움이 필요하시면 언제든지 문의해 주세요. 감사합니다!\n"
     ]
    }
   ],
   "source": [
    "# 2-2. OpenAI Multiple Chains (오케스트레이터 패턴)\n",
    "def openai_multiple_chains():\n",
    "    \"\"\"OpenAI를 사용한 고급 다단계 체인 처리\"\"\"\n",
    "    \n",
    "    def chat_stage(prompt: str, **kwargs):\n",
    "        \"\"\"단일 스테이지 실행\"\"\"\n",
    "        return client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            **kwargs\n",
    "        ).choices[0].message.content\n",
    "    \n",
    "    # 테스트 케이스들\n",
    "    test_cases = [\n",
    "        \"배송이 너무 늦었고, 포장도 찢어져 왔습니다. 환불 절차 알려주세요.\",\n",
    "        \"제품이 정말 좋네요! 다음에도 주문하고 싶어요. 할인 혜택 있나요?\",\n",
    "        \"사이즈가 안 맞아서 교환하고 싶은데, 어떻게 해야 하나요?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== OpenAI Multiple Chains 처리 ===\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, text in enumerate(test_cases, 1):\n",
    "        print(f\"\\n--- 케이스 {i}: {text} ---\")\n",
    "        \n",
    "        # Stage 1: 언어 감지 및 정규화\n",
    "        stage1 = chat_stage(\n",
    "            f\"Detect language and normalize text. Return JSON {{\\\"language\\\": \\\"...\\\", \\\"normalized\\\": \\\"...\\\", \\\"length\\\": number}}.\\nText: --- {text} ---\",\n",
    "            max_completion_tokens=150\n",
    "        )\n",
    "        print(f\"Stage 1 (정규화): {stage1}\")\n",
    "        \n",
    "        # Stage 2: 병렬 분석 (감성 + 의도)\n",
    "        stage2a = chat_stage(\n",
    "            f\"Sentiment analysis. Return JSON {{\\\"sentiment\\\": \\\"positive|negative|neutral\\\", \\\"confidence\\\": 0.0-1.0, \\\"keywords\\\": [...]}}.\\nText: --- {text} ---\",\n",
    "            max_completion_tokens=120\n",
    "        )\n",
    "        \n",
    "        stage2b = chat_stage(\n",
    "            f\"Intent classification. Return JSON {{\\\"intent\\\": \\\"refund|exchange|question|compliment\\\", \\\"urgency\\\": \\\"low|medium|high\\\"}}.\\nText: --- {text} ---\",\n",
    "            max_completion_tokens=100\n",
    "        )\n",
    "        \n",
    "        print(f\"Stage 2a (감성): {stage2a}\")\n",
    "        print(f\"Stage 2b (의도): {stage2b}\")\n",
    "        \n",
    "        # Stage 3: 통합 분석 및 전략 수립\n",
    "        stage3 = chat_stage(\n",
    "            f\"\"\"Based on the analysis results, create a response strategy.\n",
    "Normalization: {stage1}\n",
    "Sentiment: {stage2a}\n",
    "Intent: {stage2b}\n",
    "\n",
    "Return JSON:\n",
    "{{\"strategy\": \"empathize|celebrate|inform|escalate\", \"tone\": \"apologetic|friendly|neutral|urgent\", \"actions\": [...]}}\"\"\",\n",
    "            max_completion_tokens=200\n",
    "        )\n",
    "        print(f\"Stage 3 (전략): {stage3}\")\n",
    "        \n",
    "        # Stage 4: 최종 응답 생성\n",
    "        stage4 = chat_stage(\n",
    "            f\"\"\"Generate final customer service response in Korean based on:\n",
    "Strategy: {stage3}\n",
    "Original message: {text}\n",
    "\n",
    "Requirements:\n",
    "- 2-3 sentences\n",
    "- Professional and empathetic tone\n",
    "- Include specific next steps\n",
    "- Match the determined strategy and tone\"\"\",\n",
    "            max_completion_tokens=200\n",
    "        )\n",
    "        print(f\"Stage 4 (최종응답): {stage4}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"input\": text,\n",
    "            \"normalization\": stage1,\n",
    "            \"sentiment\": stage2a,\n",
    "            \"intent\": stage2b,\n",
    "            \"strategy\": stage3,\n",
    "            \"response\": stage4\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "openai_results = openai_multiple_chains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fan-out/Fan-in 패턴 ===\n",
      "입력: 이 제품은 가격 대비 성능이 좋지만, 디자인이 조금 아쉬워요. 전반적으로는 만족합니다.\n",
      "\n",
      "[quality_focus]: \n",
      "\n",
      "[price_focus]: \n",
      "\n",
      "[design_focus]: \n",
      "\n",
      "[satisfaction_focus]: \n",
      "\n",
      "[통합 결과]: \n"
     ]
    }
   ],
   "source": [
    "# 2-3. Fan-out/Fan-in 패턴 (병렬 처리 + 결과 집계)\n",
    "def fan_out_fan_in_pattern():\n",
    "    \"\"\"팬아웃/팬인 패턴으로 다중 관점 분석\"\"\"\n",
    "    \n",
    "    input_text = \"이 제품은 가격 대비 성능이 좋지만, 디자인이 조금 아쉬워요. 전반적으로는 만족합니다.\"\n",
    "    \n",
    "    print(\"=== Fan-out/Fan-in 패턴 ===\")\n",
    "    print(f\"입력: {input_text}\")\n",
    "    \n",
    "    # Fan-out: 동일 입력을 여러 관점으로 분석\n",
    "    perspectives = {\n",
    "        \"quality_focus\": \"Analyze from quality perspective. Focus on performance, durability, reliability.\",\n",
    "        \"price_focus\": \"Analyze from price/value perspective. Focus on cost-effectiveness, worth.\",\n",
    "        \"design_focus\": \"Analyze from design/UX perspective. Focus on aesthetics, usability.\",\n",
    "        \"satisfaction_focus\": \"Analyze from overall satisfaction perspective. Focus on recommendation likelihood.\"\n",
    "    }\n",
    "    \n",
    "    fan_out_results = {}\n",
    "    \n",
    "    for perspective_name, instruction in perspectives.items():\n",
    "        prompt = f\"\"\"{instruction}\n",
    "        \n",
    "Return JSON: {{\"aspect\": \"{perspective_name}\", \"rating\": 1-5, \"comment\": \"...\", \"keywords\": [...]}}\n",
    "\n",
    "Review: --- {input_text} ---\"\"\"\n",
    "        \n",
    "        result = run_openai(prompt, max_completion_tokens=150)\n",
    "        fan_out_results[perspective_name] = result\n",
    "        print(f\"\\n[{perspective_name}]: {result}\")\n",
    "    \n",
    "    # Fan-in: 여러 관점 결과를 통합\n",
    "    fan_in_prompt = f\"\"\"Aggregate these perspective analyses into a final assessment:\n",
    "    \n",
    "{json.dumps(fan_out_results, indent=2, ensure_ascii=False)}\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"overall_rating\": 1-5,\n",
    "  \"key_strengths\": [...],\n",
    "  \"key_weaknesses\": [...],\n",
    "  \"recommendation\": \"...\",\n",
    "  \"confidence\": 0.0-1.0\n",
    "}}\"\"\"\n",
    "    \n",
    "    final_result = run_openai(fan_in_prompt, max_completion_tokens=250)\n",
    "    print(f\"\\n[통합 결과]: {final_result}\")\n",
    "    \n",
    "    return {\n",
    "        \"input\": input_text,\n",
    "        \"fan_out\": fan_out_results,\n",
    "        \"fan_in\": final_result\n",
    "    }\n",
    "\n",
    "fan_result = fan_out_fan_in_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Meta-Prompting\n",
    "\n",
    "**핵심 아이디어**: 모델이 프롬프트 자체를 진단/개선\n",
    "\n",
    "**절차**:\n",
    "1. 초기 프롬프트 제시\n",
    "2. 개선 포인트 도출\n",
    "3. 개선안 제시\n",
    "4. 개선안으로 실행\n",
    "\n",
    "**체크리스트**:\n",
    "- 목표/산출형식/제약/예시/실패 시 재질의 포함 여부\n",
    "- 금칙어, 길이 제한, 스키마/포맷, 평가 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama Meta-Prompting ===\n",
      "초기 프롬프트: Classify sentiment of a review.\n",
      "\n",
      "개선된 프롬프트:\n",
      "Here is the improved prompt:\n",
      "\n",
      "--- Classify sentiment of a review as either positive, negative, or neutral, and provide a corresponding confidence score (on a scale of 0-1).\n",
      "\n",
      "**Output Format:**\n",
      "\n",
      "* Return a JSON object with the following fields:\n",
      "\t+ `sentiment`: string (positive, negative, or neutral)\n",
      "\t+ `confidence`: float (confidence score between 0 and 1)\n",
      "\t+ `error`: string (optional, describing any error that occurred during processing)\n",
      "\n",
      "**Constraints:**\n",
      "\n",
      "* The input review text must be at least 10 characters long and not exceed 500 characters.\n",
      "* The response length must not exceed 1024 bytes.\n",
      "\n",
      "**Error Handling Guidelines:**\n",
      "\n",
      "* If the input review is empty or exceeds the character limit, return an `error` field with a descriptive message.\n",
      "* If there is any issue with processing the review (e.g., network error), return an `error` field with a generic error message.\n",
      "\n",
      "**Edge Case Handling:**\n",
      "\n",
      "* Handle cases where the sentiment cannot be determined with high confidence (e.g., ambiguous or neutral text).\n",
      "* Consider using a fallback classification method in case of errors or unexpected input.\n",
      "\n",
      "=== 개선된 프롬프트 테스트 ===\n",
      "\n",
      "테스트 1 (포장이 엉망이었고 배송도 늦었습니다.):\n",
      "결과: Based on the input review, here is the classified sentiment and corresponding confidence score:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"sentiment\": \"negative\",\n",
      "  \"confidence\": 0.8,\n",
      "  \"error\": null\n",
      "}\n",
      "```\n",
      "\n",
      "Here's my reasoning behind the classification:\n",
      "\n",
      "* The review contains negative phrases such as \"\" (포장이 엉망이었고, which means \"the packaging was a mess\") and \"\" (배송도 늦었습니다, which means \"and delivery was also delayed\").\n",
      "* Although there is no explicit positive phrase in the review, the tone of the negative phrases suggests that the sentiment is indeed negative.\n",
      "* The confidence score is 0.8 because while the review is clearly negative, it's not extremely extreme or emotive, suggesting a moderate level of certainty.\n",
      "\n",
      "Note that the error field is null since there are no issues with processing the input review.\n",
      "JSON 유효성: False\n",
      "\n",
      "테스트 2 (제품은 괜찮은데 가격이 좀 비싸요.):\n",
      "결과: Here is the classified sentiment and corresponding confidence score:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"sentiment\": \"negative\",\n",
      "  \"confidence\": 0.8,\n",
      "  \"error\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The review text contains a negative comment about the price of the product (\"가격이 좀 비싸요\"), which indicates that the overall sentiment is negative.\n",
      "* The confidence score is set to 0.8, as the text is not overwhelmingly negative, but still expresses a critical opinion.\n",
      "* There is no error message, indicating that the input review text meets the constraints and can be processed successfully.\n",
      "\n",
      "**Note:**\n",
      "\n",
      "* The model used for sentiment analysis may have different confidence threshold values, which could affect the classification results. However, based on the provided text, a negative sentiment with moderate confidence (0.8) seems to be a reasonable classification.\n",
      "* It's worth noting that the review also mentions that the product itself is \"괜찮은데\" (fair/okay), which suggests that it has some positive aspects, but the price is the main issue. This nuance may not be fully captured by the sentiment analysis model used here.\n",
      "JSON 유효성: False\n",
      "\n",
      "테스트 3 (완벽합니다! 추천해요!):\n",
      "결과: Here is the classification result:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"sentiment\": \"positive\",\n",
      "  \"confidence\": 0.95,\n",
      "  \"error\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "The review text `--- 완벽합니다! 추천해요! ---` is a Korean sentence that translates to \"It's perfect! I recommend it!\". The sentiment of this review is clearly positive, indicating that the reviewer had a very favorable experience with the product or service. The confidence score of 0.95 reflects the high degree of certainty in this classification.\n",
      "\n",
      "**Note:**\n",
      "\n",
      "* The review text is within the allowed length limit and contains at least 10 characters.\n",
      "* No errors occurred during processing, so no error message is returned.\n",
      "* The sentiment classification is based on a combination of natural language processing (NLP) techniques, including part-of-speech tagging, named entity recognition, and sentiment analysis. In this case, the positive sentiment is evident from the presence of the words `완벽합니다` (\"It's perfect\") and `추천해요` (\"I recommend it\").\n",
      "JSON 유효성: False\n"
     ]
    }
   ],
   "source": [
    "# 3-1. Ollama Meta-Prompting\n",
    "def ollama_meta_prompting():\n",
    "    \"\"\"Ollama를 사용한 메타 프롬프팅\"\"\"\n",
    "    \n",
    "    initial_prompt = \"Classify sentiment of a review.\"\n",
    "    \n",
    "    print(\"=== Ollama Meta-Prompting ===\")\n",
    "    print(f\"초기 프롬프트: {initial_prompt}\")\n",
    "    \n",
    "    # 1) 개선 요청\n",
    "    improvement_request = f\"\"\"Improve this prompt for reliability & JSON output. \n",
    "Add schema, constraints, and clarifying questions rule:\n",
    "\n",
    "--- {initial_prompt} ---\n",
    "\n",
    "Make it robust for production use with:\n",
    "1. Clear output format specification\n",
    "2. Error handling guidelines  \n",
    "3. Constraints on response length\n",
    "4. Confidence scoring\n",
    "5. Edge case handling\n",
    "\n",
    "Return the improved prompt only.\"\"\"\n",
    "    \n",
    "    improved_prompt = run_ollama(\"llama3.1:8b\", improvement_request)\n",
    "    print(f\"\\n개선된 프롬프트:\\n{improved_prompt}\")\n",
    "    \n",
    "    # 2) 개선된 프롬프트로 실제 테스트\n",
    "    test_reviews = [\n",
    "        \"포장이 엉망이었고 배송도 늦었습니다.\",\n",
    "        \"제품은 괜찮은데 가격이 좀 비싸요.\",\n",
    "        \"완벽합니다! 추천해요!\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== 개선된 프롬프트 테스트 ===\")\n",
    "    \n",
    "    for i, review in enumerate(test_reviews, 1):\n",
    "        test_prompt = f\"\"\"{improved_prompt}\n",
    "\n",
    "Review: --- {review} ---\"\"\"\n",
    "        \n",
    "        result = run_ollama(\"llama3.1:8b\", test_prompt)\n",
    "        print(f\"\\n테스트 {i} ({review}):\")\n",
    "        print(f\"결과: {result}\")\n",
    "        \n",
    "        # JSON 파싱 가능성 체크\n",
    "        is_valid_json = safe_json_parse(result) is not None\n",
    "        print(f\"JSON 유효성: {is_valid_json}\")\n",
    "    \n",
    "    return improved_prompt\n",
    "\n",
    "improved_ollama = ollama_meta_prompting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI Meta-Prompting ===\n",
      "\n",
      "--- 프롬프트 1 개선 ---\n",
      "초기: Classify sentiment of a review.\n",
      "\n",
      "분석 결과: Here's the analysis of the prompt \"Classify sentiment of a review\" along with the corresponding JSON response outlining the weaknesses and improvement plan.\n",
      "\n",
      "### Analysis of the Prompt\n",
      "\n",
      "1. **Clarity Issues**:\n",
      "   - The prompt does not specify what types of reviews are being classified (e.g., product reviews, service reviews, etc.).\n",
      "   - It lacks guidance on the sentiment categories (e.g., positive, negative, neutral) to be used in the classification.\n",
      "\n",
      "2. **Missing Constraints**:\n",
      "   - No character limit or length constraints for the reviews are defined, which could lead to inconsistent classification results.\n",
      "   - There are no requirements regarding the language or tone of the reviews.\n",
      "\n",
      "3. **Output Format Problems**:\n",
      "   - The output format for the classification result is unspecified (e.g., whether it should be a text label, numerical score, or structured JSON).\n",
      "\n",
      "4. **Edge Case Handling**:\n",
      "   - The prompt does not address how to handle ambiguous or mixed-sentiment reviews.\n",
      "   - It does not consider potential issues with sarcasm, irony, or cultural nuances in sentiment interpretation.\n",
      "\n",
      "5. **Scalability Concerns**:\n",
      "   - There is no indication of how to manage a large volume of reviews, particularly if the classification needs to be processed in real-time.\n",
      "   - The prompt does not discuss the potential need for training on domain-specific language or varying contexts over time.\n",
      "\n",
      "### JSON Response\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"weaknesses\": [\n",
      "    \"Un\n",
      "\n",
      "개선됨: **Improved Prompt:**\n",
      "\n",
      "You are tasked with classifying the sentiment of a review. The review may pertain to any type of product or service (e.g., electronics, restaurants, etc.). Please follow these guidelines to ensure accurate and consistent sentiment classification:\n",
      "\n",
      "**Input Specifications:**\n",
      "- Review Text: [String] (The review text should be provided in English and should not exceed 500 characters.)\n",
      "  \n",
      "**Sentiment Categories:**\n",
      "- Please classify the sentiment of the review into one of the following categories:\n",
      "  - Positive\n",
      "  - Negative\n",
      "  - Neutral\n",
      "  - Mixed (if the review contains sentiments from multiple categories)\n",
      "\n",
      "**Output Format:**\n",
      "You must return a JSON object with the following schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"sentiment\": \"string\",           // One of: \"Positive\", \"Negative\", \"Neutral\", \"Mixed\"\n",
      "  \"confidence_score\": \"number\",    // A floating-point number between 0 and 1 representing the confidence in the classification\n",
      "  \"original_review\": \"string\"       // The original review text provided for classification\n",
      "}\n",
      "```\n",
      "\n",
      "**Error Handling:**\n",
      "- If the review is empty or exceeds the specified length, return the following JSON object:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"error\": \"Invalid input. Please ensure the review is a non-empty string of 500 characters or fewer.\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Edge Case Considerations:**\n",
      "- When encountering sarcasm, irony, or ambiguous sentiments, report this in the output as \"Mixed\" and provide a confidence score less than 0.5 if clarity is compromised.\n",
      "  \n",
      "Please process the input review and return the structured JSON output as specified.\n",
      "\n",
      "--- 프롬프트 2 개선 ---\n",
      "초기: Summarize this text.\n",
      "\n",
      "분석 결과: Here's the analysis of the prompt \"Summarize this text\" along with an improvement plan presented in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"weaknesses\": [\n",
      "    \"Clarity issues: The prompt lacks specificity regarding which text is to be summarized.\",\n",
      "    \"Missing constraints: No guidelines on summary length, depth, or key points to focus on.\",\n",
      "    \"Output format problems: The expected format of the summary (bullet points, paragraph, etc.) is not specified.\",\n",
      "    \"Edge case handling: There are no considerations for edge cases such as highly technical texts, creative works, or normal summaries of exceptionally long texts.\",\n",
      "    \"Scalability concerns: The prompt does not address how to handle varying text lengths or complexities and how the summarizer should adjust to them.\"\n",
      "  ],\n",
      "  \"improvement_plan\": [\n",
      "    \"Add specificity by clearly stating what text needs to be summarized (e.g., 'Summarize the following passage: [insert passage]').\",\n",
      "    \"Introduce constraints regarding the length of the summary (e.g., 'in 3-5 sentences' or 'in under 150 words').\",\n",
      "    \"Specify the desired output format (e.g., 'Provide the summary as a single paragraph' or 'List main ideas as bullet points').\",\n",
      "    \"Include guidance for handling edge cases, such as special instructions for technical or narrative texts.\",\n",
      "    \"Create a more robust prompt that allows for varying levels of detail based on text complexity, potentially with tiered guidelines (\n",
      "\n",
      "개선됨: ```plaintext\n",
      "Please summarize the following text: [insert text here]. Your summary should adhere to the following guidelines:\n",
      "\n",
      "- **Length**: The summary must be between 3 to 5 sentences or under 150 words.\n",
      "- **Format**: Provide the summary as either a single paragraph or as a list of main ideas in bullet points.\n",
      "- **Content Focus**: Highlight the key points, main arguments, and any critical conclusions.\n",
      "- **Edge Case Handling**: For highly technical texts, emphasize the core concepts without jargon. For creative works, focus on themes and character development. If the text is exceptionally long, prioritize summarizing the main sections.\n",
      "  \n",
      "Additionally, please include a confidence score (1-10) reflecting how certain you are of the accuracy and completeness of your summary.\n",
      "\n",
      "**JSON Schema for Output**:\n",
      "```json\n",
      "{\n",
      "  \"summary\": \"string\",      // The summary text\n",
      "  \"format\": \"string\",      // \"paragraph\" or \"bullet points\"\n",
      "  \"confidence_score\": 1-10 // Your confidence score\n",
      "}\n",
      "```\n",
      "\n",
      "**Error Handling Instructions**: If the text input is empty or significantly exceeds 1500 words, or if it contains unclear or ambiguous content, please return an error message indicating the issue. For example: 'Error: Input text is empty.' or 'Error: Text exceeds word limit.'\n",
      "```\n",
      "\n",
      "--- 프롬프트 3 개선 ---\n",
      "초기: Answer the question about the document.\n",
      "\n",
      "분석 결과: Here's an analysis of the given prompt that identifies its weaknesses and outlines an improvement plan.\n",
      "\n",
      "### Analysis\n",
      "\n",
      "1. **Clarity Issues**: The prompt is vague and does not specify which question needs to be answered or provide context about the document. This ambiguity can lead to misunderstanding and incorrect responses.\n",
      "\n",
      "2. **Missing Constraints**: There are no constraints provided on the type of question, the expected depth of the answer, or any specific aspects of the document that should be focused on, making it open to interpretation.\n",
      "\n",
      "3. **Output Format Problems**: The prompt does not define the required format of the response (e.g., short answer, detailed explanation, bullet points, etc.), which could lead to diverse and inconsistent outputs.\n",
      "\n",
      "4. **Edge Case Handling**: The prompt does not take into account scenarios where the document might contain multiple questions or where the question may not directly relate to the content of the document, leading to confusion.\n",
      "\n",
      "5. **Scalability Concerns**: If the prompt is to be used for various types of documents (e.g., academic papers, legal documents, articles), it lacks adaptability to handle the specific nuances and types of questions that arise from different contexts.\n",
      "\n",
      "### JSON Response\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"weaknesses\": [\n",
      "    \"Vague wording leading to ambiguity.\",\n",
      "    \"No specification on which question to answer.\",\n",
      "    \"Lack of constraints on the answer's depth or focus.\",\n",
      "    \"Undefined output format leading to inconsistencies.\",\n",
      "    \"\n",
      "\n",
      "개선됨: **Improved Prompt:**\n",
      "\n",
      "```\n",
      "You are tasked with analyzing the provided document and answering a specific question. Follow the guidelines below to ensure clarity and consistency in your response:\n",
      "\n",
      "### Document Analysis Prompt\n",
      "\n",
      "1. **Document**: [Insert the document text here]\n",
      "2. **Question**: [Insert precise question that needs to be answered]\n",
      "\n",
      "### Response Specifications\n",
      "\n",
      "- **Output Format**: Provide your response in a structured format.\n",
      "  - Use bullet points for key points.\n",
      "  - Include a brief explanation (2-3 sentences).\n",
      "  \n",
      "- **Length Constraints**: \n",
      "  - Bullet points should not exceed 100 characters each.\n",
      "  - The explanation should be between 50 to 150 words.\n",
      "\n",
      "### JSON Schema for Response\n",
      "```json\n",
      "{\n",
      "  \"key_points\": [\n",
      "    \"string\"\n",
      "  ],\n",
      "  \"explanation\": \"string\",\n",
      "  \"confidence_score\": \"number ranging from 0 to 1\"\n",
      "}\n",
      "```\n",
      "\n",
      "### Error Handling Instructions\n",
      "- If the question is unclear or ambiguous, respond with:\n",
      "  ```json\n",
      "  {\n",
      "    \"error\": \"The question is unclear. Please provide a more specific question.\"\n",
      "  }\n",
      "  ```\n",
      "- If the document does not relate to the question:\n",
      "  ```json\n",
      "  {\n",
      "    \"error\": \"The question does not pertain to the contents of the document.\"\n",
      "  }\n",
      "  ```\n",
      "\n",
      "### Edge Case Handling\n",
      "- If multiple questions are present in the document, prioritize answering the first question stated.\n",
      "\n",
      "### Confidence Scoring\n",
      "- Provide a confidence score (0 to 1) representing how assured you are in your answer based on the document's content.\n",
      "\n",
      "**End of Prompt**\n",
      "``` \n",
      "\n",
      "This prompt addresses previous weaknesses by providing clarity, specifications for output format, constraints, error handling instructions, and a method for assessing confidence in the response.\n",
      "\n",
      "=== 개선 품질 평가 ===\n",
      "\n",
      "프롬프트 1 평가: ```json\n",
      "{\n",
      "  \"clarity_score\": 9,\n",
      "  \"robustness_score\": 8,\n",
      "  \"production_score\": 9,\n",
      "  \"completeness_score\": 9,\n",
      "  \"overall_score\": 8,\n",
      "  \"comments\": \"The improved prompt significantly enhances clarity by providing detailed input specifications and sentiment categories. It outlines expected output formats and includes error handling, which increases robustness and makes it more suitable for production use. However, depending on the complexity of sentiment analysis (such as handling subtle nuances), additional guidance could be beneficial for edge cases.\"\n",
      "}\n",
      "```\n",
      "\n",
      "프롬프트 2 평가: ```json\n",
      "{\n",
      "  \"clarity_score\": 9,\n",
      "  \"robustness_score\": 8,\n",
      "  \"production_score\": 9,\n",
      "  \"completeness_score\": 9,\n",
      "  \"overall_score\": 8.75,\n",
      "  \"comments\": \"The improved prompt significantly enhances clarity by explicitly outlining expectations for the summary, including guidelines on length, format, and focus. It also introduces handling for edge cases, which adds robustness to the instruction. The inclusion of a JSON schema for output helps with production readiness, ensuring that the results are structured and usable. Overall, it is a well-rounded and improved prompt; however, slight adjustments could be made for more nuanced edge cases or error handling.\"\n",
      "}\n",
      "```\n",
      "\n",
      "프롬프트 3 평가: {\n",
      "  \"clarity_score\": 9,\n",
      "  \"robustness_score\": 9,\n",
      "  \"production_score\": 8,\n",
      "  \"completeness_score\": 9,\n",
      "  \"overall_score\": 9,\n",
      "  \"comments\": \"The improved prompt significantly enhances clarity by specifying the format and structure of the response, which helps users understand exactly what is expected. It also introduces robust error handling for ambiguous questions and unexpected document content, making it more versatile. The production readiness is high, though some minor adjustments could be made based on the context of the intended users. Overall, a comprehensive and well-structured transformation.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 3-2. OpenAI Meta-Prompting (더 정교한 개선)\n",
    "def openai_meta_prompting():\n",
    "    \"\"\"OpenAI를 사용한 고급 메타 프롬프팅\"\"\"\n",
    "    \n",
    "    initial_prompts = [\n",
    "        \"Classify sentiment of a review.\",\n",
    "        \"Summarize this text.\",\n",
    "        \"Answer the question about the document.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== OpenAI Meta-Prompting ===\")\n",
    "    \n",
    "    improved_prompts = []\n",
    "    \n",
    "    for i, initial in enumerate(initial_prompts, 1):\n",
    "        print(f\"\\n--- 프롬프트 {i} 개선 ---\")\n",
    "        print(f\"초기: {initial}\")\n",
    "        \n",
    "        # 1) 프롬프트 분석 및 개선 계획\n",
    "        analysis_prompt = f\"\"\"Analyze this prompt for weaknesses and create an improvement plan:\n",
    "        \n",
    "Prompt: \"{initial}\"\n",
    "\n",
    "Analyze:\n",
    "1. Clarity issues\n",
    "2. Missing constraints\n",
    "3. Output format problems\n",
    "4. Edge case handling\n",
    "5. Scalability concerns\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"weaknesses\": [...],\n",
    "  \"improvement_plan\": [...],\n",
    "  \"priority_fixes\": [...]\n",
    "}}\"\"\"\n",
    "        \n",
    "        analysis = run_openai(analysis_prompt, max_completion_tokens=300)\n",
    "        print(f\"\\n분석 결과: {analysis}\")\n",
    "        \n",
    "        # 2) 개선된 프롬프트 생성\n",
    "        improvement_prompt = f\"\"\"Based on this analysis, create a robust, production-ready prompt:\n",
    "\n",
    "Analysis: {analysis}\n",
    "Original: \"{initial}\"\n",
    "\n",
    "Create an improved prompt that:\n",
    "1. Has clear input/output specifications\n",
    "2. Includes JSON schema\n",
    "3. Has error handling instructions\n",
    "4. Specifies constraints (length, format, etc.)\n",
    "5. Includes confidence scoring\n",
    "6. Handles edge cases\n",
    "\n",
    "Return only the improved prompt text.\"\"\"\n",
    "        \n",
    "        improved = run_openai(improvement_prompt, max_completion_tokens=400)\n",
    "        print(f\"\\n개선됨: {improved}\")\n",
    "        \n",
    "        improved_prompts.append({\n",
    "            \"original\": initial,\n",
    "            \"analysis\": analysis,\n",
    "            \"improved\": improved\n",
    "        })\n",
    "    \n",
    "    # 3) 개선된 프롬프트들의 품질 평가\n",
    "    print(\"\\n=== 개선 품질 평가 ===\")\n",
    "    \n",
    "    for i, prompt_data in enumerate(improved_prompts, 1):\n",
    "        evaluation_prompt = f\"\"\"Rate the improvement quality of this prompt transformation:\n",
    "\n",
    "Original: \"{prompt_data['original']}\"\n",
    "Improved: \"{prompt_data['improved']}\"\n",
    "\n",
    "Rate (1-10) on:\n",
    "- Clarity improvement\n",
    "- Robustness\n",
    "- Production readiness\n",
    "- Completeness\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"clarity_score\": 1-10,\n",
    "  \"robustness_score\": 1-10,\n",
    "  \"production_score\": 1-10,\n",
    "  \"completeness_score\": 1-10,\n",
    "  \"overall_score\": 1-10,\n",
    "  \"comments\": \"...\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        evaluation = run_openai(evaluation_prompt, max_completion_tokens=250)\n",
    "        print(f\"\\n프롬프트 {i} 평가: {evaluation}\")\n",
    "    \n",
    "    return improved_prompts\n",
    "\n",
    "improved_openai = openai_meta_prompting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version sentiment_v1.0 added\n",
      "Version sentiment_v2.0 added\n",
      "=== 프롬프트 A/B 테스트 ===\n",
      "\n",
      "A/B 테스트 결과:\n",
      "sentiment_v1.0:\n",
      "  평균 점수: 0.400\n",
      "  평균 지연: 2.307s\n",
      "  JSON 유효율: 0.000\n",
      "sentiment_v2.0:\n",
      "  평균 점수: 0.867\n",
      "  평균 지연: 1.561s\n",
      "  JSON 유효율: 1.000\n",
      "\n",
      "최고 성능: sentiment_v2.0\n"
     ]
    }
   ],
   "source": [
    "# 3-3. 프롬프트 버전 관리 시스템\n",
    "class PromptVersionManager:\n",
    "    \"\"\"프롬프트 버전 관리 및 A/B 테스트\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.versions = {}\n",
    "        self.test_results = {}\n",
    "    \n",
    "    def add_version(self, name: str, version: str, prompt: str, metadata: dict = None):\n",
    "        \"\"\"새 프롬프트 버전 추가\"\"\"\n",
    "        key = f\"{name}_v{version}\"\n",
    "        self.versions[key] = {\n",
    "            \"prompt\": prompt,\n",
    "            \"metadata\": metadata or {},\n",
    "            \"created_at\": time.time()\n",
    "        }\n",
    "        print(f\"Version {key} added\")\n",
    "    \n",
    "    def run_ab_test(self, name: str, versions: list, test_cases: list):\n",
    "        \"\"\"A/B 테스트 실행\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for version in versions:\n",
    "            key = f\"{name}_v{version}\"\n",
    "            if key not in self.versions:\n",
    "                continue\n",
    "                \n",
    "            version_results = []\n",
    "            prompt_template = self.versions[key][\"prompt\"]\n",
    "            \n",
    "            for test_case in test_cases:\n",
    "                # 프롬프트에 테스트 케이스 삽입\n",
    "                full_prompt = prompt_template.replace(\"{input}\", test_case[\"input\"])\n",
    "                \n",
    "                start_time = time.time()\n",
    "                result = run_openai(full_prompt, temperature=0.1, max_tokens=200)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                # 결과 평가\n",
    "                score = self._evaluate_result(result, test_case.get(\"expected\"))\n",
    "                \n",
    "                version_results.append({\n",
    "                    \"input\": test_case[\"input\"],\n",
    "                    \"output\": result,\n",
    "                    \"score\": score,\n",
    "                    \"latency\": end_time - start_time,\n",
    "                    \"json_valid\": safe_json_parse(result) is not None\n",
    "                })\n",
    "            \n",
    "            results[key] = version_results\n",
    "        \n",
    "        # 결과 분석\n",
    "        analysis = self._analyze_ab_results(results)\n",
    "        self.test_results[f\"{name}_ab_{int(time.time())}\"] = {\n",
    "            \"results\": results,\n",
    "            \"analysis\": analysis\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _evaluate_result(self, result: str, expected: dict = None) -> float:\n",
    "        \"\"\"결과 품질 평가 (간단한 버전)\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # JSON 유효성 (+30점)\n",
    "        if safe_json_parse(result):\n",
    "            score += 0.3\n",
    "        \n",
    "        # 길이 적절성 (+20점)\n",
    "        if 50 <= len(result) <= 500:\n",
    "            score += 0.2\n",
    "        \n",
    "        # 한글 포함 여부 (+20점)\n",
    "        if re.search(r'[가-힣]', result):\n",
    "            score += 0.2\n",
    "        \n",
    "        # 기본 구조 (+30점)\n",
    "        if any(keyword in result.lower() for keyword in ['label', 'confidence', 'reason']):\n",
    "            score += 0.3\n",
    "        \n",
    "        return min(1.0, score)\n",
    "    \n",
    "    def _analyze_ab_results(self, results: dict) -> dict:\n",
    "        \"\"\"A/B 테스트 결과 분석\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        for version, version_results in results.items():\n",
    "            scores = [r[\"score\"] for r in version_results]\n",
    "            latencies = [r[\"latency\"] for r in version_results]\n",
    "            json_valid_rate = sum(1 for r in version_results if r[\"json_valid\"]) / len(version_results)\n",
    "            \n",
    "            analysis[version] = {\n",
    "                \"avg_score\": sum(scores) / len(scores),\n",
    "                \"avg_latency\": sum(latencies) / len(latencies),\n",
    "                \"json_valid_rate\": json_valid_rate,\n",
    "                \"total_tests\": len(version_results)\n",
    "            }\n",
    "        \n",
    "        # 최고 성능 버전 선정\n",
    "        best_version = max(analysis.keys(), \n",
    "                         key=lambda x: analysis[x][\"avg_score\"] * analysis[x][\"json_valid_rate\"])\n",
    "        analysis[\"best_version\"] = best_version\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# 버전 관리 시스템 테스트\n",
    "def test_version_manager():\n",
    "    manager = PromptVersionManager()\n",
    "    \n",
    "    # 프롬프트 버전들 등록\n",
    "    manager.add_version(\n",
    "        \"sentiment\", \"1.0\", \n",
    "        \"Classify sentiment: {input}\",\n",
    "        {\"author\": \"initial\", \"notes\": \"Basic version\"}\n",
    "    )\n",
    "    \n",
    "    manager.add_version(\n",
    "        \"sentiment\", \"2.0\",\n",
    "        \"\"\"Classify sentiment with confidence scoring.\n",
    "Return JSON: {\"label\": \"positive|negative|neutral\", \"confidence\": 0.0-1.0, \"reason\": \"...\"}\n",
    "Text: {input}\"\"\",\n",
    "        {\"author\": \"improved\", \"notes\": \"Added JSON structure\"}\n",
    "    )\n",
    "    \n",
    "    # 테스트 케이스\n",
    "    test_cases = [\n",
    "        {\"input\": \"배송이 빠르고 좋아요!\", \"expected\": {\"label\": \"positive\"}},\n",
    "        {\"input\": \"포장이 엉망이고 실망스러워요\", \"expected\": {\"label\": \"negative\"}},\n",
    "        {\"input\": \"그냥 보통이에요\", \"expected\": {\"label\": \"neutral\"}}\n",
    "    ]\n",
    "    \n",
    "    # A/B 테스트 실행\n",
    "    print(\"=== 프롬프트 A/B 테스트 ===\")\n",
    "    analysis = manager.run_ab_test(\"sentiment\", [\"1.0\", \"2.0\"], test_cases)\n",
    "    \n",
    "    print(\"\\nA/B 테스트 결과:\")\n",
    "    for version, metrics in analysis.items():\n",
    "        if version != \"best_version\":\n",
    "            print(f\"{version}:\")\n",
    "            print(f\"  평균 점수: {metrics['avg_score']:.3f}\")\n",
    "            print(f\"  평균 지연: {metrics['avg_latency']:.3f}s\")\n",
    "            print(f\"  JSON 유효율: {metrics['json_valid_rate']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n최고 성능: {analysis['best_version']}\")\n",
    "    \n",
    "    return manager\n",
    "\n",
    "version_manager = test_version_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. APE (Automatic Prompt Engineering)\n",
    "\n",
    "**핵심 아이디어**: 후보 프롬프트 자동 생성→평가→선정(루프)\n",
    "\n",
    "**구성**:\n",
    "1. 후보 K개 생성\n",
    "2. Dev set 평가\n",
    "3. 베스트 선택\n",
    "4. 버전 태깅\n",
    "\n",
    "**평가 방식**:\n",
    "- 정답형: 정확도/EM/F1\n",
    "- 생성형: LM-as-Judge(0~5), 포맷 준수율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama APE 데모 ===\n",
      "Dev set 크기: 4\n",
      "\n",
      "후보 1: Classify the sentiment of a given product review as positive, negative, or neutral based on its emotional tone and content, outputting a JSON response with 'label' and 'reason' keys to specify the classification decision.\n",
      "\n",
      "후보 2: Classify the sentiment of the given product review as positive, negative, or neutral, providing a brief reason for your classification in JSON format: `{\"label\": \"positive/negative/neutral\", \"reason\": \"brief_reason_here\"}`.\n",
      "\n",
      "후보 3: Classify the sentiment of a given review as positive, negative, or neutral, returning a JSON object with keys 'label' and 'reason'.\n",
      "\n",
      "--- 후보 1 평가 ---\n",
      "  배송이 빠르고 만족합니다... -> positive (정답: positive)\n",
      "  불량품이 와서 화가 납니다... -> negative (정답: negative)\n",
      "  보통 수준입니다... -> positive (정답: neutral)\n",
      "  가격이 적당하고 품질도 괜찮아요... -> positive (정답: positive)\n",
      "\n",
      "후보 1 정확도: 0.750 (3/4)\n",
      "\n",
      "--- 후보 2 평가 ---\n",
      "  배송이 빠르고 만족합니다... -> positive (정답: positive)\n",
      "  불량품이 와서 화가 납니다... -> negative (정답: negative)\n",
      "  보통 수준입니다... -> positive (정답: neutral)\n",
      "  가격이 적당하고 품질도 괜찮아요... -> positive (정답: positive)\n",
      "\n",
      "후보 2 정확도: 0.750 (3/4)\n",
      "\n",
      "--- 후보 3 평가 ---\n",
      "  배송이 빠르고 만족합니다... -> positive (정답: positive)\n",
      "  불량품이 와서 화가 납니다... -> negative (정답: negative)\n",
      "  보통 수준입니다... -> neutral (정답: neutral)\n",
      "  가격이 적당하고 품질도 괜찮아요... -> positive (정답: positive)\n",
      "\n",
      "후보 3 정확도: 1.000 (4/4)\n",
      "\n",
      "=== 최고 성능 프롬프트 (정확도: 1.000) ===\n",
      "Classify the sentiment of a given review as positive, negative, or neutral, returning a JSON object with keys 'label' and 'reason'.\n"
     ]
    }
   ],
   "source": [
    "# 4-1. Ollama APE (간단한 버전)\n",
    "def ollama_ape_demo():\n",
    "    \"\"\"Ollama를 사용한 간단한 APE 데모\"\"\"\n",
    "    \n",
    "    # Dev set 정의\n",
    "    dev_set = [\n",
    "        {\"text\": \"배송이 빠르고 만족합니다\", \"label\": \"positive\"},\n",
    "        {\"text\": \"불량품이 와서 화가 납니다\", \"label\": \"negative\"},\n",
    "        {\"text\": \"보통 수준입니다\", \"label\": \"neutral\"},\n",
    "        {\"text\": \"가격이 적당하고 품질도 괜찮아요\", \"label\": \"positive\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Ollama APE 데모 ===\")\n",
    "    print(f\"Dev set 크기: {len(dev_set)}\")\n",
    "    \n",
    "    # 1) 후보 프롬프트 생성 (3개)\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(1, 4):\n",
    "        generation_prompt = f\"\"\"Generate a one-line prompt to classify sentiment (positive|negative|neutral) with JSON output.\n",
    "Requirements:\n",
    "- Must return JSON with keys: label, reason\n",
    "- Should be clear and specific\n",
    "- Version {i} - make it different from previous versions\n",
    "\n",
    "Return only the prompt text.\"\"\"\n",
    "        \n",
    "        candidate = run_ollama(\"llama3.1:8b\", generation_prompt)\n",
    "        candidates.append(candidate)\n",
    "        print(f\"\\n후보 {i}: {candidate}\")\n",
    "    \n",
    "    # 2) 각 후보를 dev set으로 평가\n",
    "    best_score = 0\n",
    "    best_candidate = None\n",
    "    \n",
    "    for i, candidate in enumerate(candidates, 1):\n",
    "        correct = 0\n",
    "        total = len(dev_set)\n",
    "        \n",
    "        print(f\"\\n--- 후보 {i} 평가 ---\")\n",
    "        \n",
    "        for example in dev_set:\n",
    "            test_prompt = f\"\"\"{candidate}\n",
    "            \n",
    "Review: --- {example['text']} ---\"\"\"\n",
    "            \n",
    "            result = run_ollama(\"llama3.1:8b\", test_prompt)\n",
    "            \n",
    "            # 라벨 추출 (간단한 파싱)\n",
    "            predicted_label = \"unknown\"\n",
    "            for label in [\"positive\", \"negative\", \"neutral\"]:\n",
    "                if label.lower() in result.lower():\n",
    "                    predicted_label = label\n",
    "                    break\n",
    "            \n",
    "            if predicted_label == example[\"label\"]:\n",
    "                correct += 1\n",
    "            \n",
    "            print(f\"  {example['text'][:30]}... -> {predicted_label} (정답: {example['label']})\")\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f\"\\n후보 {i} 정확도: {accuracy:.3f} ({correct}/{total})\")\n",
    "        \n",
    "        if accuracy > best_score:\n",
    "            best_score = accuracy\n",
    "            best_candidate = candidate\n",
    "    \n",
    "    print(f\"\\n=== 최고 성능 프롬프트 (정확도: {best_score:.3f}) ===\")\n",
    "    print(best_candidate)\n",
    "    \n",
    "    return {\n",
    "        \"candidates\": candidates,\n",
    "        \"best_prompt\": best_candidate,\n",
    "        \"best_score\": best_score,\n",
    "        \"dev_set\": dev_set\n",
    "    }\n",
    "\n",
    "ollama_ape_result = ollama_ape_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2. OpenAI APE (고급 버전 - LM-as-Judge 포함)\n",
    "def openai_ape_advanced():\n",
    "    \"\"\"OpenAI를 사용한 고급 APE 시스템\"\"\"\n",
    "    \n",
    "    # 더 큰 Dev set\n",
    "    dev_set = [\n",
    "        {\"text\": \"배송이 빠르고 만족합니다\", \"label\": \"positive\"},\n",
    "        {\"text\": \"불량품이 와서 화가 납니다\", \"label\": \"negative\"}, \n",
    "        {\"text\": \"보통 수준입니다\", \"label\": \"neutral\"},\n",
    "        {\"text\": \"가격이 적당하고 품질도 괜찮아요\", \"label\": \"positive\"},\n",
    "        {\"text\": \"포장이 찢어져서 실망이에요\", \"label\": \"negative\"},\n",
    "        {\"text\": \"특별하지 않아요. 그냥 평범해요\", \"label\": \"neutral\"},\n",
    "        {\"text\": \"완벽합니다! 강력 추천해요!\", \"label\": \"positive\"},\n",
    "        {\"text\": \"돈 아까워요. 환불하고 싶네요\", \"label\": \"negative\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"=== OpenAI 고급 APE 시스템 ===\")\n",
    "    print(f\"Dev set 크기: {len(dev_set)}\")\n",
    "    \n",
    "    # 1) 후보 프롬프트 생성 (5개, 다양한 접근법)\n",
    "    generation_strategies = [\n",
    "        \"Generate a direct, simple prompt for sentiment classification with JSON output.\",\n",
    "        \"Generate a detailed prompt with examples and constraints for sentiment classification.\",\n",
    "        \"Generate a prompt that uses role-playing (persona) for sentiment classification.\", \n",
    "        \"Generate a prompt with step-by-step reasoning for sentiment classification.\",\n",
    "        \"Generate a prompt with confidence scoring and edge case handling for sentiment classification.\"\n",
    "    ]\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for i, strategy in enumerate(generation_strategies, 1):\n",
    "        generation_prompt = f\"\"\"{strategy}\n",
    "        \n",
    "Requirements:\n",
    "- Must return valid JSON with keys: {{\"label\": \"positive|negative|neutral\", \"confidence\": 0.0-1.0, \"reason\": \"...\"}}\n",
    "- Should handle Korean text\n",
    "- Should be robust and production-ready\n",
    "- Include placeholder {{input}} for the text to classify\n",
    "\n",
    "Return only the prompt text.\"\"\"\n",
    "        \n",
    "        candidate = run_openai(generation_prompt, temperature=0.7, max_tokens=300)\n",
    "        candidates.append({\n",
    "            \"id\": i,\n",
    "            \"strategy\": strategy,\n",
    "            \"prompt\": candidate\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n후보 {i} ({strategy.split()[2]}):\")\n",
    "        print(f\"  {candidate[:100]}...\")\n",
    "    \n",
    "    # 2) 다중 메트릭 평가\n",
    "    def evaluate_candidate(candidate_data: dict) -> dict:\n",
    "        \"\"\"후보 프롬프트 종합 평가\"\"\"\n",
    "        prompt_template = candidate_data[\"prompt\"]\n",
    "        results = []\n",
    "        \n",
    "        for example in dev_set:\n",
    "            # 프롬프트 실행\n",
    "            full_prompt = prompt_template.replace(\"{input}\", example[\"text\"])\n",
    "            \n",
    "            try:\n",
    "                response = run_openai(full_prompt, temperature=0.1, max_tokens=200)\n",
    "                \n",
    "                # JSON 파싱\n",
    "                json_data = safe_json_parse(response)\n",
    "                \n",
    "                if json_data:\n",
    "                    predicted = json_data.get(\"label\", \"unknown\").lower()\n",
    "                    confidence = json_data.get(\"confidence\", 0.0)\n",
    "                    reason = json_data.get(\"reason\", \"\")\n",
    "                else:\n",
    "                    # Fallback: 텍스트에서 라벨 추출\n",
    "                    predicted = \"unknown\"\n",
    "                    confidence = 0.0\n",
    "                    reason = \"\"\n",
    "                    \n",
    "                    for label in [\"positive\", \"negative\", \"neutral\"]:\n",
    "                        if label in response.lower():\n",
    "                            predicted = label\n",
    "                            break\n",
    "                \n",
    "                is_correct = predicted == example[\"label\"]\n",
    "                \n",
    "                results.append({\n",
    "                    \"input\": example[\"text\"],\n",
    "                    \"expected\": example[\"label\"],\n",
    "                    \"predicted\": predicted,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"reason\": reason,\n",
    "                    \"correct\": is_correct,\n",
    "                    \"json_valid\": json_data is not None,\n",
    "                    \"response\": response\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"input\": example[\"text\"],\n",
    "                    \"expected\": example[\"label\"],\n",
    "                    \"error\": str(e),\n",
    "                    \"correct\": False,\n",
    "                    \"json_valid\": False\n",
    "                })\n",
    "        \n",
    "        # 메트릭 계산\n",
    "        total = len(results)\n",
    "        correct = sum(1 for r in results if r.get(\"correct\", False))\n",
    "        json_valid = sum(1 for r in results if r.get(\"json_valid\", False))\n",
    "        avg_confidence = sum(r.get(\"confidence\", 0) for r in results if \"confidence\" in r) / total\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": correct / total,\n",
    "            \"json_valid_rate\": json_valid / total,\n",
    "            \"avg_confidence\": avg_confidence,\n",
    "            \"results\": results\n",
    "        }\n",
    "    \n",
    "    # 3) 모든 후보 평가\n",
    "    evaluations = []\n",
    "    \n",
    "    print(\"\\n=== 후보 평가 진행 중... ===\")\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        print(f\"후보 {candidate['id']} 평가 중...\")\n",
    "        evaluation = evaluate_candidate(candidate)\n",
    "        \n",
    "        # LM-as-Judge 평가 추가\n",
    "        judge_prompt = f\"\"\"Rate the quality of this prompt for sentiment classification (1-10):\n",
    "        \n",
    "Prompt: \"{candidate['prompt']}\"\n",
    "\n",
    "Consider:\n",
    "- Clarity and specificity\n",
    "- Output format specification\n",
    "- Robustness for edge cases\n",
    "- Production readiness\n",
    "\n",
    "Return JSON: {{\"score\": 1-10, \"reasoning\": \"...\"}}\"\"\"\n",
    "        \n",
    "        judge_response = run_openai(judge_prompt, temperature=0.2, max_tokens=200)\n",
    "        judge_data = safe_json_parse(judge_response)\n",
    "        judge_score = judge_data.get(\"score\", 5) if judge_data else 5\n",
    "        \n",
    "        evaluation[\"judge_score\"] = judge_score / 10.0  # 0-1로 정규화\n",
    "        evaluation[\"judge_reasoning\"] = judge_data.get(\"reasoning\", \"\") if judge_data else \"\"\n",
    "        \n",
    "        evaluations.append({\n",
    "            \"candidate\": candidate,\n",
    "            \"evaluation\": evaluation\n",
    "        })\n",
    "    \n",
    "    # 4) 종합 점수 계산 및 최고 후보 선정\n",
    "    for eval_data in evaluations:\n",
    "        eval_metrics = eval_data[\"evaluation\"]\n",
    "        \n",
    "        # 가중 종합 점수 (정확도 40% + JSON유효율 30% + Judge점수 30%)\n",
    "        composite_score = (\n",
    "            eval_metrics[\"accuracy\"] * 0.4 +\n",
    "            eval_metrics[\"json_valid_rate\"] * 0.3 +\n",
    "            eval_metrics[\"judge_score\"] * 0.3\n",
    "        )\n",
    "        eval_metrics[\"composite_score\"] = composite_score\n",
    "    \n",
    "    # 결과 정렬 (종합 점수 기준)\n",
    "    evaluations.sort(key=lambda x: x[\"evaluation\"][\"composite_score\"], reverse=True)\n",
    "    \n",
    "    # 5) 결과 출력\n",
    "    print(\"\\n=== APE 평가 결과 ===\")\n",
    "    \n",
    "    for i, eval_data in enumerate(evaluations):\n",
    "        candidate = eval_data[\"candidate\"]\n",
    "        metrics = eval_data[\"evaluation\"]\n",
    "        \n",
    "        print(f\"\\n순위 {i+1}: 후보 {candidate['id']}\")\n",
    "        print(f\"  종합점수: {metrics['composite_score']:.3f}\")\n",
    "        print(f\"  정확도: {metrics['accuracy']:.3f}\")\n",
    "        print(f\"  JSON유효율: {metrics['json_valid_rate']:.3f}\")\n",
    "        print(f\"  Judge점수: {metrics['judge_score']:.3f}\")\n",
    "        print(f\"  평균신뢰도: {metrics['avg_confidence']:.3f}\")\n",
    "    \n",
    "    best_candidate = evaluations[0]\n",
    "    print(f\"\\n=== 최고 성능 프롬프트 (종합점수: {best_candidate['evaluation']['composite_score']:.3f}) ===\")\n",
    "    print(best_candidate[\"candidate\"][\"prompt\"])\n",
    "    \n",
    "    return {\n",
    "        \"candidates\": candidates,\n",
    "        \"evaluations\": evaluations,\n",
    "        \"best_candidate\": best_candidate,\n",
    "        \"dev_set\": dev_set\n",
    "    }\n",
    "\n",
    "ape_result = openai_ape_advanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ape_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m❌ 프롬프트가 기준을 충족하지 못합니다. 추가 개선이 필요합니다.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mvalidate_ape_winner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mvalidate_ape_winner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_ape_winner\u001b[39m():\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"APE 우승 프롬프트 검증\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mape_result\u001b[49m:\n\u001b[32m      6\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAPE 결과가 없습니다.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'ape_result' is not defined"
     ]
    }
   ],
   "source": [
    "# 4-3. APE 결과 검증 및 최종 테스트\n",
    "def validate_ape_winner():\n",
    "    \"\"\"APE 우승 프롬프트 검증\"\"\"\n",
    "    \n",
    "    if not ape_result:\n",
    "        print(\"APE 결과가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    best_prompt = ape_result[\"best_candidate\"][\"candidate\"][\"prompt\"]\n",
    "    \n",
    "    # 새로운 테스트 케이스 (Dev set과 다름)\n",
    "    test_cases = [\n",
    "        \"이 제품 정말 최고예요! 친구들에게도 추천했어요.\",\n",
    "        \"배송비가 너무 비싸고 포장도 부실해요. 다시는 안 사겠어요.\",\n",
    "        \"그냥 평범한 제품이에요. 나쁘지도 좋지도 않네요.\",\n",
    "        \"생각보다 괜찮은데 가격이 좀 아쉬워요.\",\n",
    "        \"완전 대박! 이런 걸 찾고 있었어요!\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== APE 우승 프롬프트 최종 검증 ===\")\n",
    "    print(\"\\n우승 프롬프트:\")\n",
    "    print(best_prompt)\n",
    "    \n",
    "    print(\"\\n=== 새로운 테스트 케이스 결과 ===\")\n",
    "    \n",
    "    total_tests = len(test_cases)\n",
    "    json_valid_count = 0\n",
    "    total_confidence = 0\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        full_prompt = best_prompt.replace(\"{input}\", test_case)\n",
    "        \n",
    "        result = run_openai(full_prompt, temperature=0.1, max_tokens=200)\n",
    "        \n",
    "        print(f\"\\n테스트 {i}: {test_case}\")\n",
    "        print(f\"결과: {result}\")\n",
    "        \n",
    "        # JSON 유효성 및 구조 검증\n",
    "        json_data = safe_json_parse(result)\n",
    "        if json_data:\n",
    "            json_valid_count += 1\n",
    "            confidence = json_data.get(\"confidence\", 0)\n",
    "            total_confidence += confidence\n",
    "            \n",
    "            print(f\"  ✓ JSON 유효, 라벨: {json_data.get('label')}, 신뢰도: {confidence}\")\n",
    "        else:\n",
    "            print(f\"  ✗ JSON 파싱 실패\")\n",
    "    \n",
    "    # 최종 통계\n",
    "    json_success_rate = json_valid_count / total_tests\n",
    "    avg_confidence = total_confidence / max(json_valid_count, 1)\n",
    "    \n",
    "    print(f\"\\n=== 최종 검증 통계 ===\")\n",
    "    print(f\"JSON 성공률: {json_success_rate:.1%} ({json_valid_count}/{total_tests})\")\n",
    "    print(f\"평균 신뢰도: {avg_confidence:.3f}\")\n",
    "    \n",
    "    # 성능 기준 평가\n",
    "    if json_success_rate >= 0.8 and avg_confidence >= 0.6:\n",
    "        print(\"\\n🎉 프롬프트가 프로덕션 기준을 충족합니다!\")\n",
    "    elif json_success_rate >= 0.6:\n",
    "        print(\"\\n⚠️ 프롬프트가 기본 기준은 충족하지만 개선이 필요합니다.\")\n",
    "    else:\n",
    "        print(\"\\n❌ 프롬프트가 기준을 충족하지 못합니다. 추가 개선이 필요합니다.\")\n",
    "\n",
    "validate_ape_winner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종합 실습: 통합 시스템\n",
    "\n",
    "모든 고급 통합 기법을 결합한 종합 시스템을 구축해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 고급 프롬프트 엔지니어링 통합 시스템 데모 ===\n",
      "\n",
      "테스트 입력: 배송이 너무 늦어서 화가 나는데, 언제 받을 수 있나요?\n",
      "\n",
      "=== 체인 실행 결과 ===\n",
      "\n",
      "preprocessing (단계 1):\n",
      "  입력: 배송이 너무 늦어서 화가 나는데, 언제 받을 수 있나요?...\n",
      "  출력: 배송이 너무 늦어서 화가 나는데, 언제 받을 수 있나요? \n",
      "\n",
      "**Normalized Text:**\n",
      "배송이 늦어서 화가 나는데, 언제 받을 수 있나요?...\n",
      "\n",
      "sentiment_analysis (단계 2):\n",
      "  입력: 배송이 너무 늦어서 화가 나는데, 언제 받을 수 있나요? \n",
      "\n",
      "**Normalized Tex...\n",
      "  출력: The sentiment of the customer message is negative, with a confidence level of 80%. The customer expr...\n",
      "\n",
      "response_generation (단계 3):\n",
      "  입력: The sentiment of the customer message is negative,...\n",
      "  출력: Subject: Update on Your Order Delivery\n",
      "\n",
      "Dear [Customer's Name],\n",
      "\n",
      "Thank you for reaching out to us. I...\n",
      "\n",
      "최종 결과: Subject: Update on Your Order Delivery\n",
      "\n",
      "Dear [Customer's Name],\n",
      "\n",
      "Thank you for reaching out to us. I sincerely apologize for the frustration and inconvenience caused by the delay in your order delivery. I understand how important it is for you to receive your items on time.\n",
      "\n",
      "I am currently looking into the status of your order and will provide you with an update as soon as possible. In the meantime, I appreciate your patience and understanding during this process.\n",
      "\n",
      "If you have any further questions or concerns, please don’t hesitate to let me know. We’re here to help!\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]  \n",
      "[Your Position]  \n",
      "[Company Name]  \n",
      "[Contact Information]\n",
      "\n",
      "=== 프롬프트 자동 개선 ===\n",
      "초기 프롬프트: 답변해주세요.\n",
      "개선된 프롬프트: 질문에 대한 구체적이고 유용한 답변을 제공해주세요. 예를 들어, 날씨에 대한 질문에는 현재 날씨, 기온, 강수 확률 등을 포함하고, 추천 요청에는 추천 이유와 함께 구체적인 항목을 제시해주세요.\n",
      "개선 사항: ['Specify the context or subject matter for the questions to guide the responses.', 'Provide a more detailed format for the expected responses, such as length or style.', 'Clarify the type of information or recommendations that should be included in the responses.']\n"
     ]
    }
   ],
   "source": [
    "# 통합 시스템: 모든 기법을 결합한 고급 프롬프트 엔지니어링 시스템\n",
    "class AdvancedPromptSystem:\n",
    "    \"\"\"고급 프롬프트 엔지니어링 통합 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.version_manager = PromptVersionManager()\n",
    "        self.tool_registry = {}\n",
    "        self.chain_definitions = {}\n",
    "        self.ape_history = []\n",
    "    \n",
    "    def register_tool(self, name: str, func, description: str, parameters: dict):\n",
    "        \"\"\"도구 등록\"\"\"\n",
    "        self.tool_registry[name] = {\n",
    "            \"function\": func,\n",
    "            \"description\": description,\n",
    "            \"parameters\": parameters\n",
    "        }\n",
    "    \n",
    "    def define_chain(self, name: str, stages: list):\n",
    "        \"\"\"처리 체인 정의\"\"\"\n",
    "        self.chain_definitions[name] = stages\n",
    "    \n",
    "    def execute_with_tools(self, prompt: str, available_tools: list = None):\n",
    "        \"\"\"도구를 활용한 프롬프트 실행\"\"\"\n",
    "        if available_tools is None:\n",
    "            available_tools = list(self.tool_registry.keys())\n",
    "        \n",
    "        # OpenAI Function Calling 형식으로 도구 변환\n",
    "        tools_schema = []\n",
    "        for tool_name in available_tools:\n",
    "            if tool_name in self.tool_registry:\n",
    "                tool_info = self.tool_registry[tool_name]\n",
    "                tools_schema.append({\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tool_name,\n",
    "                        \"description\": tool_info[\"description\"],\n",
    "                        \"parameters\": tool_info[\"parameters\"]\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        # 실행\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=tools_schema,\n",
    "            tool_choice=\"auto\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        # 도구 호출 처리\n",
    "        if response.choices[0].message.tool_calls:\n",
    "            messages.append(response.choices[0].message)\n",
    "            \n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                tool_name = tool_call.function.name\n",
    "                if tool_name in self.tool_registry:\n",
    "                    try:\n",
    "                        args = json.loads(tool_call.function.arguments)\n",
    "                        result = self.tool_registry[tool_name][\"function\"](**args)\n",
    "                        \n",
    "                        messages.append({\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"name\": tool_name,\n",
    "                            \"content\": json.dumps(result, ensure_ascii=False)\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        messages.append({\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"name\": tool_name,\n",
    "                            \"content\": f\"Error: {str(e)}\"\n",
    "                        })\n",
    "            \n",
    "            # 최종 응답\n",
    "            final_response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            return final_response.choices[0].message.content\n",
    "        else:\n",
    "            return response.choices[0].message.content\n",
    "    \n",
    "    def execute_chain(self, chain_name: str, input_data: str):\n",
    "        \"\"\"정의된 체인 실행\"\"\"\n",
    "        if chain_name not in self.chain_definitions:\n",
    "            return f\"Chain '{chain_name}' not found\"\n",
    "        \n",
    "        stages = self.chain_definitions[chain_name]\n",
    "        current_data = input_data\n",
    "        stage_results = []\n",
    "        \n",
    "        for i, stage in enumerate(stages):\n",
    "            stage_prompt = stage[\"prompt\"].format(input=current_data)\n",
    "            \n",
    "            if stage.get(\"use_tools\", False):\n",
    "                result = self.execute_with_tools(\n",
    "                    stage_prompt, \n",
    "                    stage.get(\"tools\", [])\n",
    "                )\n",
    "            else:\n",
    "                result = run_openai(\n",
    "                    stage_prompt, \n",
    "                    **stage.get(\"params\", {})\n",
    "                )\n",
    "            \n",
    "            stage_results.append({\n",
    "                \"stage\": i + 1,\n",
    "                \"name\": stage.get(\"name\", f\"Stage {i+1}\"),\n",
    "                \"input\": current_data,\n",
    "                \"output\": result\n",
    "            })\n",
    "            \n",
    "            current_data = result\n",
    "        \n",
    "        return {\n",
    "            \"final_result\": current_data,\n",
    "            \"stage_results\": stage_results\n",
    "        }\n",
    "    \n",
    "    def auto_improve_prompt(self, initial_prompt: str, task_examples: list):\n",
    "        \"\"\"메타 프롬프팅을 통한 자동 개선\"\"\"\n",
    "        # 분석\n",
    "        analysis_prompt = f\"\"\"Analyze this prompt and suggest improvements:\n",
    "        \n",
    "Prompt: \"{initial_prompt}\"\n",
    "Task examples: {json.dumps(task_examples, ensure_ascii=False)}\n",
    "\n",
    "Return JSON with:\n",
    "{{\n",
    "  \"weaknesses\": [...],\n",
    "  \"improvement_suggestions\": [...],\n",
    "  \"improved_prompt\": \"...\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        analysis = run_openai(analysis_prompt, temperature=0.2, max_tokens=500)\n",
    "        analysis_data = safe_json_parse(analysis)\n",
    "        \n",
    "        if analysis_data:\n",
    "            return analysis_data\n",
    "        else:\n",
    "            return {\"error\": \"Failed to parse analysis\", \"original\": initial_prompt}\n",
    "\n",
    "# 통합 시스템 데모\n",
    "def demo_integrated_system():\n",
    "    \"\"\"통합 시스템 데모\"\"\"\n",
    "    \n",
    "    system = AdvancedPromptSystem()\n",
    "    \n",
    "    print(\"=== 고급 프롬프트 엔지니어링 통합 시스템 데모 ===\")\n",
    "    \n",
    "    # 1) 도구 등록\n",
    "    def analyze_sentiment(text: str) -> dict:\n",
    "        \"\"\"감성 분석 도구\"\"\"\n",
    "        # 간단한 키워드 기반 분석\n",
    "        positive_words = ['좋', '최고', '만족', '추천', '완벽']\n",
    "        negative_words = ['나쁘', '최악', '실망', '화', '문제']\n",
    "        \n",
    "        pos_count = sum(1 for word in positive_words if word in text)\n",
    "        neg_count = sum(1 for word in negative_words if word in text)\n",
    "        \n",
    "        if pos_count > neg_count:\n",
    "            return {\"sentiment\": \"positive\", \"confidence\": 0.8}\n",
    "        elif neg_count > pos_count:\n",
    "            return {\"sentiment\": \"negative\", \"confidence\": 0.8}\n",
    "        else:\n",
    "            return {\"sentiment\": \"neutral\", \"confidence\": 0.6}\n",
    "    \n",
    "    system.register_tool(\n",
    "        \"analyze_sentiment\",\n",
    "        analyze_sentiment,\n",
    "        \"텍스트의 감성을 분석합니다\",\n",
    "        {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"string\", \"description\": \"분석할 텍스트\"}\n",
    "            },\n",
    "            \"required\": [\"text\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 2) 처리 체인 정의\n",
    "    system.define_chain(\"customer_service\", [\n",
    "        {\n",
    "            \"name\": \"preprocessing\",\n",
    "            \"prompt\": \"Clean and normalize this text: {input}\",\n",
    "            \"params\": {\"temperature\": 0.1, \"max_tokens\": 100}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"sentiment_analysis\", \n",
    "            \"prompt\": \"Analyze the sentiment of this customer message: {input}\",\n",
    "            \"use_tools\": True,\n",
    "            \"tools\": [\"analyze_sentiment\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"response_generation\",\n",
    "            \"prompt\": \"Generate appropriate customer service response based on: {input}\",\n",
    "            \"params\": {\"temperature\": 0.3, \"max_tokens\": 200}\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # 3) 테스트 실행\n",
    "    test_input = \"배송이 너무 늦어서 화가 나는데, 언제 받을 수 있나요?\"\n",
    "    \n",
    "    print(f\"\\n테스트 입력: {test_input}\")\n",
    "    \n",
    "    # 체인 실행\n",
    "    chain_result = system.execute_chain(\"customer_service\", test_input)\n",
    "    \n",
    "    print(\"\\n=== 체인 실행 결과 ===\")\n",
    "    for stage in chain_result[\"stage_results\"]:\n",
    "        print(f\"\\n{stage['name']} (단계 {stage['stage']}):\")\n",
    "        print(f\"  입력: {stage['input'][:50]}...\")\n",
    "        print(f\"  출력: {stage['output'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n최종 결과: {chain_result['final_result']}\")\n",
    "    \n",
    "    # 4) 프롬프트 자동 개선\n",
    "    print(\"\\n=== 프롬프트 자동 개선 ===\")\n",
    "    initial_prompt = \"답변해주세요.\"\n",
    "    examples = [\n",
    "        {\"input\": \"날씨가 어때요?\", \"expected\": \"구체적인 정보 제공\"},\n",
    "        {\"input\": \"추천해주세요\", \"expected\": \"명확한 추천과 이유\"}\n",
    "    ]\n",
    "    \n",
    "    improvement = system.auto_improve_prompt(initial_prompt, examples)\n",
    "    \n",
    "    print(f\"초기 프롬프트: {initial_prompt}\")\n",
    "    if \"improved_prompt\" in improvement:\n",
    "        print(f\"개선된 프롬프트: {improvement['improved_prompt']}\")\n",
    "        print(f\"개선 사항: {improvement.get('improvement_suggestions', [])}\")\n",
    "    \n",
    "    return system\n",
    "\n",
    "integrated_system = demo_integrated_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리 및 베스트 프랙티스\n",
    "\n",
    "### 고급 통합 기법 요약\n",
    "\n",
    "| 기법 | 핵심 아이디어 | 주요 장점 | 적용 상황 |\n",
    "|------|---------------|-----------|----------|\n",
    "| **Function Calling** | 외부 도구/API 연동 | 실시간 데이터, 정확한 계산 | 최신 정보 필요, 복잡한 연산 |\n",
    "| **Multiple Chains** | 단계적 파이프라인 처리 | 복잡한 워크플로우 처리 | 다단계 분석, 의사결정 |\n",
    "| **Meta-Prompting** | 프롬프트 자체 개선 | 지속적 품질 향상 | 프롬프트 최적화, 유지보수 |\n",
    "| **APE** | 자동 프롬프트 생성/평가 | 객관적 성능 비교 | 대규모 최적화, A/B 테스트 |\n",
    "\n",
    "### 실무 적용 가이드\n",
    "\n",
    "#### 1. 시스템 설계 원칙\n",
    "- **모듈화**: 각 기법을 독립적 모듈로 구현\n",
    "- **확장성**: 새로운 도구/체인 쉽게 추가 가능\n",
    "- **관찰성**: 각 단계별 로깅 및 메트릭 수집\n",
    "- **안전성**: 도구 호출 시 보안 검증 및 예외 처리\n",
    "\n",
    "#### 2. 성능 최적화\n",
    "- **캐싱**: 반복 호출 결과 캐시\n",
    "- **병렬 처리**: Fan-out 패턴으로 독립적 작업 병렬화\n",
    "- **배치 처리**: 유사한 요청들 배치로 처리\n",
    "- **지연 로딩**: 필요한 시점에만 리소스 로드\n",
    "\n",
    "#### 3. 품질 보증\n",
    "- **자동 테스트**: Dev/Test 셋으로 지속적 검증\n",
    "- **A/B 테스트**: 새로운 프롬프트 버전 비교\n",
    "- **사용자 피드백**: 실제 사용 결과 수집 및 반영\n",
    "- **버전 관리**: 프롬프트 변경사항 추적\n",
    "\n",
    "#### 4. 운영 고려사항\n",
    "- **비용 모니터링**: 토큰 사용량 및 API 호출 비용 추적\n",
    "- **지연 시간**: 응답 시간 SLA 설정 및 모니터링\n",
    "- **오류 처리**: 실패 시 fallback 전략\n",
    "- **보안**: 민감 정보 필터링 및 접근 제어"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajou-llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
