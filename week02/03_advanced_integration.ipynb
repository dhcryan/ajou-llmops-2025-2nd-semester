{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week02 - Advanced Integration Techniques\n",
    "\n",
    "본 노트북은 고급 프롬프트 엔지니어링 통합 기법들을 실습합니다.\n",
    "\n",
    "## 다룰 기법들\n",
    "1. **Function/Tool Calling**: 외부 함수/도구 호출 및 결과 활용\n",
    "2. **Multiple Chains**: 파이프라인 연결 및 단계별 처리\n",
    "3. **Meta-Prompting**: 프롬프트 자체를 진단하고 개선\n",
    "4. **APE (Automatic Prompt Engineering)**: 프롬프트 자동 생성 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라이브러리 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치 및 import\n",
    "import subprocess\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "from io import StringIO\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI 라이브러리\n",
    "try:\n",
    "    from openai import OpenAI, APIError, RateLimitError\n",
    "except ImportError:\n",
    "    !pip install openai\n",
    "    from openai import OpenAI, APIError, RateLimitError\n",
    "\n",
    "# 설정\n",
    "client = OpenAI()  # 환경변수에서 API 키 자동 로드\n",
    "\n",
    "# 헬퍼 함수들\n",
    "def run_ollama(model: str, prompt: str) -> str:\n",
    "    \"\"\"Ollama 모델 실행\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", model],\n",
    "            input=prompt,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            timeout=60\n",
    "        )\n",
    "        return result.stdout.strip()\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: Timeout\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def run_openai(prompt: str, model: str = \"gpt-4o-mini\", **kwargs) -> str:\n",
    "    \"\"\"OpenAI 모델 실행\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            **kwargs\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def safe_json_parse(text: str) -> Union[dict, None]:\n",
    "    \"\"\"안전한 JSON 파싱\"\"\"\n",
    "    try:\n",
    "        # 코드 블록 제거\n",
    "        clean_text = re.sub(r'```json\\s*|```\\s*', '', text).strip()\n",
    "        return json.loads(clean_text)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print(\"라이브러리 로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Function/Tool Calling\n",
    "\n",
    "**핵심 아이디어**: LLM이 외부 함수/도구를 \"이름+파라미터\"로 호출 → 실제 결과를 받아 최종 답변에 반영\n",
    "\n",
    "**설계 팁**:\n",
    "- 스키마 명세: `name`, `description`, `parameters(JSON Schema)` 명확히\n",
    "- Idempotency: 재시도 시 중복 실행 방지\n",
    "- 타임아웃/백오프: 툴 실패·지연 대응\n",
    "- 보안: 파라미터 화이트리스트, 출력 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama 툴 호출 시뮬레이션 ===\n",
      "생성된 툴 호출: `{\"name\": \"get_weather\", \"args\": {\"city\": \"Seoul\", \"date\": \"tomorrow\"}}`\n",
      "JSON 파싱 실패\n"
     ]
    }
   ],
   "source": [
    "# 1-1. Ollama 툴 시뮬레이션\n",
    "def simulate_tool_calling_ollama():\n",
    "    \"\"\"Ollama로 툴 호출 시뮬레이션\"\"\"\n",
    "    \n",
    "    # 1) 모델에게 툴 호출 JSON 생성하도록 유도\n",
    "    tool_request_prompt = \"\"\"Return ONLY JSON tool_call: \n",
    "    {\"name\": \"get_weather\", \"args\": {\"city\": \"Seoul\", \"date\": \"tomorrow\"}}\"\"\"\n",
    "    \n",
    "    print(\"=== Ollama 툴 호출 시뮬레이션 ===\")\n",
    "    tool_call_json = run_ollama(\"llama3.1:8b\", tool_request_prompt)\n",
    "    print(f\"생성된 툴 호출: {tool_call_json}\")\n",
    "    \n",
    "    # 2) 더미 툴 함수들 정의\n",
    "    def fake_tools_handler(tool_call: dict) -> dict:\n",
    "        \"\"\"더미 툴 핸들러\"\"\"\n",
    "        name = tool_call.get(\"name\", \"\")\n",
    "        args = tool_call.get(\"args\", {})\n",
    "        \n",
    "        if name == \"get_weather\":\n",
    "            return {\"forecast\": \"Sunny\", \"temp_c\": 27, \"humidity\": \"60%\"}\n",
    "        elif name == \"fx_usd_krw\":\n",
    "            return {\"rate\": 1385.2, \"trend\": \"stable\"}\n",
    "        else:\n",
    "            return {\"error\": \"unknown tool\"}\n",
    "    \n",
    "    # 3) JSON 파싱 후 툴 실행\n",
    "    try:\n",
    "        tool_data = json.loads(tool_call_json)\n",
    "        result = fake_tools_handler(tool_data)\n",
    "        print(f\"툴 실행 결과: {result}\")\n",
    "        \n",
    "        # 4) 결과를 바탕으로 최종 응답 생성\n",
    "        final_prompt = f\"\"\"Based on this tool result: {result}\n",
    "        Write a one-line weather summary in Korean.\"\"\"\n",
    "        \n",
    "        final_response = run_ollama(\"llama3.1:8b\", final_prompt)\n",
    "        print(f\"최종 응답: {final_response}\")\n",
    "        \n",
    "    except json.JSONDecodeError:\n",
    "        print(\"JSON 파싱 실패\")\n",
    "\n",
    "simulate_tool_calling_ollama()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI Function Calling 데모 ===\n",
      "\n",
      "--- 테스트 1: 서울 날씨 확인해줘 ---\n",
      "도구 호출: get_weather({'city': '서울', 'date': '오늘'})\n",
      "도구 결과: {'forecast': '맑음', 'temp_c': 27, 'humidity': '65%'}\n",
      "최종 응답: 오늘 서울 날씨는 맑음입니다. 기온은 약 27°C, 습도는 65%입니다.  \n",
      "야외 활동하기 좋은 날이니 가벼운 옷차림을 추천드리고, 자외선 차단제는 챙기세요. 우산은 필요하지 않을 것 같습니다.  \n",
      "다른 시간대 예보나 내일 날씨도 알려드릴까요?\n",
      "\n",
      "--- 테스트 2: USD-KRW 환율이랑 25 * 47 계산 결과 알려줘 ---\n",
      "도구 호출: fx_usd_krw({})\n",
      "도구 결과: {'rate': 1385.2, 'change': '+0.5%', 'timestamp': '2024-01-15 09:00'}\n",
      "도구 호출: calculate_math({'expression': '25 * 47'})\n",
      "도구 결과: {'result': 1175, 'expression': '25 * 47'}\n",
      "최종 응답: USD-KRW 환율: 1,385.2원 (변동 +0.5%, 기준 시각 2024-01-15 09:00)  \n",
      "25 × 47 = 1,175\n",
      "\n",
      "더 최신 시세가 필요하시면 알려주세요.\n",
      "\n",
      "--- 테스트 3: 부산 날씨와 현재 환율로 여행 예산 조언해줘 ---\n",
      "도구 호출: get_weather({'city': '부산', 'date': '오늘'})\n",
      "도구 결과: {'error': 'City not found'}\n",
      "도구 호출: fx_usd_krw({})\n",
      "도구 결과: {'rate': 1385.2, 'change': '+0.5%', 'timestamp': '2024-01-15 09:00'}\n",
      "최종 응답: 먼저 알려드린 환율(및 날씨 조회) 상태에 대해 안내드립니다.\n",
      "- 환율(제가 조회한 값): 1 USD = 1,385.2 KRW (출처 타임스탬프: 2024-01-15 09:00). 이 값은 최신이 아닐 수 있으니 실환전 전에 실시간 시세·환전 수수료를 꼭 확인하세요.\n",
      "- 부산 날씨 조회는 시도했지만 현재 도시 조회에 실패했습니다. (다시 시도해도 되고, 여행 날짜를 알려주시면 정확한 예보로 재조회해 드릴게요.)\n",
      "\n",
      "우선 현재 가진 환율로 예산 샘플을 만들어 드립니다. 원하시는 여행 기간·인원·여행 스타일(저예산/중간/고급)을 알려주시면 더 정확히 맞춰 드릴게요.\n",
      "\n",
      "환율 기준: 1 USD = 1,385.2 KRW\n",
      "\n",
      "예산 가이드 (1인 기준, 환율 적용 예시)\n",
      "- 하루당 예상(대략)\n",
      "  - 저예산: 40,000 KRW ≒ 29 USD\n",
      "  - 중간형: 100,000 KRW ≒ 72 USD\n",
      "  - 고급형: 250,000 KRW ≒ 180 USD\n",
      "\n",
      "- 총액 예시\n",
      "  - 3일\n",
      "    - 저예산: 120,000 KRW ≒ 86.6 USD\n",
      "    - 중간형: 300,000 KRW ≒ 216.5 USD\n",
      "    - 고급형: 750,000 KRW ≒ 541.4 USD\n",
      "  - 5일\n",
      "    - 저예산: 200,000 KRW ≒ 144.3 USD\n",
      "    - 중간형: 500,000 KRW ≒ 360.9 USD\n",
      "    - 고급형: 1,250,000 KRW ≒ 901.8 USD\n",
      "\n",
      "예산 구성(권장 배분)\n",
      "- 숙박: 전체 예산의 40~50% (게스트하우스/비즈니스호텔/리조트에 따라 다름)\n",
      "- 식비: 하루 10,000~40,000 KRW (길거리/현지식 vs 레스토랑)\n",
      "- 교통: 시내교통(지하철·버스) 싸고, 공항-도심 이동 비용 별도\n",
      "- 관광·체험: 입장료·투어 비용 예산에 포함\n",
      "- 여유비: 총 예산의 10~20% (환전 수수료·기념품·비상)\n",
      "\n",
      "환전·결제 팁\n",
      "- 카드 사용이 편리(대부분 매장·택시에서 사용 가능). 소액은 현금 권장.\n",
      "- 환전은 공항·시내 환전소·은행 수수료 확인. 해외카드 수수료(먹통 수수료) 고려.\n",
      "- 큰 금액은 카드로 결제하고, 현금은 교통·시장·작은 가게용으로 약간 준비.\n",
      "\n",
      "날씨 관련\n",
      "- 현재(실시간) 부산 날씨를 다시 조회하려면 여행 날짜(예: “이번주 토요일”)를 알려 주세요.\n",
      "- 일반 참고: 9월 초(현재 시스템 날짜 기준 9월)는 부산이 여전히 따뜻하고 습함(낮 23–30°C 범위), 태풍/비가 올 수 있으니 우산·간편한 방수 대비 권장합니다. (정확한 예보는 날짜 지정 후 재조회 필요)\n",
      "\n",
      "원하시면 지금 바로:\n",
      "- 정확한 여행 날짜/기간과 인원 및 여행 스타일을 알려 주세요 → 최신 환율로 다시 확인하고, 해당 날짜의 부산 일기예보(또는 7일간 예보) 조회해 맞춤 예산 다시 계산해 드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# 1-2. OpenAI 실제 Function Calling\n",
    "def openai_function_calling_demo():\n",
    "    \"\"\"OpenAI Function Calling 실제 구현\"\"\"\n",
    "    \n",
    "    # 실제 도구 함수들\n",
    "    def get_weather(city: str, date: str = \"today\") -> dict:\n",
    "        \"\"\"날씨 정보 조회 (더미 데이터)\"\"\"\n",
    "        weather_db = {\n",
    "            \"Seoul\": {\"forecast\": \"Sunny\", \"temp_c\": 27, \"humidity\": \"65%\"},\n",
    "            \"Busan\": {\"forecast\": \"Cloudy\", \"temp_c\": 24, \"humidity\": \"70%\"},\n",
    "            \"서울\": {\"forecast\": \"맑음\", \"temp_c\": 27, \"humidity\": \"65%\"}\n",
    "        }\n",
    "        return weather_db.get(city, {\"error\": \"City not found\"})\n",
    "    \n",
    "    def fx_usd_krw() -> dict:\n",
    "        \"\"\"USD->KRW 환율 조회 (더미 데이터)\"\"\"\n",
    "        return {\"rate\": 1385.2, \"change\": \"+0.5%\", \"timestamp\": \"2024-01-15 09:00\"}\n",
    "    \n",
    "    def calculate_math(expression: str) -> dict:\n",
    "        \"\"\"안전한 수학 계산\"\"\"\n",
    "        try:\n",
    "            if re.match(r'^[0-9+\\-*/().\\s]+$', expression):\n",
    "                result = eval(expression)  # 실제로는 더 안전한 파서 사용 권장\n",
    "                return {\"result\": result, \"expression\": expression}\n",
    "            else:\n",
    "                return {\"error\": \"Unsupported expression\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    # Function Calling 도구 스키마 정의\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"지정된 도시의 날씨 정보를 조회합니다\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"city\": {\"type\": \"string\", \"description\": \"도시 이름\"},\n",
    "                        \"date\": {\"type\": \"string\", \"description\": \"날짜 (오늘/내일/yyyy-mm-dd)\"}\n",
    "                    },\n",
    "                    \"required\": [\"city\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"fx_usd_krw\",\n",
    "                \"description\": \"USD에서 KRW로의 현재 환율을 조회합니다\",\n",
    "                \"parameters\": {\"type\": \"object\", \"properties\": {}}\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\", \n",
    "            \"function\": {\n",
    "                \"name\": \"calculate_math\",\n",
    "                \"description\": \"수학 계산을 수행합니다\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\"type\": \"string\", \"description\": \"계산할 수식\"}\n",
    "                    },\n",
    "                    \"required\": [\"expression\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"=== OpenAI Function Calling 데모 ===\")\n",
    "    \n",
    "    # 테스트 케이스들\n",
    "    test_queries = [\n",
    "        \"서울 날씨 확인해줘\",\n",
    "        \"USD-KRW 환율이랑 25 * 47 계산 결과 알려줘\",\n",
    "        \"부산 날씨와 현재 환율로 여행 예산 조언해줘\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n--- 테스트 {i}: {query} ---\")\n",
    "        \n",
    "        # 첫 번째 요청\n",
    "        messages = [{\"role\": \"user\", \"content\": query}]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "        \n",
    "        # 도구 호출 처리\n",
    "        if response.choices[0].message.tool_calls:\n",
    "            messages.append(response.choices[0].message)\n",
    "            \n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                try:\n",
    "                    arguments = json.loads(tool_call.function.arguments)\n",
    "                except:\n",
    "                    arguments = {}\n",
    "                \n",
    "                print(f\"도구 호출: {function_name}({arguments})\")\n",
    "                \n",
    "                # 실제 함수 호출\n",
    "                if function_name == \"get_weather\":\n",
    "                    result = get_weather(**arguments)\n",
    "                elif function_name == \"fx_usd_krw\":\n",
    "                    result = fx_usd_krw()\n",
    "                elif function_name == \"calculate_math\":\n",
    "                    result = calculate_math(arguments.get(\"expression\", \"\"))\n",
    "                else:\n",
    "                    result = {\"error\": \"Unknown function\"}\n",
    "                \n",
    "                print(f\"도구 결과: {result}\")\n",
    "                \n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": json.dumps(result, ensure_ascii=False)\n",
    "                })\n",
    "            \n",
    "            # 최종 응답 생성\n",
    "            final_response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "            )\n",
    "            \n",
    "            print(f\"최종 응답: {final_response.choices[0].message.content}\")\n",
    "        else:\n",
    "            print(f\"도구 호출 없음: {response.choices[0].message.content}\")\n",
    "\n",
    "openai_function_calling_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple Chains\n",
    "\n",
    "**핵심 아이디어**: 파이프라인 연결 - 정제→요약→분류→행동추천\n",
    "\n",
    "**패턴**:\n",
    "- 팬아웃: 동일 입력→여러 모델/프롬프트 병렬\n",
    "- 팬인: 여러 결과→집계/투표\n",
    "\n",
    "**설계 포인트**:\n",
    "- 각 단계의 입출력 스키마 고정(JSON)\n",
    "- 에러 분기(누락 필드→재질문/기본값)\n",
    "- 캐시/재사용: 반복 호출 절감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama Multiple Chains 처리 ===\n",
      "입력: 배송이 너무 늦었고, 포장도 찢어져 왔습니다. 환불 절차 알려주세요. 정말 화가 나네요.\n",
      "\n",
      "[Stage 1] 요약: 배송이 매우늦게 도착했으며 제품의 포장이 찢어져있었습니다. 환불을 처리하려면 어떻게 해야할까요? 정말 기대가 안된 일이었다는 생각에 큰 불편함을 느끼고 있습니다.\n",
      "\n",
      "[Stage 2] 의도 분류: Based on the text, I would classify the intent as:\n",
      "\n",
      "{\"intent\": \"refund\", \"confidence\": 0.9}\n",
      "\n",
      "Here's my reasoning:\n",
      "\n",
      "* The user mentions that the delivery was delayed and the packaging was damaged (\"배송이 너무 늦었고, 포장도 찢어져 왔습니다.\"), which suggests a problem with the product or service.\n",
      "* The user explicitly asks for the refund procedure (\"환불 절차 알려주세요.\"), indicating a clear intent to request a refund.\n",
      "* The tone of the text is negative and frustrated (\"정말 화가 나네요\"), but this doesn't necessarily affect the classification, as it's still focused on requesting a refund.\n",
      "\n",
      "Note that the confidence level is 0.9 because there's no explicit mention of an exchange or any other specific issue, so while it's likely that the user wants a refund, it's not a certainty. However, based on the context and language used, refund is the most likely intent.\n",
      "\n",
      "[Stage 3] 감정 분석: After analyzing the text, I conclude that:\n",
      "\n",
      "* Emotion: Anger\n",
      "* Intensity: High\n",
      "* Keywords: [\"배송\", \"늦었고\", \"포장\", \"찢어져\", \"환불\", \"절차\", \"화가\"]\n",
      "\n",
      "Here's a breakdown of why I chose these results:\n",
      "\n",
      "* The text contains several words and phrases that indicate strong negative emotions, such as:\n",
      "\t+ \"화가 나네요\" (I'm really angry)\n",
      "\t+ \"배송이 너무 늦었고\" (The delivery was too slow)\n",
      "\t+ \"포장도 찢어져 왔습니다\" (The packaging was torn apart)\n",
      "* The intensity of the emotion is High because of the strong language used and the explicit statement of being angry.\n",
      "* The keywords I extracted are related to the issues mentioned in the text, such as delayed delivery, damaged packaging, and a request for a refund.\n",
      "\n",
      "Here's the JSON output:\n",
      "```\n",
      "{\n",
      "  \"emotion\": \"Anger\",\n",
      "  \"intensity\": \"High\",\n",
      "  \"keywords\": [\"배송\", \"늦었고\", \"포장\", \"찢어져\", \"환불\", \"절차\", \"화가\"]\n",
      "}\n",
      "```\n",
      "\n",
      "[Stage 4] 최종 대응: Here's a polite 3-sentence response in Korean:\n",
      "\n",
      "\"죄송합니다. 배송 및 포장에 문제가 발생한 것을 확인했습니다. 환불을 처리하기 위해, 주문 내역과 함께 고객센터로 연락해 주시면 친절하게 안내드리겠습니다.\" (Sorry about the inconvenience. We've confirmed that there was an issue with delivery and packaging. Please contact our customer service with your order details so we can assist you with the refund process.)\n"
     ]
    }
   ],
   "source": [
    "# 2-1. Ollama Multiple Chains (쉘 파이프라인 시뮬레이션)\n",
    "def ollama_multiple_chains():\n",
    "    \"\"\"Ollama를 사용한 다단계 체인 처리\"\"\"\n",
    "    \n",
    "    input_text = \"배송이 너무 늦었고, 포장도 찢어져 왔습니다. 환불 절차 알려주세요. 정말 화가 나네요.\"\n",
    "    \n",
    "    print(\"=== Ollama Multiple Chains 처리 ===\")\n",
    "    print(f\"입력: {input_text}\")\n",
    "    \n",
    "    # Stage 1: 요약 (3문장)\n",
    "    stage1_prompt = f\"Summarize in Korean in 3 sentences: --- {input_text} ---\"\n",
    "    summary = run_ollama(\"llama3.1:8b\", stage1_prompt)\n",
    "    print(f\"\\n[Stage 1] 요약: {summary}\")\n",
    "    \n",
    "    # Stage 2: 의도 분류 (JSON)\n",
    "    stage2_prompt = f\"\"\"Classify intent -> refund|exchange|question. Return JSON {{\"intent\": \"...\", \"confidence\": 0.0-1.0}}.\n",
    "Text: --- {input_text} ---\"\"\"\n",
    "    classification = run_ollama(\"llama3.1:8b\", stage2_prompt)\n",
    "    print(f\"\\n[Stage 2] 의도 분류: {classification}\")\n",
    "    \n",
    "    # Stage 3: 감정 분석\n",
    "    stage3_prompt = f\"\"\"Analyze emotion intensity -> low|medium|high. Return JSON {{\"emotion\": \"...\", \"intensity\": \"...\", \"keywords\": [...]}}.\n",
    "Text: --- {input_text} ---\"\"\"\n",
    "    emotion = run_ollama(\"llama3.1:8b\", stage3_prompt)\n",
    "    print(f\"\\n[Stage 3] 감정 분석: {emotion}\")\n",
    "    \n",
    "    # Stage 4: 통합 대응문 생성\n",
    "    stage4_prompt = f\"\"\"You are a CS agent. Create a response based on:\n",
    "Summary: {summary}\n",
    "Intent: {classification}\n",
    "Emotion: {emotion}\n",
    "\n",
    "Write a polite 3-sentence response in Korean:\n",
    "1) Acknowledge the emotion\n",
    "2) Apologize if needed\n",
    "3) Provide clear next step\"\"\"\n",
    "    \n",
    "    final_response = run_ollama(\"llama3.1:8b\", stage4_prompt)\n",
    "    print(f\"\\n[Stage 4] 최종 대응: {final_response}\")\n",
    "    \n",
    "    return {\n",
    "        \"original\": input_text,\n",
    "        \"summary\": summary,\n",
    "        \"classification\": classification,\n",
    "        \"emotion\": emotion,\n",
    "        \"response\": final_response\n",
    "    }\n",
    "\n",
    "ollama_result = ollama_multiple_chains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI Multiple Chains 처리 ===\n",
      "\n",
      "--- 케이스 1: 배송이 너무 늦었고, 포장도 찢어져 왔습니다. 환불 절차 알려주세요. ---\n",
      "Stage 1 (정규화): ```json\n",
      "{\n",
      "  \"language\": \"Korean\",\n",
      "  \"normalized\": \"배송이 너무 늦었고, 포장도 찢어져 왔습니다. 환불 절차 알려주세요.\",\n",
      "  \"length\": 39\n",
      "}\n",
      "```\n",
      "Stage 2a (감성): ```json\n",
      "{\n",
      "  \"sentiment\": \"negative\",\n",
      "  \"confidence\": 0.85,\n",
      "  \"keywords\": [\"배송\", \"늦었고\", \"포장\", \"찢어져\", \"환불\", \"절차\"]\n",
      "}\n",
      "```\n",
      "Stage 2b (의도): ```json\n",
      "{\n",
      "  \"intent\": \"refund\",\n",
      "  \"urgency\": \"high\"\n",
      "}\n",
      "```\n",
      "Stage 3 (전략): ```json\n",
      "{\n",
      "  \"strategy\": \"empathize|inform|escalate\",\n",
      "  \"tone\": \"apologetic|urgent\",\n",
      "  \"actions\": [\n",
      "    \"Acknowledge the customer's frustration due to delayed delivery and damaged packaging.\",\n",
      "    \"Provide clear instructions on the refund process.\",\n",
      "    \"Offer direct contact information for a customer service representative who can assist further.\",\n",
      "    \"Prioritize this case for a quick resolution.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Stage 4 (최종응답): 안녕하세요. 배송 지연과 포장이 손상된 점 정말 죄송합니다. 환불 절차에 대해서는 저희 고객 서비스팀에 직접 연락해 주시면 신속하게 도와드리겠습니다. 아래 연락처로 연락 주시면, 우선적으로 이 건을 처리하도록 하겠습니다. 감사합니다.\n",
      "\n",
      "--- 케이스 2: 제품이 정말 좋네요! 다음에도 주문하고 싶어요. 할인 혜택 있나요? ---\n",
      "Stage 1 (정규화): ```json\n",
      "{\n",
      "  \"language\": \"Korean\",\n",
      "  \"normalized\": \"제품이 정말 좋네요! 다음에도 주문하고 싶어요. 할인 혜택 있나요?\",\n",
      "  \"length\": 43\n",
      "}\n",
      "```\n",
      "Stage 2a (감성): ```json\n",
      "{\n",
      "  \"sentiment\": \"positive\",\n",
      "  \"confidence\": 0.95,\n",
      "  \"keywords\": [\"제품\", \"좋네요\", \"주문\", \"할인\", \"혜택\"]\n",
      "}\n",
      "```\n",
      "Stage 2b (의도): ```json\n",
      "{\n",
      "  \"intent\": \"question\",\n",
      "  \"urgency\": \"medium\"\n",
      "}\n",
      "```\n",
      "Stage 3 (전략): ```json\n",
      "{\n",
      "  \"strategy\": \"empathize|celebrate|inform\",\n",
      "  \"tone\": \"friendly\",\n",
      "  \"actions\": [\n",
      "    \"Acknowledge the customer's positive feedback about the product and express appreciation.\",\n",
      "    \"Celebrate the customer's willingness to order again and encourage further engagement.\",\n",
      "    \"Provide information about any current or upcoming discount offers.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Stage 4 (최종응답): 안녕하세요! 제품에 대한 긍정적인 피드백을 주셔서 정말 감사합니다. 고객님께서 다시 주문하고 싶어하신다니 저희도 매우 기쁩니다! 현재 진행 중인 할인 혜택에 대한 정보는 저희 웹사이트를 통해 확인하실 수 있으며, 추가적으로 할인 소식도 정기적으로 보내드릴 예정입니다. 앞으로도 많은 관심 부탁드립니다!\n",
      "\n",
      "--- 케이스 3: 사이즈가 안 맞아서 교환하고 싶은데, 어떻게 해야 하나요? ---\n",
      "Stage 1 (정규화): {\"language\": \"Korean\", \"normalized\": \"사이즈가 안 맞아서 교환하고 싶은데, 어떻게 해야 하나요?\", \"length\": 41}\n",
      "Stage 2a (감성): ```json\n",
      "{\n",
      "  \"sentiment\": \"neutral\",\n",
      "  \"confidence\": 0.85,\n",
      "  \"keywords\": [\"사이즈\", \"교환\", \"어떻게\"]\n",
      "}\n",
      "```\n",
      "Stage 2b (의도): ```json\n",
      "{\"intent\": \"exchange\", \"urgency\": \"medium\"}\n",
      "```\n",
      "Stage 3 (전략): ```json\n",
      "{\n",
      "  \"strategy\": \"empathize|inform\",\n",
      "  \"tone\": \"friendly|neutral\",\n",
      "  \"actions\": [\n",
      "    \"Acknowledge the customer's concern about the size not fitting.\",\n",
      "    \"Provide clear instructions on how to initiate the exchange process.\",\n",
      "    \"Offer assistance in selecting the correct size if needed.\",\n",
      "    \"Ensure the customer knows they can contact support for any further questions.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Stage 4 (최종응답): 안녕하세요! 사이즈가 맞지 않아 불편을 드린 점 이해합니다. 교환을 원하신다면, 저희 웹사이트의 '교환 안내' 섹션에서 간단한 절차를 확인하실 수 있습니다. 필요하시면 적절한 사이즈 선택에 도움이 필요하니 언제든지 문의해 주세요! 추가적인 질문이 있으시면 저희 고객 지원 센터에 연락해주시면 됩니다. 감사합니다!\n"
     ]
    }
   ],
   "source": [
    "# 2-2. OpenAI Multiple Chains (오케스트레이터 패턴)\n",
    "def openai_multiple_chains():\n",
    "    \"\"\"OpenAI를 사용한 고급 다단계 체인 처리\"\"\"\n",
    "    \n",
    "    def chat_stage(prompt: str, **kwargs):\n",
    "        \"\"\"단일 스테이지 실행\"\"\"\n",
    "        return client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            **kwargs\n",
    "        ).choices[0].message.content\n",
    "    \n",
    "    # 테스트 케이스들\n",
    "    test_cases = [\n",
    "        \"배송이 너무 늦었고, 포장도 찢어져 왔습니다. 환불 절차 알려주세요.\",\n",
    "        \"제품이 정말 좋네요! 다음에도 주문하고 싶어요. 할인 혜택 있나요?\",\n",
    "        \"사이즈가 안 맞아서 교환하고 싶은데, 어떻게 해야 하나요?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== OpenAI Multiple Chains 처리 ===\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, text in enumerate(test_cases, 1):\n",
    "        print(f\"\\n--- 케이스 {i}: {text} ---\")\n",
    "        \n",
    "        # Stage 1: 언어 감지 및 정규화\n",
    "        stage1 = chat_stage(\n",
    "            f\"Detect language and normalize text. Return JSON {{\\\"language\\\": \\\"...\\\", \\\"normalized\\\": \\\"...\\\", \\\"length\\\": number}}.\\nText: --- {text} ---\",\n",
    "            max_completion_tokens=150\n",
    "        )\n",
    "        print(f\"Stage 1 (정규화): {stage1}\")\n",
    "        \n",
    "        # Stage 2: 병렬 분석 (감성 + 의도)\n",
    "        stage2a = chat_stage(\n",
    "            f\"Sentiment analysis. Return JSON {{\\\"sentiment\\\": \\\"positive|negative|neutral\\\", \\\"confidence\\\": 0.0-1.0, \\\"keywords\\\": [...]}}.\\nText: --- {text} ---\",\n",
    "            max_completion_tokens=120\n",
    "        )\n",
    "        \n",
    "        stage2b = chat_stage(\n",
    "            f\"Intent classification. Return JSON {{\\\"intent\\\": \\\"refund|exchange|question|compliment\\\", \\\"urgency\\\": \\\"low|medium|high\\\"}}.\\nText: --- {text} ---\",\n",
    "            max_completion_tokens=100\n",
    "        )\n",
    "        \n",
    "        print(f\"Stage 2a (감성): {stage2a}\")\n",
    "        print(f\"Stage 2b (의도): {stage2b}\")\n",
    "        \n",
    "        # Stage 3: 통합 분석 및 전략 수립\n",
    "        stage3 = chat_stage(\n",
    "            f\"\"\"Based on the analysis results, create a response strategy.\n",
    "Normalization: {stage1}\n",
    "Sentiment: {stage2a}\n",
    "Intent: {stage2b}\n",
    "\n",
    "Return JSON:\n",
    "{{\"strategy\": \"empathize|celebrate|inform|escalate\", \"tone\": \"apologetic|friendly|neutral|urgent\", \"actions\": [...]}}\"\"\",\n",
    "            max_completion_tokens=200\n",
    "        )\n",
    "        print(f\"Stage 3 (전략): {stage3}\")\n",
    "        \n",
    "        # Stage 4: 최종 응답 생성\n",
    "        stage4 = chat_stage(\n",
    "            f\"\"\"Generate final customer service response in Korean based on:\n",
    "Strategy: {stage3}\n",
    "Original message: {text}\n",
    "\n",
    "Requirements:\n",
    "- 2-3 sentences\n",
    "- Professional and empathetic tone\n",
    "- Include specific next steps\n",
    "- Match the determined strategy and tone\"\"\",\n",
    "            max_completion_tokens=200\n",
    "        )\n",
    "        print(f\"Stage 4 (최종응답): {stage4}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"input\": text,\n",
    "            \"normalization\": stage1,\n",
    "            \"sentiment\": stage2a,\n",
    "            \"intent\": stage2b,\n",
    "            \"strategy\": stage3,\n",
    "            \"response\": stage4\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "openai_results = openai_multiple_chains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fan-out/Fan-in 패턴 ===\n",
      "입력: 이 제품은 가격 대비 성능이 좋지만, 디자인이 조금 아쉬워요. 전반적으로는 만족합니다.\n",
      "\n",
      "[quality_focus]: ```json\n",
      "{\n",
      "  \"aspect\": \"quality_focus\",\n",
      "  \"rating\": 4,\n",
      "  \"comment\": \"The product offers good performance for its price, indicating acceptable quality, but there are concerns regarding the design, which could impact durability and reliability over time.\",\n",
      "  \"keywords\": [\"performance\", \"durability\", \"reliability\", \"value for money\", \"design\"]\n",
      "}\n",
      "```\n",
      "\n",
      "[price_focus]: ```json\n",
      "{\n",
      "  \"aspect\": \"price_focus\",\n",
      "  \"rating\": 4,\n",
      "  \"comment\": \"The product offers good performance for its price, though the design leaves something to be desired. Overall, it is satisfactory in terms of value.\",\n",
      "  \"keywords\": [\"cost-effectiveness\", \"performance\", \"design\", \"satisfaction\"]\n",
      "}\n",
      "```\n",
      "\n",
      "[design_focus]: ```json\n",
      "{\n",
      "  \"aspect\": \"design_focus\",\n",
      "  \"rating\": 3,\n",
      "  \"comment\": \"The product offers good performance for the price, but the design could be improved, which affects its overall appeal. While the user is generally satisfied, there is room for enhancement in aesthetics.\",\n",
      "  \"keywords\": [\"design\", \"performance\", \"aesthetics\", \"usability\", \"satisfaction\"]\n",
      "}\n",
      "```\n",
      "\n",
      "[satisfaction_focus]: ```json\n",
      "{\n",
      "  \"aspect\": \"satisfaction_focus\",\n",
      "  \"rating\": 4,\n",
      "  \"comment\": \"Overall, I'm satisfied with the product despite some design concerns.\",\n",
      "  \"keywords\": [\"satisfaction\", \"performance\", \"design\", \"recommendation\"]\n",
      "}\n",
      "```\n",
      "\n",
      "[통합 결과]: ```json\n",
      "{\n",
      "  \"overall_rating\": 4,\n",
      "  \"key_strengths\": [\n",
      "    \"Good performance for the price\",\n",
      "    \"High level of satisfaction among users\",\n",
      "    \"Acceptable quality indicating good value for money\"\n",
      "  ],\n",
      "  \"key_weaknesses\": [\n",
      "    \"Design could be improved for better aesthetics\",\n",
      "    \"Concerns regarding durability and reliability due to design\"\n",
      "  ],\n",
      "  \"recommendation\": \"Consider this product if performance and value for money are top priorities, but be aware of potential design drawbacks.\",\n",
      "  \"confidence\": 0.85\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 2-3. Fan-out/Fan-in 패턴 (병렬 처리 + 결과 집계)\n",
    "def fan_out_fan_in_pattern():\n",
    "    \"\"\"팬아웃/팬인 패턴으로 다중 관점 분석\"\"\"\n",
    "    \n",
    "    input_text = \"이 제품은 가격 대비 성능이 좋지만, 디자인이 조금 아쉬워요. 전반적으로는 만족합니다.\"\n",
    "    \n",
    "    print(\"=== Fan-out/Fan-in 패턴 ===\")\n",
    "    print(f\"입력: {input_text}\")\n",
    "    \n",
    "    # Fan-out: 동일 입력을 여러 관점으로 분석\n",
    "    perspectives = {\n",
    "        \"quality_focus\": \"Analyze from quality perspective. Focus on performance, durability, reliability.\",\n",
    "        \"price_focus\": \"Analyze from price/value perspective. Focus on cost-effectiveness, worth.\",\n",
    "        \"design_focus\": \"Analyze from design/UX perspective. Focus on aesthetics, usability.\",\n",
    "        \"satisfaction_focus\": \"Analyze from overall satisfaction perspective. Focus on recommendation likelihood.\"\n",
    "    }\n",
    "    \n",
    "    fan_out_results = {}\n",
    "    \n",
    "    for perspective_name, instruction in perspectives.items():\n",
    "        prompt = f\"\"\"{instruction}\n",
    "        \n",
    "Return JSON: {{\"aspect\": \"{perspective_name}\", \"rating\": 1-5, \"comment\": \"...\", \"keywords\": [...]}}\n",
    "\n",
    "Review: --- {input_text} ---\"\"\"\n",
    "        \n",
    "        result = run_openai(prompt, max_completion_tokens=150)\n",
    "        fan_out_results[perspective_name] = result\n",
    "        print(f\"\\n[{perspective_name}]: {result}\")\n",
    "    \n",
    "    # Fan-in: 여러 관점 결과를 통합\n",
    "    fan_in_prompt = f\"\"\"Aggregate these perspective analyses into a final assessment:\n",
    "    \n",
    "{json.dumps(fan_out_results, indent=2, ensure_ascii=False)}\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"overall_rating\": 1-5,\n",
    "  \"key_strengths\": [...],\n",
    "  \"key_weaknesses\": [...],\n",
    "  \"recommendation\": \"...\",\n",
    "  \"confidence\": 0.0-1.0\n",
    "}}\"\"\"\n",
    "    \n",
    "    final_result = run_openai(fan_in_prompt, max_completion_tokens=250)\n",
    "    print(f\"\\n[통합 결과]: {final_result}\")\n",
    "    \n",
    "    return {\n",
    "        \"input\": input_text,\n",
    "        \"fan_out\": fan_out_results,\n",
    "        \"fan_in\": final_result\n",
    "    }\n",
    "\n",
    "fan_result = fan_out_fan_in_pattern()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Meta-Prompting\n",
    "\n",
    "**핵심 아이디어**: 모델이 프롬프트 자체를 진단/개선\n",
    "\n",
    "**절차**:\n",
    "1. 초기 프롬프트 제시\n",
    "2. 개선 포인트 도출\n",
    "3. 개선안 제시\n",
    "4. 개선안으로 실행\n",
    "\n",
    "**체크리스트**:\n",
    "- 목표/산출형식/제약/예시/실패 시 재질의 포함 여부\n",
    "- 금칙어, 길이 제한, 스키마/포맷, 평가 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama Meta-Prompting ===\n",
      "초기 프롬프트: Classify sentiment of a review.\n",
      "\n",
      "개선된 프롬프트:\n",
      "Here is the improved prompt:\n",
      "\n",
      "--- Classify sentiment of a review as either positive, negative, or neutral, and provide a corresponding confidence score (on a scale of 0-1).\n",
      "\n",
      "**Output Format:**\n",
      "\n",
      "* Return a JSON object with the following fields:\n",
      "\t+ `sentiment`: string (positive, negative, or neutral)\n",
      "\t+ `confidence`: float (confidence score between 0 and 1)\n",
      "\t+ `error`: string (optional, describing any error that occurred during processing)\n",
      "\n",
      "**Constraints:**\n",
      "\n",
      "* The input review text must be at least 10 characters long and not exceed 500 characters.\n",
      "* The response length must not exceed 1024 bytes.\n",
      "\n",
      "**Error Handling Guidelines:**\n",
      "\n",
      "* If the input review is empty or exceeds the character limit, return an `error` field with a descriptive message.\n",
      "* If there is any issue with processing the review (e.g., network error), return an `error` field with a generic error message.\n",
      "\n",
      "**Edge Case Handling:**\n",
      "\n",
      "* Handle cases where the sentiment cannot be determined with high confidence (e.g., ambiguous or neutral text).\n",
      "* Consider using a fallback classification method in case of errors or unexpected input.\n",
      "\n",
      "=== 개선된 프롬프트 테스트 ===\n",
      "\n",
      "테스트 1 (포장이 엉망이었고 배송도 늦었습니다.):\n",
      "결과: Based on the input review, here is the classified sentiment and corresponding confidence score:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"sentiment\": \"negative\",\n",
      "  \"confidence\": 0.8,\n",
      "  \"error\": null\n",
      "}\n",
      "```\n",
      "\n",
      "Here's my reasoning behind the classification:\n",
      "\n",
      "* The review contains negative phrases such as \"\" (포장이 엉망이었고, which means \"the packaging was a mess\") and \"\" (배송도 늦었습니다, which means \"and delivery was also delayed\").\n",
      "* Although there is no explicit positive phrase in the review, the tone of the negative phrases suggests that the sentiment is indeed negative.\n",
      "* The confidence score is 0.8 because while the review is clearly negative, it's not extremely extreme or emotive, suggesting a moderate level of certainty.\n",
      "\n",
      "Note that the error field is null since there are no issues with processing the input review.\n",
      "JSON 유효성: False\n",
      "\n",
      "테스트 2 (제품은 괜찮은데 가격이 좀 비싸요.):\n",
      "결과: Here is the classified sentiment and corresponding confidence score:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"sentiment\": \"negative\",\n",
      "  \"confidence\": 0.8,\n",
      "  \"error\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* The review text contains a negative comment about the price of the product (\"가격이 좀 비싸요\"), which indicates that the overall sentiment is negative.\n",
      "* The confidence score is set to 0.8, as the text is not overwhelmingly negative, but still expresses a critical opinion.\n",
      "* There is no error message, indicating that the input review text meets the constraints and can be processed successfully.\n",
      "\n",
      "**Note:**\n",
      "\n",
      "* The model used for sentiment analysis may have different confidence threshold values, which could affect the classification results. However, based on the provided text, a negative sentiment with moderate confidence (0.8) seems to be a reasonable classification.\n",
      "* It's worth noting that the review also mentions that the product itself is \"괜찮은데\" (fair/okay), which suggests that it has some positive aspects, but the price is the main issue. This nuance may not be fully captured by the sentiment analysis model used here.\n",
      "JSON 유효성: False\n",
      "\n",
      "테스트 3 (완벽합니다! 추천해요!):\n",
      "결과: Here is the classification result:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"sentiment\": \"positive\",\n",
      "  \"confidence\": 0.95,\n",
      "  \"error\": \"\"\n",
      "}\n",
      "```\n",
      "\n",
      "**Reasoning:**\n",
      "\n",
      "The review text `--- 완벽합니다! 추천해요! ---` is a Korean sentence that translates to \"It's perfect! I recommend it!\". The sentiment of this review is clearly positive, indicating that the reviewer had a very favorable experience with the product or service. The confidence score of 0.95 reflects the high degree of certainty in this classification.\n",
      "\n",
      "**Note:**\n",
      "\n",
      "* The review text is within the allowed length limit and contains at least 10 characters.\n",
      "* No errors occurred during processing, so no error message is returned.\n",
      "* The sentiment classification is based on a combination of natural language processing (NLP) techniques, including part-of-speech tagging, named entity recognition, and sentiment analysis. In this case, the positive sentiment is evident from the presence of the words `완벽합니다` (\"It's perfect\") and `추천해요` (\"I recommend it\").\n",
      "JSON 유효성: False\n"
     ]
    }
   ],
   "source": [
    "# 3-1. Ollama Meta-Prompting\n",
    "def ollama_meta_prompting():\n",
    "    \"\"\"Ollama를 사용한 메타 프롬프팅\"\"\"\n",
    "    \n",
    "    initial_prompt = \"Classify sentiment of a review.\"\n",
    "    \n",
    "    print(\"=== Ollama Meta-Prompting ===\")\n",
    "    print(f\"초기 프롬프트: {initial_prompt}\")\n",
    "    \n",
    "    # 1) 개선 요청\n",
    "    improvement_request = f\"\"\"Improve this prompt for reliability & JSON output. \n",
    "Add schema, constraints, and clarifying questions rule:\n",
    "\n",
    "--- {initial_prompt} ---\n",
    "\n",
    "Make it robust for production use with:\n",
    "1. Clear output format specification\n",
    "2. Error handling guidelines  \n",
    "3. Constraints on response length\n",
    "4. Confidence scoring\n",
    "5. Edge case handling\n",
    "\n",
    "Return the improved prompt only.\"\"\"\n",
    "    \n",
    "    improved_prompt = run_ollama(\"llama3.1:8b\", improvement_request)\n",
    "    print(f\"\\n개선된 프롬프트:\\n{improved_prompt}\")\n",
    "    \n",
    "    # 2) 개선된 프롬프트로 실제 테스트\n",
    "    test_reviews = [\n",
    "        \"포장이 엉망이었고 배송도 늦었습니다.\",\n",
    "        \"제품은 괜찮은데 가격이 좀 비싸요.\",\n",
    "        \"완벽합니다! 추천해요!\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== 개선된 프롬프트 테스트 ===\")\n",
    "    \n",
    "    for i, review in enumerate(test_reviews, 1):\n",
    "        test_prompt = f\"\"\"{improved_prompt}\n",
    "\n",
    "Review: --- {review} ---\"\"\"\n",
    "        \n",
    "        result = run_ollama(\"llama3.1:8b\", test_prompt)\n",
    "        print(f\"\\n테스트 {i} ({review}):\")\n",
    "        print(f\"결과: {result}\")\n",
    "        \n",
    "        # JSON 파싱 가능성 체크\n",
    "        is_valid_json = safe_json_parse(result) is not None\n",
    "        print(f\"JSON 유효성: {is_valid_json}\")\n",
    "    \n",
    "    return improved_prompt\n",
    "\n",
    "improved_ollama = ollama_meta_prompting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI Meta-Prompting ===\n",
      "\n",
      "--- 프롬프트 1 개선 ---\n",
      "초기: Classify sentiment of a review.\n",
      "\n",
      "분석 결과: Here's an analysis of the prompt \"Classify sentiment of a review\" along with an improvement plan:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"weaknesses\": [\n",
      "    \"Clarity issues: The prompt does not specify what types of sentiment categories should be used (e.g., positive, negative, neutral).\",\n",
      "    \"Missing constraints: There are no limitations provided regarding the length or format of the review text.\",\n",
      "    \"Output format problems: The expected format of the output (e.g., a label or a score) is not specified.\",\n",
      "    \"Edge case handling: There is no guidance on how to treat ambiguous reviews or those with mixed sentiments.\",\n",
      "    \"Scalability concerns: The prompt does not address how the system should handle large volumes of reviews or varying lengths of text.\"\n",
      "  ],\n",
      "  \"improvement_plan\": [\n",
      "    \"Clarify sentiment categories: Specify a clear set of sentiment categories (e.g., positive, negative, neutral, mixed).\",\n",
      "    \"Add constraints: Define limitations on the input, such as maximum character count or the type of reviews (e.g., product reviews, service reviews).\",\n",
      "    \"Outline output format: Indicate the preferred output format (e.g., JSON structure, simple text label, numeric score).\",\n",
      "    \"Provide edge case guidelines: Include scenarios for handling reviews that are ambiguous or contain mixed sentiments, with examples.\",\n",
      "    \"Consider scalability: Suggest strategies for processing large datasets, such as batch processing or prioritizing certain reviews.\"\n",
      "  ],\n",
      "  \"\n",
      "\n",
      "개선됨: Analyze the sentiment of a given review by classifying it into one of the following categories: \"positive,\" \"negative,\" \"neutral,\" or \"mixed.\" \n",
      "\n",
      "**Input Specifications:**\n",
      "- The input will consist of a text review, which can be up to 2,000 characters in length.\n",
      "- The review can be related to any product or service.\n",
      "\n",
      "**Output Specifications:**\n",
      "- The output should be a JSON object that follows this schema:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"sentiment\": \"string\",      // one of \"positive\", \"negative\", \"neutral\", or \"mixed\"\n",
      "  \"confidence_score\": float,  // a score between 0 and 1 indicating the model's confidence in its classification\n",
      "  \"review_length\": integer,    // the length of the review in characters\n",
      "  \"error\": \"string\"           // optional field to indicate any issues encountered during processing; should be null if no errors exist\n",
      "}\n",
      "```\n",
      "\n",
      "**Error Handling Instructions:**\n",
      "- If the review is empty or exceeds the character limit, return an error message in the \"error\" field describing the issue.\n",
      "- For ambiguous reviews or those containing mixed sentiments, classify the primary sentiment and indicate the confidence score to reflect uncertainty (e.g., for mixed reviews, choose either \"positive\" or \"negative\" based on predominant sentiment).\n",
      "\n",
      "**Additional Notes:**\n",
      "- Ensure the system can handle multiple reviews in succession without loss of context or performance, optimizing for batch processing if necessary.\n",
      "- Use an appropriate confidence scoring method that reflects the reliability of the sentiment classification.\n",
      "\n",
      "--- 프롬프트 2 개선 ---\n",
      "초기: Summarize this text.\n",
      "\n",
      "분석 결과: ```json\n",
      "{\n",
      "  \"weaknesses\": [\n",
      "    \"Clarity issues: The prompt is vague and does not specify the text to be summarized.\",\n",
      "    \"Missing constraints: There are no limits on the length or depth of the summary expected.\",\n",
      "    \"Output format problems: The prompt does not specify the desired format of the summary (e.g., bullet points, paragraph, etc.).\",\n",
      "    \"Edge case handling: The prompt does not address how to deal with particularly complex or lengthy texts.\",\n",
      "    \"Scalability concerns: The prompt does not account for how summaries might vary based on the type or genre of the text.\"\n",
      "  ],\n",
      "  \"improvement_plan\": [\n",
      "    \"Specify the text to be summarized clearly to avoid ambiguity.\",\n",
      "    \"Introduce constraints on the length and detail of the summary to provide guidance.\",\n",
      "    \"Define the output format to ensure consistency in summaries (e.g., word count, paragraph style).\",\n",
      "    \"Include instructions for handling complex texts, such as strategies for distilling information effectively.\",\n",
      "    \"Outline expectations for summarizing different types of texts (academic, narrative, technical, etc.) to enhance scalability.\"\n",
      "  ],\n",
      "  \"priority_fixes\": [\n",
      "    \"Clarify the input text that needs to be summarized.\",\n",
      "    \"Define constraints on summary length and detail.\",\n",
      "    \"Specify the output format for the summary.\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "개선됨: ```json\n",
      "{\n",
      "  \"prompt\": {\n",
      "    \"description\": \"Summarize the provided text according to the specifications below.\",\n",
      "    \"input\": {\n",
      "      \"text\": {\n",
      "        \"type\": \"string\",\n",
      "        \"description\": \"The text that needs to be summarized. Please provide clear, coherent content.\"\n",
      "      }\n",
      "    },\n",
      "    \"output\": {\n",
      "      \"summary\": {\n",
      "        \"type\": \"string\",\n",
      "        \"description\": \"A concise summary of the text provided.\",\n",
      "        \"constraints\": {\n",
      "          \"length\": {\n",
      "            \"max\": 150,\n",
      "            \"min\": 50\n",
      "          },\n",
      "          \"format\": \"Paragraph style, no bullets or lists.\"\n",
      "        }\n",
      "      },\n",
      "      \"confidence_score\": {\n",
      "        \"type\": \"number\",\n",
      "        \"description\": \"A score from 0 to 1 indicating the confidence level in the summary accuracy.\"\n",
      "      }\n",
      "    },\n",
      "    \"error_handling\": {\n",
      "      \"instructions\": [\n",
      "        \"If the input text is empty, return an error message: 'Text input is required.'\",\n",
      "        \"If the text exceeds 1000 words, return an error message: 'Text is too long to summarize; please shorten it and try again.'\",\n",
      "        \"If the summary length does not meet the specified constraints, return an error message: 'Summary must be between 50 and 150 words.'\"\n",
      "      ]\n",
      "    },\n",
      "    \"edge_cases\": {\n",
      "      \"instructions\": [\n",
      "        \"For particularly complex texts, focus on identifying the main arguments and supporting evidence.\",\n",
      "        \"In case of ambiguous terms or phrases, provide definitions or explanations in parentheses to enhance clarity.\"\n",
      "      ]\n",
      "    },\n",
      "    \"scalability\": {\n",
      "      \"approach\": [\n",
      "        \"Academic texts should highlight hypotheses, methods, and conclusions.\",\n",
      "        \"Narrative texts should focus on plot, characters, and themes.\",\n",
      "        \"Technical texts should distill key principles and practical applications.\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "--- 프롬프트 3 개선 ---\n",
      "초기: Answer the question about the document.\n",
      "\n",
      "분석 결과: Here’s an analysis of the prompt along with an improvement plan in the requested JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"weaknesses\": [\n",
      "    \"Clarity issues: The prompt does not specify what the question is or which document it refers to, making it vague.\",\n",
      "    \"Missing constraints: There are no constraints on how to answer the question (e.g., word limit, detail required).\",\n",
      "    \"Output format problems: The prompt does not specify the required format or structure of the answer, which could lead to inconsistent responses.\",\n",
      "    \"Edge case handling: It does not account for potential edge cases, such as if the document is ambiguous or if the question is not clear.\",\n",
      "    \"Scalability concerns: The prompt lacks scalability, as it does not specify how to handle multiple documents or questions.\"\n",
      "  ],\n",
      "  \"improvement_plan\": [\n",
      "    \"Clarify the prompt by specifying the exact question and the document being referred to.\",\n",
      "    \"Define constraints, such as required answer length and level of detail necessary for a complete response.\",\n",
      "    \"Specify the output format (e.g., bullet points, paragraphs, or numerical responses) to ensure consistency.\",\n",
      "    \"Include guidance on how to handle ambiguous questions or documents, potentially suggesting follow-up questions for clarification.\",\n",
      "    \"Design a scalable format that allows for addressing multiple documents or questions within the same request.\"\n",
      "  ],\n",
      "  \"priority_fixes\": [\n",
      "    \"Enhance clarity by specifying the question and document.\",\n",
      "    \"Define output format to\n",
      "\n",
      "개선됨: ```json\n",
      "{\n",
      "  \"prompt\": {\n",
      "    \"description\": \"Please provide a detailed answer to the specified question based on the provided document.\",\n",
      "    \"input\": {\n",
      "      \"document\": {\n",
      "        \"type\": \"string\",\n",
      "        \"description\": \"The text of the document to be analyzed.\"\n",
      "      },\n",
      "      \"question\": {\n",
      "        \"type\": \"string\",\n",
      "        \"description\": \"The specific question regarding the document.\"\n",
      "      }\n",
      "    },\n",
      "    \"output\": {\n",
      "      \"answer\": {\n",
      "        \"type\": \"string\",\n",
      "        \"description\": \"A comprehensive response to the question, adhering to the specified constraints.\"\n",
      "      },\n",
      "      \"confidence_score\": {\n",
      "        \"type\": \"number\",\n",
      "        \"description\": \"A score from 0 to 1 indicating the confidence level of the answer, where 1 means highly confident.\"\n",
      "      }\n",
      "    },\n",
      "    \"constraints\": {\n",
      "      \"length\": {\n",
      "        \"min\": 100,\n",
      "        \"max\": 500,\n",
      "        \"description\": \"The answer must be between 100 and 500 words.\"\n",
      "      },\n",
      "      \"format\": {\n",
      "        \"description\": \"The answer should be structured in paragraphs, with clear headings for sections if necessary.\"\n",
      "      }\n",
      "    },\n",
      "    \"error_handling\": {\n",
      "      \"ambiguity\": \"If the question is unclear or the document contains ambiguous information, provide possible interpretations and seek clarification if needed.\",\n",
      "      \"missing_information\": \"If the required information is not present in the document to answer the question, state what additional information is needed.\"\n",
      "    },\n",
      "    \"edge_cases\": {\n",
      "      \"multiple_documents\": \"If there are multiple documents, summarize the main points from each document that are relevant to the question.\",\n",
      "      \"unanswerable_questions\": \"If the question cannot be answered based on the document, provide a rationale explaining the limitations.\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "=== 개선 품질 평가 ===\n",
      "\n",
      "프롬프트 1 평가: {\n",
      "  \"clarity_score\": 9,\n",
      "  \"robustness_score\": 8,\n",
      "  \"production_score\": 9,\n",
      "  \"completeness_score\": 9,\n",
      "  \"overall_score\": 8.75,\n",
      "  \"comments\": \"The improved prompt provides a clear and detailed instruction for sentiment analysis, breaking down input and output specifications effectively. It handles edge cases well, such as error handling for empty or excessively long reviews. The additional notes about batching and confidence scoring enhance its robustness, making it ready for production scenarios. However, there might be room for minor improvement in terms of handling edge cases further, especially around ambiguous inputs.\" \n",
      "}\n",
      "\n",
      "프롬프트 2 평가: ```json\n",
      "{\n",
      "  \"clarity_score\": 9,\n",
      "  \"robustness_score\": 10,\n",
      "  \"production_score\": 9,\n",
      "  \"completeness_score\": 10,\n",
      "  \"overall_score\": 9,\n",
      "  \"comments\": \"The improved prompt transformation significantly enhances clarity by providing structured details on how to summarize the text, including input requirements, output specifications, and error handling instructions. It is robust due to comprehensive error checks and guidance for edge cases that could arise during text summarization. The format aligns well with production readiness, demonstrating clear expectations for both input and output. Completeness is high, as the transformation covers various scenarios and requirements for summarization, ensuring all potential aspects are addressed.\"\n",
      "}\n",
      "```\n",
      "\n",
      "프롬프트 3 평가: ```json\n",
      "{\n",
      "  \"clarity_score\": 9,\n",
      "  \"robustness_score\": 10,\n",
      "  \"production_score\": 9,\n",
      "  \"completeness_score\": 10,\n",
      "  \"overall_score\": 9,\n",
      "  \"comments\": \"The improved prompt significantly enhances clarity by providing detailed structure and definitions for inputs and outputs. The robustness score is high due to comprehensive error handling and edge cases being covered. The format is production-ready, making it easy for developers to implement. Completeness is also at a maximum as all necessary aspects of the question-answering process have been addressed, including constraints and necessary details.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 3-2. OpenAI Meta-Prompting (더 정교한 개선)\n",
    "def openai_meta_prompting():\n",
    "    \"\"\"OpenAI를 사용한 고급 메타 프롬프팅\"\"\"\n",
    "    \n",
    "    initial_prompts = [\n",
    "        \"Classify sentiment of a review.\",\n",
    "        \"Summarize this text.\",\n",
    "        \"Answer the question about the document.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== OpenAI Meta-Prompting ===\")\n",
    "    \n",
    "    improved_prompts = []\n",
    "    \n",
    "    for i, initial in enumerate(initial_prompts, 1):\n",
    "        print(f\"\\n--- 프롬프트 {i} 개선 ---\")\n",
    "        print(f\"초기: {initial}\")\n",
    "        \n",
    "        # 1) 프롬프트 분석 및 개선 계획\n",
    "        analysis_prompt = f\"\"\"Analyze this prompt for weaknesses and create an improvement plan:\n",
    "        \n",
    "Prompt: \"{initial}\"\n",
    "\n",
    "Analyze:\n",
    "1. Clarity issues\n",
    "2. Missing constraints\n",
    "3. Output format problems\n",
    "4. Edge case handling\n",
    "5. Scalability concerns\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"weaknesses\": [...],\n",
    "  \"improvement_plan\": [...],\n",
    "  \"priority_fixes\": [...]\n",
    "}}\"\"\"\n",
    "        \n",
    "        analysis = run_openai(analysis_prompt, max_completion_tokens=300)\n",
    "        print(f\"\\n분석 결과: {analysis}\")\n",
    "        \n",
    "        # 2) 개선된 프롬프트 생성\n",
    "        improvement_prompt = f\"\"\"Based on this analysis, create a robust, production-ready prompt:\n",
    "\n",
    "Analysis: {analysis}\n",
    "Original: \"{initial}\"\n",
    "\n",
    "Create an improved prompt that:\n",
    "1. Has clear input/output specifications\n",
    "2. Includes JSON schema\n",
    "3. Has error handling instructions\n",
    "4. Specifies constraints (length, format, etc.)\n",
    "5. Includes confidence scoring\n",
    "6. Handles edge cases\n",
    "\n",
    "Return only the improved prompt text.\"\"\"\n",
    "        \n",
    "        improved = run_openai(improvement_prompt, max_completion_tokens=400)\n",
    "        print(f\"\\n개선됨: {improved}\")\n",
    "        \n",
    "        improved_prompts.append({\n",
    "            \"original\": initial,\n",
    "            \"analysis\": analysis,\n",
    "            \"improved\": improved\n",
    "        })\n",
    "    \n",
    "    # 3) 개선된 프롬프트들의 품질 평가\n",
    "    print(\"\\n=== 개선 품질 평가 ===\")\n",
    "    \n",
    "    for i, prompt_data in enumerate(improved_prompts, 1):\n",
    "        evaluation_prompt = f\"\"\"Rate the improvement quality of this prompt transformation:\n",
    "\n",
    "Original: \"{prompt_data['original']}\"\n",
    "Improved: \"{prompt_data['improved']}\"\n",
    "\n",
    "Rate (1-10) on:\n",
    "- Clarity improvement\n",
    "- Robustness\n",
    "- Production readiness\n",
    "- Completeness\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"clarity_score\": 1-10,\n",
    "  \"robustness_score\": 1-10,\n",
    "  \"production_score\": 1-10,\n",
    "  \"completeness_score\": 1-10,\n",
    "  \"overall_score\": 1-10,\n",
    "  \"comments\": \"...\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        evaluation = run_openai(evaluation_prompt, max_completion_tokens=250)\n",
    "        print(f\"\\n프롬프트 {i} 평가: {evaluation}\")\n",
    "    \n",
    "    return improved_prompts\n",
    "\n",
    "improved_openai = openai_meta_prompting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version sentiment_v1.0 added\n",
      "Version sentiment_v2.0 added\n",
      "=== 프롬프트 A/B 테스트 ===\n",
      "\n",
      "A/B 테스트 결과:\n",
      "sentiment_v1.0:\n",
      "  평균 점수: 0.400\n",
      "  평균 지연: 2.307s\n",
      "  JSON 유효율: 0.000\n",
      "sentiment_v2.0:\n",
      "  평균 점수: 0.867\n",
      "  평균 지연: 1.561s\n",
      "  JSON 유효율: 1.000\n",
      "\n",
      "최고 성능: sentiment_v2.0\n"
     ]
    }
   ],
   "source": [
    "# 3-3. 프롬프트 버전 관리 시스템\n",
    "class PromptVersionManager:\n",
    "    \"\"\"프롬프트 버전 관리 및 A/B 테스트\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.versions = {}\n",
    "        self.test_results = {}\n",
    "    \n",
    "    def add_version(self, name: str, version: str, prompt: str, metadata: dict = None):\n",
    "        \"\"\"새 프롬프트 버전 추가\"\"\"\n",
    "        key = f\"{name}_v{version}\"\n",
    "        self.versions[key] = {\n",
    "            \"prompt\": prompt,\n",
    "            \"metadata\": metadata or {},\n",
    "            \"created_at\": time.time()\n",
    "        }\n",
    "        print(f\"Version {key} added\")\n",
    "    \n",
    "    def run_ab_test(self, name: str, versions: list, test_cases: list):\n",
    "        \"\"\"A/B 테스트 실행\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for version in versions:\n",
    "            key = f\"{name}_v{version}\"\n",
    "            if key not in self.versions:\n",
    "                continue\n",
    "                \n",
    "            version_results = []\n",
    "            prompt_template = self.versions[key][\"prompt\"]\n",
    "            \n",
    "            for test_case in test_cases:\n",
    "                # 프롬프트에 테스트 케이스 삽입\n",
    "                full_prompt = prompt_template.replace(\"{input}\", test_case[\"input\"])\n",
    "                \n",
    "                start_time = time.time()\n",
    "                result = run_openai(full_prompt, temperature=0.1, max_tokens=200)\n",
    "                end_time = time.time()\n",
    "                \n",
    "                # 결과 평가\n",
    "                score = self._evaluate_result(result, test_case.get(\"expected\"))\n",
    "                \n",
    "                version_results.append({\n",
    "                    \"input\": test_case[\"input\"],\n",
    "                    \"output\": result,\n",
    "                    \"score\": score,\n",
    "                    \"latency\": end_time - start_time,\n",
    "                    \"json_valid\": safe_json_parse(result) is not None\n",
    "                })\n",
    "            \n",
    "            results[key] = version_results\n",
    "        \n",
    "        # 결과 분석\n",
    "        analysis = self._analyze_ab_results(results)\n",
    "        self.test_results[f\"{name}_ab_{int(time.time())}\"] = {\n",
    "            \"results\": results,\n",
    "            \"analysis\": analysis\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _evaluate_result(self, result: str, expected: dict = None) -> float:\n",
    "        \"\"\"결과 품질 평가 (간단한 버전)\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # JSON 유효성 (+30점)\n",
    "        if safe_json_parse(result):\n",
    "            score += 0.3\n",
    "        \n",
    "        # 길이 적절성 (+20점)\n",
    "        if 50 <= len(result) <= 500:\n",
    "            score += 0.2\n",
    "        \n",
    "        # 한글 포함 여부 (+20점)\n",
    "        if re.search(r'[가-힣]', result):\n",
    "            score += 0.2\n",
    "        \n",
    "        # 기본 구조 (+30점)\n",
    "        if any(keyword in result.lower() for keyword in ['label', 'confidence', 'reason']):\n",
    "            score += 0.3\n",
    "        \n",
    "        return min(1.0, score)\n",
    "    \n",
    "    def _analyze_ab_results(self, results: dict) -> dict:\n",
    "        \"\"\"A/B 테스트 결과 분석\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        for version, version_results in results.items():\n",
    "            scores = [r[\"score\"] for r in version_results]\n",
    "            latencies = [r[\"latency\"] for r in version_results]\n",
    "            json_valid_rate = sum(1 for r in version_results if r[\"json_valid\"]) / len(version_results)\n",
    "            \n",
    "            analysis[version] = {\n",
    "                \"avg_score\": sum(scores) / len(scores),\n",
    "                \"avg_latency\": sum(latencies) / len(latencies),\n",
    "                \"json_valid_rate\": json_valid_rate,\n",
    "                \"total_tests\": len(version_results)\n",
    "            }\n",
    "        \n",
    "        # 최고 성능 버전 선정\n",
    "        best_version = max(analysis.keys(), \n",
    "                         key=lambda x: analysis[x][\"avg_score\"] * analysis[x][\"json_valid_rate\"])\n",
    "        analysis[\"best_version\"] = best_version\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# 버전 관리 시스템 테스트\n",
    "def test_version_manager():\n",
    "    manager = PromptVersionManager()\n",
    "    \n",
    "    # 프롬프트 버전들 등록\n",
    "    manager.add_version(\n",
    "        \"sentiment\", \"1.0\", \n",
    "        \"Classify sentiment: {input}\",\n",
    "        {\"author\": \"initial\", \"notes\": \"Basic version\"}\n",
    "    )\n",
    "    \n",
    "    manager.add_version(\n",
    "        \"sentiment\", \"2.0\",\n",
    "        \"\"\"Classify sentiment with confidence scoring.\n",
    "Return JSON: {\"label\": \"positive|negative|neutral\", \"confidence\": 0.0-1.0, \"reason\": \"...\"}\n",
    "Text: {input}\"\"\",\n",
    "        {\"author\": \"improved\", \"notes\": \"Added JSON structure\"}\n",
    "    )\n",
    "    \n",
    "    # 테스트 케이스\n",
    "    test_cases = [\n",
    "        {\"input\": \"배송이 빠르고 좋아요!\", \"expected\": {\"label\": \"positive\"}},\n",
    "        {\"input\": \"포장이 엉망이고 실망스러워요\", \"expected\": {\"label\": \"negative\"}},\n",
    "        {\"input\": \"그냥 보통이에요\", \"expected\": {\"label\": \"neutral\"}}\n",
    "    ]\n",
    "    \n",
    "    # A/B 테스트 실행\n",
    "    print(\"=== 프롬프트 A/B 테스트 ===\")\n",
    "    analysis = manager.run_ab_test(\"sentiment\", [\"1.0\", \"2.0\"], test_cases)\n",
    "    \n",
    "    print(\"\\nA/B 테스트 결과:\")\n",
    "    for version, metrics in analysis.items():\n",
    "        if version != \"best_version\":\n",
    "            print(f\"{version}:\")\n",
    "            print(f\"  평균 점수: {metrics['avg_score']:.3f}\")\n",
    "            print(f\"  평균 지연: {metrics['avg_latency']:.3f}s\")\n",
    "            print(f\"  JSON 유효율: {metrics['json_valid_rate']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n최고 성능: {analysis['best_version']}\")\n",
    "    \n",
    "    return manager\n",
    "\n",
    "version_manager = test_version_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. APE (Automatic Prompt Engineering)\n",
    "\n",
    "**핵심 아이디어**: 후보 프롬프트 자동 생성→평가→선정(루프)\n",
    "\n",
    "**구성**:\n",
    "1. 후보 K개 생성\n",
    "2. Dev set 평가\n",
    "3. 베스트 선택\n",
    "4. 버전 태깅\n",
    "\n",
    "**평가 방식**:\n",
    "- 정답형: 정확도/EM/F1\n",
    "- 생성형: LM-as-Judge(0~5), 포맷 준수율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ollama APE 데모 ===\n",
      "Dev set 크기: 4\n",
      "\n",
      "후보 1: Classify the sentiment of a given product review as positive, negative, or neutral based on its emotional tone and content, outputting a JSON response with 'label' and 'reason' keys to specify the classification decision.\n",
      "\n",
      "후보 2: Classify the sentiment of the given product review as positive, negative, or neutral, providing a brief reason for your classification in JSON format: `{\"label\": \"positive/negative/neutral\", \"reason\": \"brief_reason_here\"}`.\n",
      "\n",
      "후보 3: Classify the sentiment of a given review as positive, negative, or neutral, returning a JSON object with keys 'label' and 'reason'.\n",
      "\n",
      "--- 후보 1 평가 ---\n",
      "  배송이 빠르고 만족합니다... -> positive (정답: positive)\n",
      "  불량품이 와서 화가 납니다... -> negative (정답: negative)\n",
      "  보통 수준입니다... -> positive (정답: neutral)\n",
      "  가격이 적당하고 품질도 괜찮아요... -> positive (정답: positive)\n",
      "\n",
      "후보 1 정확도: 0.750 (3/4)\n",
      "\n",
      "--- 후보 2 평가 ---\n",
      "  배송이 빠르고 만족합니다... -> positive (정답: positive)\n",
      "  불량품이 와서 화가 납니다... -> negative (정답: negative)\n",
      "  보통 수준입니다... -> positive (정답: neutral)\n",
      "  가격이 적당하고 품질도 괜찮아요... -> positive (정답: positive)\n",
      "\n",
      "후보 2 정확도: 0.750 (3/4)\n",
      "\n",
      "--- 후보 3 평가 ---\n",
      "  배송이 빠르고 만족합니다... -> positive (정답: positive)\n",
      "  불량품이 와서 화가 납니다... -> negative (정답: negative)\n",
      "  보통 수준입니다... -> neutral (정답: neutral)\n",
      "  가격이 적당하고 품질도 괜찮아요... -> positive (정답: positive)\n",
      "\n",
      "후보 3 정확도: 1.000 (4/4)\n",
      "\n",
      "=== 최고 성능 프롬프트 (정확도: 1.000) ===\n",
      "Classify the sentiment of a given review as positive, negative, or neutral, returning a JSON object with keys 'label' and 'reason'.\n"
     ]
    }
   ],
   "source": [
    "# 4-1. Ollama APE (간단한 버전)\n",
    "def ollama_ape_demo():\n",
    "    \"\"\"Ollama를 사용한 간단한 APE 데모\"\"\"\n",
    "    \n",
    "    # Dev set 정의\n",
    "    dev_set = [\n",
    "        {\"text\": \"배송이 빠르고 만족합니다\", \"label\": \"positive\"},\n",
    "        {\"text\": \"불량품이 와서 화가 납니다\", \"label\": \"negative\"},\n",
    "        {\"text\": \"보통 수준입니다\", \"label\": \"neutral\"},\n",
    "        {\"text\": \"가격이 적당하고 품질도 괜찮아요\", \"label\": \"positive\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Ollama APE 데모 ===\")\n",
    "    print(f\"Dev set 크기: {len(dev_set)}\")\n",
    "    \n",
    "    # 1) 후보 프롬프트 생성 (3개)\n",
    "    candidates = []\n",
    "    \n",
    "    for i in range(1, 4):\n",
    "        generation_prompt = f\"\"\"Generate a one-line prompt to classify sentiment (positive|negative|neutral) with JSON output.\n",
    "Requirements:\n",
    "- Must return JSON with keys: label, reason\n",
    "- Should be clear and specific\n",
    "- Version {i} - make it different from previous versions\n",
    "\n",
    "Return only the prompt text.\"\"\"\n",
    "        \n",
    "        candidate = run_ollama(\"llama3.1:8b\", generation_prompt)\n",
    "        candidates.append(candidate)\n",
    "        print(f\"\\n후보 {i}: {candidate}\")\n",
    "    \n",
    "    # 2) 각 후보를 dev set으로 평가\n",
    "    best_score = 0\n",
    "    best_candidate = None\n",
    "    \n",
    "    for i, candidate in enumerate(candidates, 1):\n",
    "        correct = 0\n",
    "        total = len(dev_set)\n",
    "        \n",
    "        print(f\"\\n--- 후보 {i} 평가 ---\")\n",
    "        \n",
    "        for example in dev_set:\n",
    "            test_prompt = f\"\"\"{candidate}\n",
    "            \n",
    "Review: --- {example['text']} ---\"\"\"\n",
    "            \n",
    "            result = run_ollama(\"llama3.1:8b\", test_prompt)\n",
    "            \n",
    "            # 라벨 추출 (간단한 파싱)\n",
    "            predicted_label = \"unknown\"\n",
    "            for label in [\"positive\", \"negative\", \"neutral\"]:\n",
    "                if label.lower() in result.lower():\n",
    "                    predicted_label = label\n",
    "                    break\n",
    "            \n",
    "            if predicted_label == example[\"label\"]:\n",
    "                correct += 1\n",
    "            \n",
    "            print(f\"  {example['text'][:30]}... -> {predicted_label} (정답: {example['label']})\")\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f\"\\n후보 {i} 정확도: {accuracy:.3f} ({correct}/{total})\")\n",
    "        \n",
    "        if accuracy > best_score:\n",
    "            best_score = accuracy\n",
    "            best_candidate = candidate\n",
    "    \n",
    "    print(f\"\\n=== 최고 성능 프롬프트 (정확도: {best_score:.3f}) ===\")\n",
    "    print(best_candidate)\n",
    "    \n",
    "    return {\n",
    "        \"candidates\": candidates,\n",
    "        \"best_prompt\": best_candidate,\n",
    "        \"best_score\": best_score,\n",
    "        \"dev_set\": dev_set\n",
    "    }\n",
    "\n",
    "ollama_ape_result = ollama_ape_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI 고급 APE 시스템 ===\n",
      "Dev set 크기: 8\n",
      "\n",
      "후보 1 (direct,):\n",
      "  Classify the sentiment of the following Korean text: {input}. Return the result in JSON format with ...\n",
      "\n",
      "후보 2 (detailed):\n",
      "  You are a sentiment classification model tasked with analyzing the sentiment of a given text input i...\n",
      "\n",
      "후보 3 (prompt):\n",
      "  You are a sentiment analysis expert tasked with classifying the sentiment of a given text in Korean....\n",
      "\n",
      "후보 4 (prompt):\n",
      "  ```json\n",
      "{\n",
      "  \"label\": \"{label}\",\n",
      "  \"confidence\": {confidence},\n",
      "  \"reason\": \"The sentiment classificat...\n",
      "\n",
      "후보 5 (prompt):\n",
      "  ```json\n",
      "{\n",
      "  \"input\": \"{input}\",\n",
      "  \"output\": {\n",
      "    \"label\": \"positive|negative|neutral\",\n",
      "    \"confide...\n",
      "\n",
      "=== 후보 평가 진행 중... ===\n",
      "후보 1 평가 중...\n",
      "후보 2 평가 중...\n",
      "후보 3 평가 중...\n",
      "후보 4 평가 중...\n",
      "후보 5 평가 중...\n",
      "\n",
      "=== APE 평가 결과 ===\n",
      "\n",
      "순위 1: 후보 1\n",
      "  종합점수: 0.940\n",
      "  정확도: 1.000\n",
      "  JSON유효율: 1.000\n",
      "  Judge점수: 0.800\n",
      "  평균신뢰도: 0.925\n",
      "\n",
      "순위 2: 후보 4\n",
      "  종합점수: 0.940\n",
      "  정확도: 1.000\n",
      "  JSON유효율: 1.000\n",
      "  Judge점수: 0.800\n",
      "  평균신뢰도: 0.890\n",
      "\n",
      "순위 3: 후보 3\n",
      "  종합점수: 0.920\n",
      "  정확도: 0.875\n",
      "  JSON유효율: 1.000\n",
      "  Judge점수: 0.900\n",
      "  평균신뢰도: 0.912\n",
      "\n",
      "순위 4: 후보 2\n",
      "  종합점수: 0.600\n",
      "  정확도: 0.375\n",
      "  JSON유효율: 1.000\n",
      "  Judge점수: 0.500\n",
      "  평균신뢰도: 0.850\n",
      "\n",
      "순위 5: 후보 5\n",
      "  종합점수: 0.545\n",
      "  정확도: 0.125\n",
      "  JSON유효율: 0.750\n",
      "  Judge점수: 0.900\n",
      "  평균신뢰도: 0.000\n",
      "\n",
      "=== 최고 성능 프롬프트 (종합점수: 0.940) ===\n",
      "Classify the sentiment of the following Korean text: {input}. Return the result in JSON format with the keys: {\"label\": \"positive|negative|neutral\", \"confidence\": 0.0-1.0, \"reason\": \"...\"}\n"
     ]
    }
   ],
   "source": [
    "# 4-2. OpenAI APE (고급 버전 - LM-as-Judge 포함)\n",
    "def openai_ape_advanced():\n",
    "    \"\"\"OpenAI를 사용한 고급 APE 시스템\"\"\"\n",
    "    \n",
    "    # 더 큰 Dev set\n",
    "    dev_set = [\n",
    "        {\"text\": \"배송이 빠르고 만족합니다\", \"label\": \"positive\"},\n",
    "        {\"text\": \"불량품이 와서 화가 납니다\", \"label\": \"negative\"}, \n",
    "        {\"text\": \"보통 수준입니다\", \"label\": \"neutral\"},\n",
    "        {\"text\": \"가격이 적당하고 품질도 괜찮아요\", \"label\": \"positive\"},\n",
    "        {\"text\": \"포장이 찢어져서 실망이에요\", \"label\": \"negative\"},\n",
    "        {\"text\": \"특별하지 않아요. 그냥 평범해요\", \"label\": \"neutral\"},\n",
    "        {\"text\": \"완벽합니다! 강력 추천해요!\", \"label\": \"positive\"},\n",
    "        {\"text\": \"돈 아까워요. 환불하고 싶네요\", \"label\": \"negative\"}\n",
    "    ]\n",
    "    \n",
    "    print(\"=== OpenAI 고급 APE 시스템 ===\")\n",
    "    print(f\"Dev set 크기: {len(dev_set)}\")\n",
    "    \n",
    "    # 1) 후보 프롬프트 생성 (5개, 다양한 접근법)\n",
    "    generation_strategies = [\n",
    "        \"Generate a direct, simple prompt for sentiment classification with JSON output.\",\n",
    "        \"Generate a detailed prompt with examples and constraints for sentiment classification.\",\n",
    "        \"Generate a prompt that uses role-playing (persona) for sentiment classification.\", \n",
    "        \"Generate a prompt with step-by-step reasoning for sentiment classification.\",\n",
    "        \"Generate a prompt with confidence scoring and edge case handling for sentiment classification.\"\n",
    "    ]\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for i, strategy in enumerate(generation_strategies, 1):\n",
    "        generation_prompt = f\"\"\"{strategy}\n",
    "        \n",
    "Requirements:\n",
    "- Must return valid JSON with keys: {{\"label\": \"positive|negative|neutral\", \"confidence\": 0.0-1.0, \"reason\": \"...\"}}\n",
    "- Should handle Korean text\n",
    "- Should be robust and production-ready\n",
    "- Include placeholder {{input}} for the text to classify\n",
    "\n",
    "Return only the prompt text.\"\"\"\n",
    "        \n",
    "        candidate = run_openai(generation_prompt, temperature=0.7, max_tokens=300)\n",
    "        candidates.append({\n",
    "            \"id\": i,\n",
    "            \"strategy\": strategy,\n",
    "            \"prompt\": candidate\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n후보 {i} ({strategy.split()[2]}):\")\n",
    "        print(f\"  {candidate[:100]}...\")\n",
    "    \n",
    "    # 2) 다중 메트릭 평가\n",
    "    def evaluate_candidate(candidate_data: dict) -> dict:\n",
    "        \"\"\"후보 프롬프트 종합 평가\"\"\"\n",
    "        prompt_template = candidate_data[\"prompt\"]\n",
    "        results = []\n",
    "        \n",
    "        for example in dev_set:\n",
    "            # 프롬프트 실행\n",
    "            full_prompt = prompt_template.replace(\"{input}\", example[\"text\"])\n",
    "            \n",
    "            try:\n",
    "                response = run_openai(full_prompt, temperature=0.1, max_tokens=200)\n",
    "                \n",
    "                # JSON 파싱\n",
    "                json_data = safe_json_parse(response)\n",
    "                \n",
    "                if json_data:\n",
    "                    predicted = json_data.get(\"label\", \"unknown\").lower()\n",
    "                    confidence = json_data.get(\"confidence\", 0.0)\n",
    "                    reason = json_data.get(\"reason\", \"\")\n",
    "                else:\n",
    "                    # Fallback: 텍스트에서 라벨 추출\n",
    "                    predicted = \"unknown\"\n",
    "                    confidence = 0.0\n",
    "                    reason = \"\"\n",
    "                    \n",
    "                    for label in [\"positive\", \"negative\", \"neutral\"]:\n",
    "                        if label in response.lower():\n",
    "                            predicted = label\n",
    "                            break\n",
    "                \n",
    "                is_correct = predicted == example[\"label\"]\n",
    "                \n",
    "                results.append({\n",
    "                    \"input\": example[\"text\"],\n",
    "                    \"expected\": example[\"label\"],\n",
    "                    \"predicted\": predicted,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"reason\": reason,\n",
    "                    \"correct\": is_correct,\n",
    "                    \"json_valid\": json_data is not None,\n",
    "                    \"response\": response\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    \"input\": example[\"text\"],\n",
    "                    \"expected\": example[\"label\"],\n",
    "                    \"error\": str(e),\n",
    "                    \"correct\": False,\n",
    "                    \"json_valid\": False\n",
    "                })\n",
    "        \n",
    "        # 메트릭 계산\n",
    "        total = len(results)\n",
    "        correct = sum(1 for r in results if r.get(\"correct\", False))\n",
    "        json_valid = sum(1 for r in results if r.get(\"json_valid\", False))\n",
    "        avg_confidence = sum(r.get(\"confidence\", 0) for r in results if \"confidence\" in r) / total\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": correct / total,\n",
    "            \"json_valid_rate\": json_valid / total,\n",
    "            \"avg_confidence\": avg_confidence,\n",
    "            \"results\": results\n",
    "        }\n",
    "    \n",
    "    # 3) 모든 후보 평가\n",
    "    evaluations = []\n",
    "    \n",
    "    print(\"\\n=== 후보 평가 진행 중... ===\")\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        print(f\"후보 {candidate['id']} 평가 중...\")\n",
    "        evaluation = evaluate_candidate(candidate)\n",
    "        \n",
    "        # LM-as-Judge 평가 추가\n",
    "        judge_prompt = f\"\"\"Rate the quality of this prompt for sentiment classification (1-10):\n",
    "        \n",
    "Prompt: \"{candidate['prompt']}\"\n",
    "\n",
    "Consider:\n",
    "- Clarity and specificity\n",
    "- Output format specification\n",
    "- Robustness for edge cases\n",
    "- Production readiness\n",
    "\n",
    "Return JSON: {{\"score\": 1-10, \"reasoning\": \"...\"}}\"\"\"\n",
    "        \n",
    "        judge_response = run_openai(judge_prompt, temperature=0.2, max_tokens=200)\n",
    "        judge_data = safe_json_parse(judge_response)\n",
    "        judge_score = judge_data.get(\"score\", 5) if judge_data else 5\n",
    "        \n",
    "        evaluation[\"judge_score\"] = judge_score / 10.0  # 0-1로 정규화\n",
    "        evaluation[\"judge_reasoning\"] = judge_data.get(\"reasoning\", \"\") if judge_data else \"\"\n",
    "        \n",
    "        evaluations.append({\n",
    "            \"candidate\": candidate,\n",
    "            \"evaluation\": evaluation\n",
    "        })\n",
    "    \n",
    "    # 4) 종합 점수 계산 및 최고 후보 선정\n",
    "    for eval_data in evaluations:\n",
    "        eval_metrics = eval_data[\"evaluation\"]\n",
    "        \n",
    "        # 가중 종합 점수 (정확도 40% + JSON유효율 30% + Judge점수 30%)\n",
    "        composite_score = (\n",
    "            eval_metrics[\"accuracy\"] * 0.4 +\n",
    "            eval_metrics[\"json_valid_rate\"] * 0.3 +\n",
    "            eval_metrics[\"judge_score\"] * 0.3\n",
    "        )\n",
    "        eval_metrics[\"composite_score\"] = composite_score\n",
    "    \n",
    "    # 결과 정렬 (종합 점수 기준)\n",
    "    evaluations.sort(key=lambda x: x[\"evaluation\"][\"composite_score\"], reverse=True)\n",
    "    \n",
    "    # 5) 결과 출력\n",
    "    print(\"\\n=== APE 평가 결과 ===\")\n",
    "    \n",
    "    for i, eval_data in enumerate(evaluations):\n",
    "        candidate = eval_data[\"candidate\"]\n",
    "        metrics = eval_data[\"evaluation\"]\n",
    "        \n",
    "        print(f\"\\n순위 {i+1}: 후보 {candidate['id']}\")\n",
    "        print(f\"  종합점수: {metrics['composite_score']:.3f}\")\n",
    "        print(f\"  정확도: {metrics['accuracy']:.3f}\")\n",
    "        print(f\"  JSON유효율: {metrics['json_valid_rate']:.3f}\")\n",
    "        print(f\"  Judge점수: {metrics['judge_score']:.3f}\")\n",
    "        print(f\"  평균신뢰도: {metrics['avg_confidence']:.3f}\")\n",
    "    \n",
    "    best_candidate = evaluations[0]\n",
    "    print(f\"\\n=== 최고 성능 프롬프트 (종합점수: {best_candidate['evaluation']['composite_score']:.3f}) ===\")\n",
    "    print(best_candidate[\"candidate\"][\"prompt\"])\n",
    "    \n",
    "    return {\n",
    "        \"candidates\": candidates,\n",
    "        \"evaluations\": evaluations,\n",
    "        \"best_candidate\": best_candidate,\n",
    "        \"dev_set\": dev_set\n",
    "    }\n",
    "\n",
    "ape_result = openai_ape_advanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ape_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m❌ 프롬프트가 기준을 충족하지 못합니다. 추가 개선이 필요합니다.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mvalidate_ape_winner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mvalidate_ape_winner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_ape_winner\u001b[39m():\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"APE 우승 프롬프트 검증\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mape_result\u001b[49m:\n\u001b[32m      6\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAPE 결과가 없습니다.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'ape_result' is not defined"
     ]
    }
   ],
   "source": [
    "# 4-3. APE 결과 검증 및 최종 테스트\n",
    "def validate_ape_winner():\n",
    "    \"\"\"APE 우승 프롬프트 검증\"\"\"\n",
    "    \n",
    "    if not ape_result:\n",
    "        print(\"APE 결과가 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    best_prompt = ape_result[\"best_candidate\"][\"candidate\"][\"prompt\"]\n",
    "    \n",
    "    # 새로운 테스트 케이스 (Dev set과 다름)\n",
    "    test_cases = [\n",
    "        \"이 제품 정말 최고예요! 친구들에게도 추천했어요.\",\n",
    "        \"배송비가 너무 비싸고 포장도 부실해요. 다시는 안 사겠어요.\",\n",
    "        \"그냥 평범한 제품이에요. 나쁘지도 좋지도 않네요.\",\n",
    "        \"생각보다 괜찮은데 가격이 좀 아쉬워요.\",\n",
    "        \"완전 대박! 이런 걸 찾고 있었어요!\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== APE 우승 프롬프트 최종 검증 ===\")\n",
    "    print(\"\\n우승 프롬프트:\")\n",
    "    print(best_prompt)\n",
    "    \n",
    "    print(\"\\n=== 새로운 테스트 케이스 결과 ===\")\n",
    "    \n",
    "    total_tests = len(test_cases)\n",
    "    json_valid_count = 0\n",
    "    total_confidence = 0\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        full_prompt = best_prompt.replace(\"{input}\", test_case)\n",
    "        \n",
    "        result = run_openai(full_prompt, temperature=0.1, max_tokens=200)\n",
    "        \n",
    "        print(f\"\\n테스트 {i}: {test_case}\")\n",
    "        print(f\"결과: {result}\")\n",
    "        \n",
    "        # JSON 유효성 및 구조 검증\n",
    "        json_data = safe_json_parse(result)\n",
    "        if json_data:\n",
    "            json_valid_count += 1\n",
    "            confidence = json_data.get(\"confidence\", 0)\n",
    "            total_confidence += confidence\n",
    "            \n",
    "            print(f\"  ✓ JSON 유효, 라벨: {json_data.get('label')}, 신뢰도: {confidence}\")\n",
    "        else:\n",
    "            print(f\"  ✗ JSON 파싱 실패\")\n",
    "    \n",
    "    # 최종 통계\n",
    "    json_success_rate = json_valid_count / total_tests\n",
    "    avg_confidence = total_confidence / max(json_valid_count, 1)\n",
    "    \n",
    "    print(f\"\\n=== 최종 검증 통계 ===\")\n",
    "    print(f\"JSON 성공률: {json_success_rate:.1%} ({json_valid_count}/{total_tests})\")\n",
    "    print(f\"평균 신뢰도: {avg_confidence:.3f}\")\n",
    "    \n",
    "    # 성능 기준 평가\n",
    "    if json_success_rate >= 0.8 and avg_confidence >= 0.6:\n",
    "        print(\"\\n🎉 프롬프트가 프로덕션 기준을 충족합니다!\")\n",
    "    elif json_success_rate >= 0.6:\n",
    "        print(\"\\n⚠️ 프롬프트가 기본 기준은 충족하지만 개선이 필요합니다.\")\n",
    "    else:\n",
    "        print(\"\\n❌ 프롬프트가 기준을 충족하지 못합니다. 추가 개선이 필요합니다.\")\n",
    "\n",
    "validate_ape_winner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종합 실습: 통합 시스템\n",
    "\n",
    "이 코드는 지금까지 다룬 모든 프롬프트 엔지니어링 기법들을 하나로 통합한 **고급 프롬프트 엔지니어링 시스템(AdvancedPromptSystem)**을 보여줍니다. 이 시스템은 Function Calling, Multiple Chains, Meta-Prompting 기능을 모두 포함하여, 복합적인 AI 태스크를 체계적으로 처리합니다.\n",
    "\n",
    "시스템 구성 요소\n",
    "AdvancedPromptSystem 클래스는 여러 모듈로 구성되어 있습니다.\n",
    "\n",
    "PromptVersionManager: 이전에 설명된 클래스로, 프롬프트 버전을 관리하고 A/B 테스트를 수행하여 최적의 프롬프트를 찾는 기능을 담당합니다.\n",
    "\n",
    "tool_registry: 시스템이 사용할 수 있는 모든 **도구(함수)**를 등록하고 관리하는 저장소입니다. 각 도구는 이름, 함수, 설명, 매개변수 스키마를 포함합니다.\n",
    "\n",
    "chain_definitions: 여러 단계의 LLM 호출을 묶어 **파이프라인(chain)**으로 정의하는 저장소입니다. 각 파이프라인은 정해진 순서대로 LLM 호출과 도구 실행을 처리합니다.\n",
    "\n",
    "execute_with_tools: 사용자의 요청을 받아, 등록된 도구 스키마를 OpenAI에 전달하고 Function Calling을 실행하는 핵심 메서드입니다.\n",
    "\n",
    "execute_chain: 정의된 파이프라인을 실행하는 메서드입니다. 각 단계의 출력값을 다음 단계의 입력값으로 전달하며, 도구 사용 여부에 따라 execute_with_tools를 호출합니다.\n",
    "\n",
    "auto_improve_prompt: 메타 프롬프팅을 사용하여 초기 프롬프트의 약점을 분석하고, 개선된 프롬프트를 자동으로 생성하는 메서드입니다.\n",
    "\n",
    "통합 시스템 데모(demo_integrated_system)\n",
    "이 함수는 위에서 정의된 AdvancedPromptSystem을 실제로 사용해봅니다.\n",
    "\n",
    "도구 등록:\n",
    "system.register_tool() 메서드를 사용하여 analyze_sentiment라는 '감성 분석' 도구를 시스템에 등록합니다. 이 도구는 실제로는 간단한 키워드 기반 분석을 수행하지만, 시스템 내에서는 하나의 '도구'로 기능합니다.\n",
    "\n",
    "처리 체인 정의:\n",
    "system.define_chain() 메서드를 사용하여 \"customer_service\"라는 이름의 파이프라인을 정의합니다. 이 파이프라인은 세 단계로 구성됩니다.\n",
    "\n",
    "preprocessing: 입력 텍스트를 정규화합니다.\n",
    "\n",
    "sentiment_analysis: 이 단계는 use_tools: True로 설정되어, LLM에게 '감성 분석' 도구(analyze_sentiment)를 사용하라고 지시합니다.\n",
    "\n",
    "response_generation: 앞선 단계들의 결과를 바탕으로 고객 응대 메시지를 생성합니다.\n",
    "\n",
    "체인 실행:\n",
    "system.execute_chain(\"customer_service\", test_input)를 호출하면, 시스템은 정의된 파이프라인을 순차적으로 실행합니다. 특히, 2단계에서 LLM은 analyze_sentiment 도구를 호출할 것이고, 시스템은 이 도구의 실행 결과(예: {\"sentiment\": \"negative\", \"confidence\": 0.8})를 받아 3단계로 전달합니다.\n",
    "\n",
    "프롬프트 자동 개선:\n",
    "system.auto_improve_prompt()를 호출하여 initial_prompt(\"답변해주세요.\")와 몇 가지 예시를 제공합니다. 시스템은 이 정보를 바탕으로 더 구체적이고 효과적인 프롬프트를 자동으로 생성합니다.\n",
    "\n",
    "이 시스템의 의미\n",
    "이 코드는 단순한 개별 기법 데모를 넘어, 각 기법이 어떻게 현실적인 애플리케이션의 일부가 될 수 있는지 보여줍니다.\n",
    "\n",
    "모듈화: 각 기능(도구, 체인, 버전 관리)이 별도의 모듈로 분리되어 있어, 시스템을 쉽게 확장하고 관리할 수 있습니다.\n",
    "\n",
    "자동화: 메타 프롬프팅을 통해 최적의 프롬프트를 자동으로 생성하고, 파이프라인을 통해 복잡한 작업을 자동화하여 처리합니다.\n",
    "\n",
    "오케스트레이션: execute_chain과 execute_execute_with_tools 메서드는 LLM 호출과 외부 도구 실행을 유기적으로 연결하는 오케스트레이터 역할을 수행합니다.\n",
    "\n",
    "이는 LLM을 활용한 시스템 개발이 단순히 run_openai(prompt)를 호출하는 것을 넘어, 체계적인 설계와 자동화된 관리가 필요한 복잡한 엔지니어링 영역임을 시사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 고급 프롬프트 엔지니어링 통합 시스템 데모 ===\n",
      "\n",
      "테스트 입력: 배송이 너무 늦어서 화가 나는데, 언제 받을 수 있나요?\n",
      "\n",
      "=== 체인 실행 결과 ===\n",
      "\n",
      "preprocessing (단계 1):\n",
      "  입력: 배송이 너무 늦어서 화가 나는데, 언제 받을 수 있나요?...\n",
      "  출력: 배송이 너무 늦어서 화가 나는데, 언제 받을 수 있나요? \n",
      "\n",
      "**Normalized Text:**\n",
      "배송이 늦어서 화가 나는데, 언제 받을 수 있나요?...\n",
      "\n",
      "sentiment_analysis (단계 2):\n",
      "  입력: 배송이 너무 늦어서 화가 나는데, 언제 받을 수 있나요? \n",
      "\n",
      "**Normalized Tex...\n",
      "  출력: The sentiment of the customer message is negative, with a confidence level of 80%. The customer expr...\n",
      "\n",
      "response_generation (단계 3):\n",
      "  입력: The sentiment of the customer message is negative,...\n",
      "  출력: Subject: Update on Your Order Delivery\n",
      "\n",
      "Dear [Customer's Name],\n",
      "\n",
      "Thank you for reaching out to us. I...\n",
      "\n",
      "최종 결과: Subject: Update on Your Order Delivery\n",
      "\n",
      "Dear [Customer's Name],\n",
      "\n",
      "Thank you for reaching out to us. I sincerely apologize for the frustration and inconvenience caused by the delay in your order delivery. I understand how important it is for you to receive your items on time.\n",
      "\n",
      "I am currently looking into the status of your order and will provide you with an update as soon as possible. In the meantime, I appreciate your patience and understanding during this process.\n",
      "\n",
      "If you have any further questions or concerns, please don’t hesitate to let me know. We’re here to help!\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]  \n",
      "[Your Position]  \n",
      "[Company Name]  \n",
      "[Contact Information]\n",
      "\n",
      "=== 프롬프트 자동 개선 ===\n",
      "초기 프롬프트: 답변해주세요.\n",
      "개선된 프롬프트: 질문에 대한 구체적이고 유용한 답변을 제공해주세요. 예를 들어, 날씨에 대한 질문에는 현재 날씨, 기온, 강수 확률 등을 포함하고, 추천 요청에는 추천 이유와 함께 구체적인 항목을 제시해주세요.\n",
      "개선 사항: ['Specify the context or subject matter for the questions to guide the responses.', 'Provide a more detailed format for the expected responses, such as length or style.', 'Clarify the type of information or recommendations that should be included in the responses.']\n"
     ]
    }
   ],
   "source": [
    "# 통합 시스템: 모든 기법을 결합한 고급 프롬프트 엔지니어링 시스템\n",
    "class AdvancedPromptSystem:\n",
    "    \"\"\"고급 프롬프트 엔지니어링 통합 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.version_manager = PromptVersionManager()\n",
    "        self.tool_registry = {}\n",
    "        self.chain_definitions = {}\n",
    "        self.ape_history = []\n",
    "    \n",
    "    def register_tool(self, name: str, func, description: str, parameters: dict):\n",
    "        \"\"\"도구 등록\"\"\"\n",
    "        self.tool_registry[name] = {\n",
    "            \"function\": func,\n",
    "            \"description\": description,\n",
    "            \"parameters\": parameters\n",
    "        }\n",
    "    \n",
    "    def define_chain(self, name: str, stages: list):\n",
    "        \"\"\"처리 체인 정의\"\"\"\n",
    "        self.chain_definitions[name] = stages\n",
    "    \n",
    "    def execute_with_tools(self, prompt: str, available_tools: list = None):\n",
    "        \"\"\"도구를 활용한 프롬프트 실행\"\"\"\n",
    "        if available_tools is None:\n",
    "            available_tools = list(self.tool_registry.keys())\n",
    "        \n",
    "        # OpenAI Function Calling 형식으로 도구 변환\n",
    "        tools_schema = []\n",
    "        for tool_name in available_tools:\n",
    "            if tool_name in self.tool_registry:\n",
    "                tool_info = self.tool_registry[tool_name]\n",
    "                tools_schema.append({\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tool_name,\n",
    "                        \"description\": tool_info[\"description\"],\n",
    "                        \"parameters\": tool_info[\"parameters\"]\n",
    "                    }\n",
    "                })\n",
    "        \n",
    "        # 실행\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            tools=tools_schema,\n",
    "            tool_choice=\"auto\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        # 도구 호출 처리\n",
    "        if response.choices[0].message.tool_calls:\n",
    "            messages.append(response.choices[0].message)\n",
    "            \n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                tool_name = tool_call.function.name\n",
    "                if tool_name in self.tool_registry:\n",
    "                    try:\n",
    "                        args = json.loads(tool_call.function.arguments)\n",
    "                        result = self.tool_registry[tool_name][\"function\"](**args)\n",
    "                        \n",
    "                        messages.append({\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"name\": tool_name,\n",
    "                            \"content\": json.dumps(result, ensure_ascii=False)\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        messages.append({\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"name\": tool_name,\n",
    "                            \"content\": f\"Error: {str(e)}\"\n",
    "                        })\n",
    "            \n",
    "            # 최종 응답\n",
    "            final_response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            return final_response.choices[0].message.content\n",
    "        else:\n",
    "            return response.choices[0].message.content\n",
    "    \n",
    "    def execute_chain(self, chain_name: str, input_data: str):\n",
    "        \"\"\"정의된 체인 실행\"\"\"\n",
    "        if chain_name not in self.chain_definitions:\n",
    "            return f\"Chain '{chain_name}' not found\"\n",
    "        \n",
    "        stages = self.chain_definitions[chain_name]\n",
    "        current_data = input_data\n",
    "        stage_results = []\n",
    "        \n",
    "        for i, stage in enumerate(stages):\n",
    "            stage_prompt = stage[\"prompt\"].format(input=current_data)\n",
    "            \n",
    "            if stage.get(\"use_tools\", False):\n",
    "                result = self.execute_with_tools(\n",
    "                    stage_prompt, \n",
    "                    stage.get(\"tools\", [])\n",
    "                )\n",
    "            else:\n",
    "                result = run_openai(\n",
    "                    stage_prompt, \n",
    "                    **stage.get(\"params\", {})\n",
    "                )\n",
    "            \n",
    "            stage_results.append({\n",
    "                \"stage\": i + 1,\n",
    "                \"name\": stage.get(\"name\", f\"Stage {i+1}\"),\n",
    "                \"input\": current_data,\n",
    "                \"output\": result\n",
    "            })\n",
    "            \n",
    "            current_data = result\n",
    "        \n",
    "        return {\n",
    "            \"final_result\": current_data,\n",
    "            \"stage_results\": stage_results\n",
    "        }\n",
    "    \n",
    "    def auto_improve_prompt(self, initial_prompt: str, task_examples: list):\n",
    "        \"\"\"메타 프롬프팅을 통한 자동 개선\"\"\"\n",
    "        # 분석\n",
    "        analysis_prompt = f\"\"\"Analyze this prompt and suggest improvements:\n",
    "        \n",
    "Prompt: \"{initial_prompt}\"\n",
    "Task examples: {json.dumps(task_examples, ensure_ascii=False)}\n",
    "\n",
    "Return JSON with:\n",
    "{{\n",
    "  \"weaknesses\": [...],\n",
    "  \"improvement_suggestions\": [...],\n",
    "  \"improved_prompt\": \"...\"\n",
    "}}\"\"\"\n",
    "        \n",
    "        analysis = run_openai(analysis_prompt, temperature=0.2, max_tokens=500)\n",
    "        analysis_data = safe_json_parse(analysis)\n",
    "        \n",
    "        if analysis_data:\n",
    "            return analysis_data\n",
    "        else:\n",
    "            return {\"error\": \"Failed to parse analysis\", \"original\": initial_prompt}\n",
    "\n",
    "# 통합 시스템 데모\n",
    "def demo_integrated_system():\n",
    "    \"\"\"통합 시스템 데모\"\"\"\n",
    "    \n",
    "    system = AdvancedPromptSystem()\n",
    "    \n",
    "    print(\"=== 고급 프롬프트 엔지니어링 통합 시스템 데모 ===\")\n",
    "    \n",
    "    # 1) 도구 등록\n",
    "    def analyze_sentiment(text: str) -> dict:\n",
    "        \"\"\"감성 분석 도구\"\"\"\n",
    "        # 간단한 키워드 기반 분석\n",
    "        positive_words = ['좋', '최고', '만족', '추천', '완벽']\n",
    "        negative_words = ['나쁘', '최악', '실망', '화', '문제']\n",
    "        \n",
    "        pos_count = sum(1 for word in positive_words if word in text)\n",
    "        neg_count = sum(1 for word in negative_words if word in text)\n",
    "        \n",
    "        if pos_count > neg_count:\n",
    "            return {\"sentiment\": \"positive\", \"confidence\": 0.8}\n",
    "        elif neg_count > pos_count:\n",
    "            return {\"sentiment\": \"negative\", \"confidence\": 0.8}\n",
    "        else:\n",
    "            return {\"sentiment\": \"neutral\", \"confidence\": 0.6}\n",
    "    \n",
    "    system.register_tool(\n",
    "        \"analyze_sentiment\",\n",
    "        analyze_sentiment,\n",
    "        \"텍스트의 감성을 분석합니다\",\n",
    "        {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"string\", \"description\": \"분석할 텍스트\"}\n",
    "            },\n",
    "            \"required\": [\"text\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 2) 처리 체인 정의\n",
    "    system.define_chain(\"customer_service\", [\n",
    "        {\n",
    "            \"name\": \"preprocessing\",\n",
    "            \"prompt\": \"Clean and normalize this text: {input}\",\n",
    "            \"params\": {\"temperature\": 0.1, \"max_tokens\": 100}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"sentiment_analysis\", \n",
    "            \"prompt\": \"Analyze the sentiment of this customer message: {input}\",\n",
    "            \"use_tools\": True,\n",
    "            \"tools\": [\"analyze_sentiment\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"response_generation\",\n",
    "            \"prompt\": \"Generate appropriate customer service response based on: {input}\",\n",
    "            \"params\": {\"temperature\": 0.3, \"max_tokens\": 200}\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # 3) 테스트 실행\n",
    "    test_input = \"배송이 너무 늦어서 화가 나는데, 언제 받을 수 있나요?\"\n",
    "    \n",
    "    print(f\"\\n테스트 입력: {test_input}\")\n",
    "    \n",
    "    # 체인 실행\n",
    "    chain_result = system.execute_chain(\"customer_service\", test_input)\n",
    "    \n",
    "    print(\"\\n=== 체인 실행 결과 ===\")\n",
    "    for stage in chain_result[\"stage_results\"]:\n",
    "        print(f\"\\n{stage['name']} (단계 {stage['stage']}):\")\n",
    "        print(f\"  입력: {stage['input'][:50]}...\")\n",
    "        print(f\"  출력: {stage['output'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n최종 결과: {chain_result['final_result']}\")\n",
    "    \n",
    "    # 4) 프롬프트 자동 개선\n",
    "    print(\"\\n=== 프롬프트 자동 개선 ===\")\n",
    "    initial_prompt = \"답변해주세요.\"\n",
    "    examples = [\n",
    "        {\"input\": \"날씨가 어때요?\", \"expected\": \"구체적인 정보 제공\"},\n",
    "        {\"input\": \"추천해주세요\", \"expected\": \"명확한 추천과 이유\"}\n",
    "    ]\n",
    "    \n",
    "    improvement = system.auto_improve_prompt(initial_prompt, examples)\n",
    "    \n",
    "    print(f\"초기 프롬프트: {initial_prompt}\")\n",
    "    if \"improved_prompt\" in improvement:\n",
    "        print(f\"개선된 프롬프트: {improvement['improved_prompt']}\")\n",
    "        print(f\"개선 사항: {improvement.get('improvement_suggestions', [])}\")\n",
    "    \n",
    "    return system\n",
    "\n",
    "integrated_system = demo_integrated_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리 및 베스트 프랙티스\n",
    "\n",
    "### 고급 통합 기법 요약\n",
    "\n",
    "| 기법 | 핵심 아이디어 | 주요 장점 | 적용 상황 |\n",
    "|------|---------------|-----------|----------|\n",
    "| **Function Calling** | 외부 도구/API 연동 | 실시간 데이터, 정확한 계산 | 최신 정보 필요, 복잡한 연산 |\n",
    "| **Multiple Chains** | 단계적 파이프라인 처리 | 복잡한 워크플로우 처리 | 다단계 분석, 의사결정 |\n",
    "| **Meta-Prompting** | 프롬프트 자체 개선 | 지속적 품질 향상 | 프롬프트 최적화, 유지보수 |\n",
    "| **APE** | 자동 프롬프트 생성/평가 | 객관적 성능 비교 | 대규모 최적화, A/B 테스트 |\n",
    "\n",
    "### 실무 적용 가이드\n",
    "\n",
    "#### 1. 시스템 설계 원칙\n",
    "- **모듈화**: 각 기법을 독립적 모듈로 구현\n",
    "- **확장성**: 새로운 도구/체인 쉽게 추가 가능\n",
    "- **관찰성**: 각 단계별 로깅 및 메트릭 수집\n",
    "- **안전성**: 도구 호출 시 보안 검증 및 예외 처리\n",
    "\n",
    "#### 2. 성능 최적화\n",
    "- **캐싱**: 반복 호출 결과 캐시\n",
    "- **병렬 처리**: Fan-out 패턴으로 독립적 작업 병렬화\n",
    "- **배치 처리**: 유사한 요청들 배치로 처리\n",
    "- **지연 로딩**: 필요한 시점에만 리소스 로드\n",
    "\n",
    "#### 3. 품질 보증\n",
    "- **자동 테스트**: Dev/Test 셋으로 지속적 검증\n",
    "- **A/B 테스트**: 새로운 프롬프트 버전 비교\n",
    "- **사용자 피드백**: 실제 사용 결과 수집 및 반영\n",
    "- **버전 관리**: 프롬프트 변경사항 추적\n",
    "\n",
    "#### 4. 운영 고려사항\n",
    "- **비용 모니터링**: 토큰 사용량 및 API 호출 비용 추적\n",
    "- **지연 시간**: 응답 시간 SLA 설정 및 모니터링\n",
    "- **오류 처리**: 실패 시 fallback 전략\n",
    "- **보안**: 민감 정보 필터링 및 접근 제어"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
