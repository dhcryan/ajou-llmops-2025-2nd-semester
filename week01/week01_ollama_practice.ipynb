{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ccd45e",
   "metadata": {},
   "source": [
    "\n",
    "# Week01 — Ollama 기반 Prompt Engineering 실습\n",
    "**목표:** 로컬 LLM(ollama)을 이용해 3가지 대표 시나리오(요약, Q&A, 스타일 변환)를 실행하고 결과를 기록합니다.\n",
    "\n",
    "- 환경: Python 3.11.13 (venv 권장)\n",
    "- 엔진: Ollama (기본 포트 `http://localhost:11434`)\n",
    "- 모델 예시: `llama3.1:8b` (또는 `mistral:7b`, `qwen2.5:7b` 등)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52872c00",
   "metadata": {},
   "source": [
    "## 0) 준비물 체크 (버전 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5e84fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "Platform: macOS-15.6-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b67703",
   "metadata": {},
   "source": [
    "## 1) Ollama 연결 상태 점검"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e02a165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Ollama reachable at http://localhost:11434 | status: 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "try:\n",
    "    r = requests.get(OLLAMA_HOST)\n",
    "    print(\"[OK] Ollama reachable at\", OLLAMA_HOST, \"| status:\", r.status_code)\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Ollama not reachable at\", OLLAMA_HOST)\n",
    "    print(\" - Start Ollama app or service, e.g., 'ollama serve'\")\n",
    "    print(\" - Then pull a model: 'ollama pull llama3.1:8b'\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71758c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) 공용 유틸 함수 — Chat API 호출\n",
    "- `chat_ollama(model, messages, stream=False)`: /api/chat 호출\n",
    "- messages 포맷: `[{\"role\":\"system\",\"content\":...}, {\"role\":\"user\",\"content\":...}]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a4cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, time\n",
    "from typing import List, Dict\n",
    "\n",
    "def chat_ollama(model: str, messages: List[Dict], stream: bool = False, host: str = OLLAMA_HOST):\n",
    "    url = f\"{host}/api/chat\"\n",
    "    headers = {\"Content-Type\":\"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    t0 = time.time()\n",
    "    resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)\n",
    "    dt = time.time() - t0\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    # Ollama returns {\"message\": {\"content\": ...}, \"done\": true, ...}\n",
    "    text = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "    return text, dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbb505",
   "metadata": {},
   "source": [
    "\n",
    "## 3) 실험 로거 준비 (선택) — pandas CSV 기록\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07412fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Logging to /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/week01/results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_PATH = Path(\"results.csv\")\n",
    "cols = [\"scenario\",\"temperature\",\"top_p\",\"output_len\",\"latency_sec\",\"note\"]\n",
    "if not LOG_PATH.exists():\n",
    "    pd.DataFrame(columns=cols).to_csv(LOG_PATH, index=False)\n",
    "print(f\"[INFO] Logging to {LOG_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f1df0",
   "metadata": {},
   "source": [
    "\n",
    "## 4) 사례 1 — 텍스트 요약 (Summarization)\n",
    "- 긴 문단을 2~3문장으로 요약\n",
    "- system role로 역할/톤 지정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf6efe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are revolutionizing the AI industry with their ability to understand and interact with humans in natural language. These models can perform various tasks such as summarization, translation, question-answering, and content generation. However, they need precise input and careful monitoring to be effective and efficient. \n",
      "\n",
      "[latency_sec] 31.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/4z1x4d2x08v3pks_cvns5kzr0000gn/T/ipykernel_55350/2925449712.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL = \"llama3.1:8b\"  # 바꿔도 됨: \"mistral:7b\", \"qwen2.5:7b\" 등\n",
    "\n",
    "text_to_summarize = (\n",
    "    \"Large Language Models are transforming the AI landscape by enabling natural language interfaces \"\n",
    "    \"for a wide range of tasks. They can summarize, translate, answer questions, and generate content. \"\n",
    "    \"However, they require careful prompting, evaluation, and monitoring to be reliable and cost-effective.\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are a concise and faithful summarizer. Always write 2–3 sentences.\"},\n",
    "    {\"role\":\"user\",\"content\":f\"Summarize the following text in 2–3 sentences:\\n\\n{text_to_summarize}\"}\n",
    "]\n",
    "\n",
    "summary, dt = chat_ollama(MODEL, messages)\n",
    "print(summary, \"\\n\\n[latency_sec]\", round(dt, 2))\n",
    "\n",
    "# log\n",
    "row = {\n",
    "    \"scenario\":\"summarization\",\n",
    "    \"temperature\":None,\n",
    "    \"top_p\":None,\n",
    "    \"output_len\":len(summary.split()),\n",
    "    \"latency_sec\":dt,\n",
    "    \"note\":\"2-3 sentence summary\"\n",
    "}\n",
    "pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab5688",
   "metadata": {},
   "source": [
    "\n",
    "## 5) 사례 2 — 질의응답 (Q&A)\n",
    "- 간단한 지식 질문\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fa473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are a knowledgeable assistant for AI courses.\"},\n",
    "    {\"role\":\"user\",\"content\":\"What are the 5 stages of the LLM LifeCycle? Answer in one short paragraph.\"}\n",
    "]\n",
    "answer, dt = chat_ollama(MODEL, messages)\n",
    "print(answer, \"\\n\\n[latency_sec]\", round(dt, 2))\n",
    "\n",
    "row = {\n",
    "    \"scenario\":\"qa\",\n",
    "    \"temperature\":None,\n",
    "    \"top_p\":None,\n",
    "    \"output_len\":len(answer.split()),\n",
    "    \"latency_sec\":dt,\n",
    "    \"note\":\"LLM LifeCycle question\"\n",
    "}\n",
    "pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd0477",
   "metadata": {},
   "source": [
    "\n",
    "## 6) 사례 3 — 스타일 변환 (Style Transfer)\n",
    "- 격식 있는 문장을 친근한 톤으로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668207a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_text = \"Dear Professor, I would like to request your feedback on my draft report.\"\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You rewrite text in a friendly, casual, and polite style.\"},\n",
    "    {\"role\":\"user\",\"content\":f\"Rewrite this in a casual style:\\n\\n{source_text}\"}\n",
    "]\n",
    "styled, dt = chat_ollama(MODEL, messages)\n",
    "print(styled, \"\\n\\n[latency_sec]\", round(dt, 2))\n",
    "\n",
    "row = {\n",
    "    \"scenario\":\"style_transfer\",\n",
    "    \"temperature\":None,\n",
    "    \"top_p\":None,\n",
    "    \"output_len\":len(styled.split()),\n",
    "    \"latency_sec\":dt,\n",
    "    \"note\":\"casual rewrite\"\n",
    "}\n",
    "pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb843d6",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (선택) 파라미터 실험 템플릿\n",
    "- 일부 로컬 모델은 /api/generate 경로에서 `temperature`, `top_p` 등을 지원합니다.\n",
    "- 아래는 예시 형태(모델별 파라미터 가용성 상이). 지원되지 않으면 기본값 사용.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211562cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_raw(model: str, prompt: str, host: str = OLLAMA_HOST, temperature: float = None, top_p: float = None):\n",
    "    url = f\"{host}/api/generate\"\n",
    "    payload = {\"model\": model, \"prompt\": prompt}\n",
    "    if temperature is not None:\n",
    "        payload[\"options\"] = payload.get(\"options\", {})\n",
    "        payload[\"options\"][\"temperature\"] = float(temperature)\n",
    "    if top_p is not None:\n",
    "        payload[\"options\"] = payload.get(\"options\", {})\n",
    "        payload[\"options\"][\"top_p\"] = float(top_p)\n",
    "\n",
    "    t0 = time.time()\n",
    "    with requests.post(url, json=payload, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        out = []\n",
    "        for line in r.iter_lines():\n",
    "            if not line:\n",
    "                continue\n",
    "            data = json.loads(line.decode(\"utf-8\"))\n",
    "            if \"response\" in data:\n",
    "                out.append(data[\"response\"])\n",
    "            if data.get(\"done\"):\n",
    "                break\n",
    "    dt = time.time() - t0\n",
    "    return \"\".join(out), dt\n",
    "\n",
    "prompt = \"Summarize in one sentence: Large Language Models are changing how we build AI systems.\"\n",
    "text, dt = generate_raw(MODEL, prompt, temperature=0.7, top_p=0.9)\n",
    "print(text, \"\\n\\n[latency_sec]\", round(dt, 2))\n",
    "\n",
    "row = {\n",
    "    \"scenario\":\"summarization_raw\",\n",
    "    \"temperature\":0.7,\n",
    "    \"top_p\":0.9,\n",
    "    \"output_len\":len(text.split()),\n",
    "    \"latency_sec\":dt,\n",
    "    \"note\":\"/api/generate path with sampling\"\n",
    "}\n",
    "pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59a502",
   "metadata": {},
   "source": [
    "\n",
    "## 8) 로그 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa91838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(LOG_PATH)\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajou-llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
