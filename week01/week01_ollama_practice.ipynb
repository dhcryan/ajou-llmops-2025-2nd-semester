{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ccd45e",
   "metadata": {},
   "source": [
    "\n",
    "# Week01 — Ollama 기반 Prompt Engineering 실습\n",
    "**목표:** 로컬 LLM(ollama)을 이용해 3가지 대표 시나리오(요약, Q&A, 스타일 변환)를 실행하고 결과를 기록합니다.\n",
    "\n",
    "- 환경: Python 3.11.13 (venv 권장)\n",
    "- 엔진: Ollama (기본 포트 `http://localhost:11434`)\n",
    "- 모델 예시: `llama3.1:8b` (또는 `mistral:7b`, `qwen2.5:7b` 등)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52872c00",
   "metadata": {},
   "source": [
    "## 0) 준비물 체크 (버전 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d5e84fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "Platform: macOS-15.6-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, platform\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b67703",
   "metadata": {},
   "source": [
    "## 1) Ollama 연결 상태 점검"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e02a165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Ollama reachable at http://localhost:11434 | status: 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "try:\n",
    "    r = requests.get(OLLAMA_HOST)\n",
    "    print(\"[OK] Ollama reachable at\", OLLAMA_HOST, \"| status:\", r.status_code)\n",
    "except Exception as e:\n",
    "    print(\"[ERROR] Ollama not reachable at\", OLLAMA_HOST)\n",
    "    print(\" - Start Ollama app or service, e.g., 'ollama serve'\")\n",
    "    print(\" - Then pull a model: 'ollama pull llama3.1:8b'\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71758c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) 공용 유틸 함수 — Chat API 호출\n",
    "- `chat_ollama(model, messages, stream=False)`: /api/chat 호출\n",
    "- messages 포맷: `[{\"role\":\"system\",\"content\":...}, {\"role\":\"user\",\"content\":...}]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9a4cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, time\n",
    "from typing import List, Dict\n",
    "\n",
    "def chat_ollama(model: str, messages: List[Dict], stream: bool = False, host: str = OLLAMA_HOST):\n",
    "    url = f\"{host}/api/chat\"\n",
    "    headers = {\"Content-Type\":\"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    t0 = time.time()\n",
    "    resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)\n",
    "    dt = time.time() - t0\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    # Ollama returns {\"message\": {\"content\": ...}, \"done\": true, ...}\n",
    "    text = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "    return text, dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbb505",
   "metadata": {},
   "source": [
    "\n",
    "## 3) 실험 로거 준비 (선택) — pandas CSV 기록\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a07412fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Logging to /Users/sjcha/Documents/3. 아주대AI대학원/2025-2nd-semester/ajou-llmops-2025-2nd-semester/week01/results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "LOG_PATH = Path(\"results.csv\")\n",
    "cols = [\"scenario\",\"temperature\",\"top_p\",\"output_len\",\"latency_sec\",\"note\"]\n",
    "if not LOG_PATH.exists():\n",
    "    pd.DataFrame(columns=cols).to_csv(LOG_PATH, index=False)\n",
    "print(f\"[INFO] Logging to {LOG_PATH.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f1df0",
   "metadata": {},
   "source": [
    "\n",
    "## 4) 사례 1 — 텍스트 요약 (Summarization)\n",
    "- 긴 문단을 2~3문장으로 요약\n",
    "- system role로 역할/톤 지정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6efe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models are revolutionizing the field of artificial intelligence by providing a versatile tool for various natural language processing tasks, but their effectiveness depends on proper usage and management. \n",
      "\n",
      "[latency_sec] 4.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/4z1x4d2x08v3pks_cvns5kzr0000gn/T/ipykernel_55350/3893016846.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL = \"llama3.1:8b\"  # 바꿔도 됨: \"mistral:7b\", \"qwen2.5:7b\" 등\n",
    "\n",
    "text_to_summarize = (\n",
    "    \"Large Language Models are transforming the AI landscape by enabling natural language interfaces \"\n",
    "    \"for a wide range of tasks. They can summarize, translate, answer questions, and generate content. \"\n",
    "    \"However, they require careful prompting, evaluation, and monitoring to be reliable and cost-effective.\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are a concise and faithful summarizer. Always write 2–3 sentences.\"},\n",
    "    {\"role\":\"user\",\"content\":f\"Summarize the following text in 2–3 sentences:\\n\\n{text_to_summarize}\"}\n",
    "]\n",
    "\n",
    "summary, dt = chat_ollama(MODEL, messages)\n",
    "print(summary, \"\\n\\n[latency_sec]\", round(dt, 2))\n",
    "\n",
    "# log\n",
    "row = {\n",
    "    \"scenario\":\"summarization\",\n",
    "    \"temperature\":None,\n",
    "    \"top_p\":None,\n",
    "    \"output_len\":len(summary.split()),\n",
    "    \"latency_sec\":dt,\n",
    "    \"note\":\"2-3 sentence summary\"\n",
    "}\n",
    "pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab5688",
   "metadata": {},
   "source": [
    "\n",
    "## 5) 사례 2 — 질의응답 (Q&A)\n",
    "- 간단한 지식 질문\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b4fa473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 stages of the Large Language Model (LLM) Lifecycle are:\n",
      "\n",
      "1. **Training**: The initial stage where the model is trained on a large dataset to learn patterns and relationships.\n",
      "2. **Evaluation**: The model's performance is assessed using various metrics, such as accuracy and fluency.\n",
      "3. **Deployment**: The trained model is deployed in a production environment, where it can be fine-tuned for specific tasks or applications.\n",
      "4. **Maintenance**: The model is continuously monitored and updated to ensure its performance remains optimal over time.\n",
      "5. **Retirement**: Eventually, the model may become outdated or inefficient, at which point it is replaced with a newer, more advanced version. \n",
      "\n",
      "[latency_sec] 12.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/4z1x4d2x08v3pks_cvns5kzr0000gn/T/ipykernel_55350/330923159.py:16: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You are a knowledgeable assistant for AI courses.\"},\n",
    "    {\"role\":\"user\",\"content\":\"What are the 5 stages of the LLM LifeCycle? Answer in one short paragraph.\"}\n",
    "]\n",
    "answer, dt = chat_ollama(MODEL, messages)\n",
    "print(answer, \"\\n\\n[latency_sec]\", round(dt, 2))\n",
    "\n",
    "row = {\n",
    "    \"scenario\":\"qa\",\n",
    "    \"temperature\":None,\n",
    "    \"top_p\":None,\n",
    "    \"output_len\":len(answer.split()),\n",
    "    \"latency_sec\":dt,\n",
    "    \"note\":\"LLM LifeCycle question\"\n",
    "}\n",
    "pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fd0477",
   "metadata": {},
   "source": [
    "\n",
    "## 6) 사례 3 — 스타일 변환 (Style Transfer)\n",
    "- 격식 있는 문장을 친근한 톤으로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668207a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the rewritten text in a friendly, casual, and polite style:\n",
      "\n",
      "\"Hi Professor, hope you're doing well! I was wondering if you could take a look at my draft report and let me know what you think? Your feedback would be super helpful to me!\" \n",
      "\n",
      "[latency_sec] 7.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/4z1x4d2x08v3pks_cvns5kzr0000gn/T/ipykernel_55350/3820485004.py:17: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_text = \"Dear Professor, I would like to request your feedback on my draft report.\"\n",
    "messages = [\n",
    "    {\"role\":\"system\",\"content\":\"You rewrite text in a friendly, casual, and polite style.\"},\n",
    "    {\"role\":\"user\",\"content\":f\"Rewrite this in a casual style:\\n\\n{source_text}\"}\n",
    "]\n",
    "styled, dt = chat_ollama(MODEL, messages)\n",
    "print(styled, \"\\n\\n[latency_sec]\", round(dt, 2))\n",
    "\n",
    "row = {\n",
    "    \"scenario\":\"style_transfer\",\n",
    "    \"temperature\":None,\n",
    "    \"top_p\":None,\n",
    "    \"output_len\":len(styled.split()),\n",
    "    \"latency_sec\":dt,\n",
    "    \"note\":\"casual rewrite\"\n",
    "}\n",
    "pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb843d6",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (선택) 파라미터 실험 템플릿\n",
    "- 일부 로컬 모델은 /api/generate 경로에서 `temperature`, `top_p` 등을 지원합니다.\n",
    "- 아래는 예시 형태(모델별 파라미터 가용성 상이). 지원되지 않으면 기본값 사용.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "211562cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are revolutionizing the field of artificial intelligence by enabling more efficient, effective, and flexible approaches to building intelligent systems. \n",
      "\n",
      "[latency_sec] 4.87\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_raw(model: str, prompt: str, host: str = OLLAMA_HOST, temperature: float = None, top_p: float = None):\n",
    "    url = f\"{host}/api/generate\"\n",
    "    payload = {\"model\": model, \"prompt\": prompt}\n",
    "    if temperature is not None:\n",
    "        payload[\"options\"] = payload.get(\"options\", {})\n",
    "        payload[\"options\"][\"temperature\"] = float(temperature)\n",
    "    if top_p is not None:\n",
    "        payload[\"options\"] = payload.get(\"options\", {})\n",
    "        payload[\"options\"][\"top_p\"] = float(top_p)\n",
    "\n",
    "    t0 = time.time()\n",
    "    with requests.post(url, json=payload, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        out = []\n",
    "        for line in r.iter_lines():\n",
    "            if not line:\n",
    "                continue\n",
    "            data = json.loads(line.decode(\"utf-8\"))\n",
    "            if \"response\" in data:\n",
    "                out.append(data[\"response\"])\n",
    "            if data.get(\"done\"):\n",
    "                break\n",
    "    dt = time.time() - t0\n",
    "    return \"\".join(out), dt\n",
    "\n",
    "prompt = \"Summarize in one sentence: Large Language Models are changing how we build AI systems.\"\n",
    "text, dt = generate_raw(MODEL, prompt, temperature=0.7, top_p=0.9)\n",
    "print(text, \"\\n\\n[latency_sec]\", round(dt, 2))\n",
    "\n",
    "row = {\n",
    "    \"scenario\":\"summarization_raw\",\n",
    "    \"temperature\":0.7,\n",
    "    \"top_p\":0.9,\n",
    "    \"output_len\":len(text.split()),\n",
    "    \"latency_sec\":dt,\n",
    "    \"note\":\"/api/generate path with sampling\"\n",
    "}\n",
    "pd.concat([pd.read_csv(LOG_PATH), pd.DataFrame([row])]).to_csv(LOG_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59a502",
   "metadata": {},
   "source": [
    "\n",
    "## 8) 로그 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1aa91838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>output_len</th>\n",
       "      <th>latency_sec</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>summarization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>31.303983</td>\n",
       "      <td>2-3 sentence summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92</td>\n",
       "      <td>7.845136</td>\n",
       "      <td>LLM LifeCycle question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>style_transfer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>3.981948</td>\n",
       "      <td>casual rewrite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>summarization_raw</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>22</td>\n",
       "      <td>2.206811</td>\n",
       "      <td>/api/generate path with sampling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summarization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>13.276957</td>\n",
       "      <td>2-3 sentence summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>summarization</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "      <td>4.322007</td>\n",
       "      <td>2-3 sentence summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105</td>\n",
       "      <td>12.721062</td>\n",
       "      <td>LLM LifeCycle question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>style_transfer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45</td>\n",
       "      <td>7.215989</td>\n",
       "      <td>casual rewrite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>summarization_raw</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>23</td>\n",
       "      <td>4.869223</td>\n",
       "      <td>/api/generate path with sampling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            scenario  temperature  top_p  output_len  latency_sec  \\\n",
       "0      summarization          NaN    NaN          48    31.303983   \n",
       "1                 qa          NaN    NaN          92     7.845136   \n",
       "2     style_transfer          NaN    NaN          27     3.981948   \n",
       "3  summarization_raw          0.7    0.9          22     2.206811   \n",
       "4      summarization          NaN    NaN          49    13.276957   \n",
       "5      summarization          NaN    NaN          30     4.322007   \n",
       "6                 qa          NaN    NaN         105    12.721062   \n",
       "7     style_transfer          NaN    NaN          45     7.215989   \n",
       "8  summarization_raw          0.7    0.9          23     4.869223   \n",
       "\n",
       "                               note  \n",
       "0              2-3 sentence summary  \n",
       "1            LLM LifeCycle question  \n",
       "2                    casual rewrite  \n",
       "3  /api/generate path with sampling  \n",
       "4              2-3 sentence summary  \n",
       "5              2-3 sentence summary  \n",
       "6            LLM LifeCycle question  \n",
       "7                    casual rewrite  \n",
       "8  /api/generate path with sampling  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(LOG_PATH)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bca014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ajou-llmops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
