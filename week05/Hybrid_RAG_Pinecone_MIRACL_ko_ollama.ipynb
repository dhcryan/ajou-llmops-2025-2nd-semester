{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ê³ ê¸‰ RAG ì‹¤ìŠµ (v2): .env + Pinecone + Ollama\n",
        "- ë‚ ì§œ: 2025-09-29\n",
        "- ì´ ë…¸íŠ¸ë¶ì€ **.env**ì—ì„œ í™˜ê²½ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì™€ **Pinecone**ê³¼ **Ollama**ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
        "- êµ¬ì„±: **MIRACL-ko ê³µê°œ ë°ì´í„°** â†’ Dense(BGE-M3) + Sparse(BM25) â†’ RRF/Weighted â†’ **BGE Reranker** â†’ **Ollama ìƒì„±**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) .env í…œí”Œë¦¿ (ë¡œì»¬ì— íŒŒì¼ ìƒì„±)\n",
        "ì•„ë˜ ë‚´ìš©ì„ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ `.env` íŒŒì¼ì— ì €ì¥í•˜ì„¸ìš”.\n",
        "\n",
        "```\n",
        "PINECONE_API_KEY=pc_********************************\n",
        "PC_CLOUD=aws\n",
        "PC_REGION=us-east-1\n",
        "PC_INDEX_NAME=hybrid-miracl-ko\n",
        "\n",
        "# Ollama ê¸°ë³¸ í˜¸ìŠ¤íŠ¸ (ë¡œì»¬ ì‹¤í–‰ ì‹œ)\n",
        "OLLAMA_HOST=http://localhost:11434\n",
        "# ì„ í˜¸ ëª¨ë¸ ì˜ˆ: llama3.1:8b-instruct, qwen2.5:7b-instruct ë“±\n",
        "OLLAMA_MODEL=llama3.1:8b-instruct\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # !pip -q install -U pip\n",
        "!pip -q install -U python-dotenv requests pinecone pinecone-text FlagEmbedding datasets rank_bm25 matplotlib tqdm\n",
        "# ì„ íƒ: torch ìµœì‹  (í™˜ê²½ì— ë”°ë¼ ìƒëµ ê°€ëŠ¥)\n",
        "# !pip -q install -U torch --index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9f77f239",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: pinecone 7.3.0\n",
            "Uninstalling pinecone-7.3.0:\n",
            "  Successfully uninstalled pinecone-7.3.0\n",
            "\u001b[33mWARNING: Skipping pinecone-client as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping pinecone-plugin-inference as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pinecone\n",
            "  Using cached pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (2025.10.5)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (1.8.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: packaging<25.0,>=24.2 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.7)\n",
            "Requirement already satisfied: six>=1.5 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Using cached pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "Installing collected packages: pinecone\n",
            "Successfully installed pinecone-7.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y pinecone pinecone-client pinecone-plugin-inference\n",
        "!pip install -U pinecone\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env) & êµ¬ì„±\n",
        "- `.env`ë¥¼ ì½ì–´ **PINECONE_API_KEY**, **OLLAMA_HOST**, ê¸°íƒ€ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "- Î± í”„ë¦¬ì…‹ / NÂ·k í…œí”Œë¦¿ë„ í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ì§€ì •í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone: hybrid-miracl-ko aws us-east-1 dotproduct\n",
            "Ollama: http://localhost:11434 llama3.1:8b-instruct\n",
            "Scale: 8000 30  | Î±= 0.5  N= 100  K= 5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Pinecone\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
        "PC_CLOUD  = os.getenv(\"PC_CLOUD\", \"aws\")\n",
        "PC_REGION = os.getenv(\"PC_REGION\", \"us-east-1\")\n",
        "INDEX_NAME = os.getenv(\"PC_INDEX_NAME\", \"hybrid-miracl-ko\")\n",
        "METRIC = os.getenv(\"PC_METRIC\", \"dotproduct\")\n",
        "\n",
        "# Ollama\n",
        "OLLAMA_HOST  = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
        "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.1:8b-instruct\")\n",
        "\n",
        "# ì‹¤ìŠµ ìŠ¤ì¼€ì¼\n",
        "CORPUS_MAX = int(os.getenv(\"CORPUS_MAX\", \"8000\"))\n",
        "N_QUERIES  = int(os.getenv(\"N_QUERIES\", \"30\"))\n",
        "\n",
        "# Î± í”„ë¦¬ì…‹\n",
        "ALPHA_PRESETS = {\"balanced\":0.5, \"semantic_heavy\":0.7, \"keyword_heavy\":0.3}\n",
        "ALPHA = float(os.getenv(\"ALPHA\", ALPHA_PRESETS[\"balanced\"]))\n",
        "\n",
        "# N/k í…œí”Œë¦¿\n",
        "TEMPLATES = {\n",
        "    \"speed\":   {\"N\": 50,  \"k\": 3},\n",
        "    \"balanced\":{\"N\": 100, \"k\": 5},\n",
        "    \"quality\": {\"N\": 200, \"k\": 10}\n",
        "}\n",
        "N = int(os.getenv(\"RETRIEVER_N\", TEMPLATES[\"balanced\"][\"N\"]))\n",
        "K = int(os.getenv(\"LLM_K\",       TEMPLATES[\"balanced\"][\"k\"]))\n",
        "\n",
        "print(\"Pinecone:\", INDEX_NAME, PC_CLOUD, PC_REGION, METRIC)\n",
        "print(\"Ollama:\", OLLAMA_HOST, OLLAMA_MODEL)\n",
        "print(\"Scale:\", CORPUS_MAX, N_QUERIES, \" | Î±=\", ALPHA, \" N=\", N, \" K=\", K)\n",
        "assert PINECONE_API_KEY, \"PINECONE_API_KEY is required (set in .env)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Ollama ì±„íŒ… í•¨ìˆ˜ (ìš”ì²­í•˜ì‹  íŒ¨í„´ ë°˜ì˜)\n",
        "- ì•„ë˜ í•¨ìˆ˜ëŠ” `.env`ì˜ `OLLAMA_HOST`ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, time, requests\n",
        "from typing import List, Dict\n",
        "\n",
        "def chat_ollama(model: str, messages: List[Dict], stream: bool = False, host: str = None):\n",
        "    host = host or os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
        "    url = f\"{host}/api/chat\"\n",
        "    headers = {\"Content-Type\":\"application/json\"}\n",
        "    payload = {\"model\": model, \"messages\": messages, \"stream\": stream}\n",
        "    t0 = time.time()\n",
        "    resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)\n",
        "    dt = time.time() - t0\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    text = data.get(\"message\", {}).get(\"content\", \"\")\n",
        "    return text, dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) ê³µê°œ ë°ì´í„° ë¡œë“œ (MIRACL-ko)\n",
        "- ì¿¼ë¦¬/ì •ë‹µ(dev), ì½”í¼ìŠ¤(train)ì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "- ì½”í¼ìŠ¤ëŠ” `CORPUS_MAX` ë§Œí¼ë§Œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['docid', 'title', 'text'],\n",
            "    num_rows: 500000\n",
            "})\n",
            "{'docid': '5#0', 'title': 'ì§€ë¯¸ ì¹´í„°', 'text': 'ì œì„ìŠ¤ ì–¼ \"ì§€ë¯¸\" ì¹´í„° ì£¼ë‹ˆì–´(, 1924ë…„ 10ì›” 1ì¼ ~ )ëŠ” ë¯¼ì£¼ë‹¹ ì¶œì‹  ë¯¸êµ­ 39ë²ˆì§¸ ëŒ€í†µë ¹ (1977ë…„ ~ 1981ë…„)ì´ë‹¤.'}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5#0</td>\n",
              "      <td>ì§€ë¯¸ ì¹´í„°</td>\n",
              "      <td>ì œì„ìŠ¤ ì–¼ \"ì§€ë¯¸\" ì¹´í„° ì£¼ë‹ˆì–´(, 1924ë…„ 10ì›” 1ì¼ ~ )ëŠ” ë¯¼ì£¼ë‹¹ ì¶œì‹  ë¯¸...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5#1</td>\n",
              "      <td>ì§€ë¯¸ ì¹´í„°</td>\n",
              "      <td>ì§€ë¯¸ ì¹´í„°ëŠ” ì¡°ì§€ì•„ì£¼ ì„¬í„° ì¹´ìš´í‹° í”Œë ˆì¸ìŠ¤ ë§ˆì„ì—ì„œ íƒœì–´ë‚¬ë‹¤. ì¡°ì§€ì•„ ê³µê³¼ëŒ€í•™êµë¥¼ ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  doc_id  title                                               text\n",
              "0    5#0  ì§€ë¯¸ ì¹´í„°  ì œì„ìŠ¤ ì–¼ \"ì§€ë¯¸\" ì¹´í„° ì£¼ë‹ˆì–´(, 1924ë…„ 10ì›” 1ì¼ ~ )ëŠ” ë¯¼ì£¼ë‹¹ ì¶œì‹  ë¯¸...\n",
              "1    5#1  ì§€ë¯¸ ì¹´í„°  ì§€ë¯¸ ì¹´í„°ëŠ” ì¡°ì§€ì•„ì£¼ ì„¬í„° ì¹´ìš´í‹° í”Œë ˆì¸ìŠ¤ ë§ˆì„ì—ì„œ íƒœì–´ë‚¬ë‹¤. ì¡°ì§€ì•„ ê³µê³¼ëŒ€í•™êµë¥¼ ..."
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "queries_ds = load_dataset(\"Cohere/miracl-ko-queries-22-12\", split=\"dev\")\n",
        "# MIRACL í•œêµ­ì–´ ì½”í¼ìŠ¤ ìƒ¤ë“œ (docs-0.jsonl.gz ë“±)\n",
        "url = \"https://huggingface.co/datasets/miracl/miracl-corpus/resolve/main/miracl-corpus-v1.0-ko/docs-0.jsonl.gz\"\n",
        "\n",
        "corpus_ds = load_dataset(\"json\", data_files=url, split=\"train\")\n",
        "\n",
        "print(corpus_ds)\n",
        "print(corpus_ds[0])\n",
        "\n",
        "corpus_ds = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"https://huggingface.co/datasets/miracl/miracl-corpus/resolve/main/miracl-corpus-v1.0-ko/docs-*.jsonl.gz\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "\n",
        "if CORPUS_MAX and len(corpus_ds) > CORPUS_MAX:\n",
        "    corpus_ds = corpus_ds.select(range(CORPUS_MAX))\n",
        "\n",
        "corpus_df = corpus_ds.to_pandas().rename(columns={\"docid\":\"doc_id\"})\n",
        "corpus_df[\"doc_id\"] = corpus_df[\"doc_id\"].astype(str)\n",
        "\n",
        "# ê²€ìƒ‰ í›„ ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ íšŒìˆ˜í•˜ê¸° ìœ„í•´ lookup dict ì¤€ë¹„\n",
        "ID2TEXT  = dict(zip(corpus_df[\"doc_id\"], corpus_df[\"text\"].fillna(\"\").astype(str)))\n",
        "ID2TITLE = dict(zip(corpus_df[\"doc_id\"], corpus_df[\"title\"].fillna(\"\").astype(str)))\n",
        "\n",
        "corpus_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2d7eae3e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(corpus_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Sparse (BM25) ì¸ì½”ë” ì í•©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:07<00:00, 1018.40it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(8000, 8000)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pinecone_text.sparse import BM25Encoder\n",
        "from tqdm import trange\n",
        "import numpy as np, pickle, os\n",
        "\n",
        "texts = (corpus_df[\"title\"].fillna(\"\") + \" \" + corpus_df[\"text\"].fillna(\"\")).tolist()\n",
        "bm25 = BM25Encoder()\n",
        "bm25.fit(texts)\n",
        "sparse_vectors = bm25.encode_documents(texts)\n",
        "\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "with open(\"artifacts/bm25_encoder.pkl\",\"wb\") as f:\n",
        "    pickle.dump(bm25, f)\n",
        "\n",
        "len(sparse_vectors), len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Dense ì„ë² ë”©: BGE-M3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8dd8dc25",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math, numpy as np\n",
        "\n",
        "CANDIDATE_TEXT_KEYS = (\"text\", \"content\", \"body\", \"doc\", \"passage\", \"sentence\")\n",
        "\n",
        "def to_text(x):\n",
        "    \"\"\"ì…ë ¥ xë¥¼ ì•ˆì „í•œ strë¡œ ë³€í™˜(ì‹¤íŒ¨ ì‹œ None).\"\"\"\n",
        "    if x is None:\n",
        "        return None\n",
        "    # NaN ì²˜ë¦¬ (float/np.float ê³„ì—´)\n",
        "    if isinstance(x, (float, np.floating)) and math.isnan(x):\n",
        "        return None\n",
        "    # bytes -> str\n",
        "    if isinstance(x, (bytes, bytearray)):\n",
        "        try:\n",
        "            return x.decode(\"utf-8\", errors=\"ignore\")\n",
        "        except Exception:\n",
        "            return None\n",
        "    # dictì—ì„œ í…ìŠ¤íŠ¸ í•„ë“œ ì¶”ì¶œ\n",
        "    if isinstance(x, dict):\n",
        "        for k in CANDIDATE_TEXT_KEYS:\n",
        "            if k in x:\n",
        "                return to_text(x[k])\n",
        "        return None\n",
        "    # (í† í° ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë¼) ë¬¸ìì—´ ì¡°í•© í•„ìš”í•œ ê²½ìš°\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        # ì´ë¯¸ í† í°í™”ëœ list[str]ì€ í—ˆìš©ë˜ì§€ë§Œ ì—¬ê¸°ì„  concatí•´ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¦\n",
        "        if all(isinstance(t, str) for t in x):\n",
        "            return \" \".join(x)\n",
        "        # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë¬¸ìì—´í™” ì‹œë„\n",
        "        try:\n",
        "            return \" \".join(map(str, x))\n",
        "        except Exception:\n",
        "            return None\n",
        "    # ë„˜íŒŒì´ ìŠ¤ì¹¼ë¼/ì •ìˆ˜ ë“± â†’ ë¬¸ìì—´\n",
        "    if isinstance(x, (int, np.integer)) or isinstance(x, (np.floating,)):\n",
        "        return str(x)\n",
        "    # ê·¸ ì™¸ ê°ì²´ â†’ ë¬¸ìì—´í™” ì‹œë„\n",
        "    if not isinstance(x, str):\n",
        "        try:\n",
        "            x = str(x)\n",
        "        except Exception:\n",
        "            return None\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "12e945da",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_texts = texts  # ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸/ì‹œë¦¬ì¦ˆ/ì œë„ˆë ˆì´í„° ë“±\n",
        "\n",
        "cleaned_texts = []\n",
        "bad_indices = []\n",
        "for i, x in enumerate(raw_texts):\n",
        "    s = to_text(x)\n",
        "    if s is None:\n",
        "        bad_indices.append(i)\n",
        "        continue\n",
        "    s = s.strip()\n",
        "    if not s:\n",
        "        bad_indices.append(i)\n",
        "        continue\n",
        "    cleaned_texts.append(s)\n",
        "\n",
        "if bad_indices:\n",
        "    print(f\"[warn] {len(bad_indices)}ê°œì˜ ë ˆì½”ë“œê°€ ë¬¸ìì—´ì´ ì•„ë‹ˆê±°ë‚˜ ë¹„ì–´ ìˆì–´ ì œì™¸ë¨. ì˜ˆì‹œ ì¸ë±ìŠ¤: {bad_indices[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c27cb64",
      "metadata": {},
      "source": [
        "### ì•„ë˜ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° ì„ë² ë”© ë¬¸ì„œ ì €ì¥ ì‹œ ë©”ëª¨ë¦¬ ë¶€ì¡±ì´ ëœ° ìˆ˜ ìˆì–´ì„œ, Batch 32ê°œì”© ì§„í–‰ (53ë¶„ ì†Œìš”), ë”°ë¼ì„œ ë” ë¹ ë¥´ê²Œ ê³ ì‚¬ì–‘ìœ¼ë¡œ ì§„í–‰í•˜ê³ ì í•œë‹¤ë©´, Batchë¥¼ 64, 128ë¡œ ì§„í–‰í•´ì„œ ì‹œê°„ì„ ìµœì í™”í•  ê²ƒ!\n",
        "\n",
        "### BGE-M3 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ cleaned_textsë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ dense ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "ê° ë°°ì¹˜ì—ì„œ dense ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ê³  L2 ì •ê·œí™”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì•ˆì •í™”í•©ë‹ˆë‹¤.\n",
        "ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ë¥¼ ìˆ˜ì§ìœ¼ë¡œ ê²°í•©í•˜ì—¬ dense_vecs ë°°ì—´ì„ ë§Œë“¤ê³ , float32ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "ë²¡í„° ì°¨ì›ì„ í™•ì¸í•˜ê³ , ì „ì²´ shapeì™€ ì°¨ì›ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4ea2ddce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: safetensors in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (0.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Fetching 30 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:00<00:00, 171429.32it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     18\u001b[39m use_fp16 = use_cuda\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# ğŸ§  BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# - \"BAAI/bge-m3\" í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ë°›ì•„ì˜µë‹ˆë‹¤(ìµœì´ˆ 1íšŒ ìºì‹œ í›„ ì¬ì‚¬ìš©).\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# - use_fp16: ìœ„ì—ì„œ ê²°ì •í•œ half precision ì‚¬ìš©\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - devices: ì–´ë–¤ ë””ë°”ì´ìŠ¤ì— ì˜¬ë¦´ì§€ ì§€ì • (ì˜ˆ: \"cuda:0\" ë˜ëŠ” \"cpu\")\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# bge = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=use_fp16, devices=device)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m bge = \u001b[43mBGEM3FlagModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBAAI/bge-m3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# âœ‚ï¸ ìµœëŒ€ í† í° ê¸¸ì´: 512~1024 ê¶Œì¥(ê¸¸ìˆ˜ë¡ ë” ê¸´ ë¬¸ë§¥ì„ ì»¤ë²„í•˜ì§€ë§Œ ì†ë„/VRAM ì¦ê°€)\u001b[39;00m\n\u001b[32m     33\u001b[39m MAX_LEN = \u001b[32m1024\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py:95\u001b[39m, in \u001b[36mM3Embedder.__init__\u001b[39m\u001b[34m(self, model_name_or_path, normalize_embeddings, use_fp16, query_instruction_for_retrieval, query_instruction_format, devices, pooling_method, trust_remote_code, cache_dir, colbert_dim, batch_size, query_max_length, passage_max_length, return_dense, return_sparse, return_colbert_vecs, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.pooling_method = pooling_method\n\u001b[32m     89\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = AutoTokenizer.from_pretrained(\n\u001b[32m     90\u001b[39m     model_name_or_path,\n\u001b[32m     91\u001b[39m     trust_remote_code=trust_remote_code,\n\u001b[32m     92\u001b[39m     cache_dir=cache_dir\n\u001b[32m     93\u001b[39m )\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m.model = EncoderOnlyEmbedderM3ModelForInference(\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[43mEncoderOnlyEmbedderM3Runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolbert_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolbert_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    101\u001b[39m     tokenizer=\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m    102\u001b[39m     sentence_pooling_method=pooling_method,\n\u001b[32m    103\u001b[39m     normalize_embeddings=normalize_embeddings\n\u001b[32m    104\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/FlagEmbedding/finetune/embedder/encoder_only/m3/runner.py:69\u001b[39m, in \u001b[36mEncoderOnlyEmbedderM3Runner.get_model\u001b[39m\u001b[34m(model_name_or_path, trust_remote_code, colbert_dim, cache_dir)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(model_name_or_path):\n\u001b[32m     63\u001b[39m     model_name_or_path = snapshot_download(\n\u001b[32m     64\u001b[39m         repo_id=model_name_or_path,\n\u001b[32m     65\u001b[39m         cache_dir=cache_folder,\n\u001b[32m     66\u001b[39m         ignore_patterns=[\u001b[33m'\u001b[39m\u001b[33mflax_model.msgpack\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrust_model.ot\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtf_model.h5\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     67\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m colbert_linear = torch.nn.Linear(\n\u001b[32m     75\u001b[39m     in_features=model.config.hidden_size,\n\u001b[32m     76\u001b[39m     out_features=model.config.hidden_size \u001b[38;5;28;01mif\u001b[39;00m colbert_dim <= \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m colbert_dim\n\u001b[32m     77\u001b[39m )\n\u001b[32m     78\u001b[39m sparse_linear = torch.nn.Linear(\n\u001b[32m     79\u001b[39m     in_features=model.config.hidden_size,\n\u001b[32m     80\u001b[39m     out_features=\u001b[32m1\u001b[39m\n\u001b[32m     81\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/modeling_utils.py:5051\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5041\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5042\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5044\u001b[39m     (\n\u001b[32m   5045\u001b[39m         model,\n\u001b[32m   5046\u001b[39m         missing_keys,\n\u001b[32m   5047\u001b[39m         unexpected_keys,\n\u001b[32m   5048\u001b[39m         mismatched_keys,\n\u001b[32m   5049\u001b[39m         offload_index,\n\u001b[32m   5050\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5051\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5065\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5066\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5067\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5068\u001b[39m model.tie_weights()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/modeling_utils.py:5319\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5316\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   5317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5318\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m5319\u001b[39m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m.keys()\n\u001b[32m   5320\u001b[39m     )\n\u001b[32m   5322\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   5323\u001b[39m prefix = model.base_model_prefix\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/modeling_utils.py:508\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/utils/import_utils.py:1647\u001b[39m, in \u001b[36mcheck_torch_load_is_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1645\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_torch_load_is_safe\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1646\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[33m\"\u001b[39m\u001b[33m2.6\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1648\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1649\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1650\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen loading files with safetensors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1651\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1652\u001b[39m         )\n",
            "\u001b[31mValueError\u001b[39m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
          ]
        }
      ],
      "source": [
        "import os, torch, numpy as np\n",
        "from tqdm import trange\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "\n",
        "# ğŸ¤« í† í¬ë‚˜ì´ì € ë©€í‹°í”„ë¡œì„¸ìŠ¤ ê²½ê³  ì–µì œìš© í™˜ê²½ë³€ìˆ˜.\n",
        "# ë³‘ë ¬ í† í¬ë‚˜ì´ì§•ìœ¼ë¡œ ì¸í•œ ê²½ê³ /ì¶œë ¥ ì„ì„ì„ í”¼í•˜ê³  ì‹¶ì„ ë•Œ Falseë¡œ ë‘¡ë‹ˆë‹¤.\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# âš™ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ CUDA(GPU)ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# ì¥ì¹˜ ì„ íƒ: GPUê°€ ìˆìœ¼ë©´ \"cuda:0\", ì—†ìœ¼ë©´ \"cpu\"\n",
        "device = \"cuda:0\" if use_cuda else \"cpu\"\n",
        "\n",
        "# FP16(half precision) ì‚¬ìš© ì—¬ë¶€: ë³´í†µ GPUê°€ ìˆì„ ë•Œë§Œ ì¼­ë‹ˆë‹¤.\n",
        "# - ì¥ì : ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ ê°€ëŠ¥\n",
        "# - ë‹¨ì : ì•„ì£¼ ë“œë¬¸ ê²½ìš° ìˆ˜ì¹˜ ì •ë°€ë„ ì´ìŠˆ\n",
        "use_fp16 = use_cuda\n",
        "\n",
        "# ğŸ§  BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
        "# - \"BAAI/bge-m3\" í—ˆê¹…í˜ì´ìŠ¤ í—ˆë¸Œì—ì„œ ë°›ì•„ì˜µë‹ˆë‹¤(ìµœì´ˆ 1íšŒ ìºì‹œ í›„ ì¬ì‚¬ìš©).\n",
        "# - use_fp16: ìœ„ì—ì„œ ê²°ì •í•œ half precision ì‚¬ìš©\n",
        "# - devices: ì–´ë–¤ ë””ë°”ì´ìŠ¤ì— ì˜¬ë¦´ì§€ ì§€ì • (ì˜ˆ: \"cuda:0\" ë˜ëŠ” \"cpu\")\n",
        "# bge = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=use_fp16, devices=device)\n",
        "bge = BGEM3FlagModel(\n",
        "    \"BAAI/bge-m3\",\n",
        "    use_fp16=use_fp16,\n",
        "    devices=device,\n",
        "    use_safetensors=True\n",
        ")\n",
        "\n",
        "# âœ‚ï¸ ìµœëŒ€ í† í° ê¸¸ì´: 512~1024 ê¶Œì¥(ê¸¸ìˆ˜ë¡ ë” ê¸´ ë¬¸ë§¥ì„ ì»¤ë²„í•˜ì§€ë§Œ ì†ë„/VRAM ì¦ê°€)\n",
        "MAX_LEN = 1024\n",
        "\n",
        "# ğŸ“¦ ë°°ì¹˜ í¬ê¸°: 16~32ë¶€í„° ì‹œì‘í•´ì„œ VRAM/ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì •\n",
        "BATCH   = 32\n",
        "\n",
        "# ëª¨ë“  ë¬¸ì„œì˜ dense ì„ë² ë”©ì„ ë‹´ì•„ë‘˜ ë¦¬ìŠ¤íŠ¸ (ë°°ì¹˜ë³„ë¡œ ìŒ“ì•˜ë‹¤ê°€ ë§ˆì§€ë§‰ì— í•©ì¹©ë‹ˆë‹¤)\n",
        "dense_vecs = []\n",
        "\n",
        "# ğŸ”’ ì¶”ë¡  ëª¨ë“œ: ìë™ ë¯¸ë¶„ OFF â†’ ë©”ëª¨ë¦¬ ì ˆì•½ & ì†ë„ í–¥ìƒ\n",
        "with torch.inference_mode():\n",
        "    # trange: ì§„í–‰ìƒí™©(progress bar)ì„ ë³´ì—¬ì£¼ëŠ” range\n",
        "    for i in trange(0, len(cleaned_texts), BATCH):\n",
        "        # í˜„ì¬ ë°°ì¹˜ ìŠ¬ë¼ì´ì‹± (âœ… ë°˜ë“œì‹œ list[str] í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤)\n",
        "        batch = cleaned_texts[i:i+BATCH]\n",
        "\n",
        "        # âœ… ì„ë² ë”© ì¶”ì¶œ\n",
        "        # - batch_size: ë‚´ë¶€ ì²˜ë¦¬ ë°°ì¹˜ í¬ê¸° (ë³´í†µ BATCHì™€ ë™ì¼í•˜ê²Œ ë‘ )\n",
        "        # - max_length: í† í° ìµœëŒ€ ê¸¸ì´(ì´ˆê³¼ë¶„ì€ ëª¨ë¸ í† í¬ë‚˜ì´ì €ì—ì„œ ì˜ë¦½ë‹ˆë‹¤)\n",
        "        # - return_dense: dense ì„ë² ë”©(ANN, ë²¡í„°DBìš©) ë°˜í™˜\n",
        "        # - return_sparse: BM25 ìœ ì‚¬í•œ sparse(í† í° ê¸°ë°˜) ë²¡í„° ë°˜í™˜ ì—¬ë¶€ (ì—¬ê¸°ì„  ë¯¸ì‚¬ìš©)\n",
        "        # - return_colbert_vecs: ColBERT ìŠ¤íƒ€ì¼ í† í° ë‹¨ìœ„ ë²¡í„° ë°˜í™˜ ì—¬ë¶€ (ì—¬ê¸°ì„  ë¯¸ì‚¬ìš©)\n",
        "        out = bge.encode(\n",
        "            batch,\n",
        "            batch_size=BATCH,\n",
        "            max_length=MAX_LEN,\n",
        "            return_dense=True,\n",
        "            return_sparse=False,\n",
        "            return_colbert_vecs=False,\n",
        "        )\n",
        "\n",
        "        # ëª¨ë¸ ì¶œë ¥ì—ì„œ dense ì„ë² ë”©(Numpy ndarray)ì„ êº¼ëƒ…ë‹ˆë‹¤. shape: (batch, dim)\n",
        "        dense = out[\"dense_vecs\"]\n",
        "\n",
        "        # ğŸ”„ L2 ì •ê·œí™”: ê° ë²¡í„°ë¥¼ ë‹¨ìœ„ë²¡í„°ë¡œ(normalize) ë§Œë“¤ì–´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ì•ˆì •í™”\n",
        "        # - np.linalg.norm(..., axis=1, keepdims=True): ê° í–‰(ë¬¸ì„œ)ë³„ L2 ë…¸ë¦„\n",
        "        # - ë¶„ëª¨ê°€ 0ì¸(edge) ê²½ìš°ê°€ ë“œë¬¼ì§€ë§Œ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ, í•„ìš”ì‹œ epsë¥¼ ë”í•´ ë°©ì–´ì½”ë“œ ì¶”ê°€ ê°€ëŠ¥\n",
        "        dense = dense / np.linalg.norm(dense, axis=1, keepdims=True)\n",
        "\n",
        "        # ë°°ì¹˜ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ìŒ“ì•„ë‘ \n",
        "        dense_vecs.append(dense)\n",
        "\n",
        "# ğŸ”§ ì—¬ëŸ¬ ë°°ì¹˜ë¡œ ìŒ“ì¸ ë°°ì—´ë“¤ì„ í•œ ë²ˆì— ì„¸ë¡œë¡œ ì´ì–´ë¶™ì´ê¸°\n",
        "# - ê²°ê³¼ shape: (num_texts, dim)\n",
        "dense_vecs = np.vstack(dense_vecs).astype(\"float32\")  # ë²¡í„°DB í˜¸í™˜/ë©”ëª¨ë¦¬ ì ˆì•½ ìœ„í•´ float32 ìºìŠ¤íŒ…\n",
        "\n",
        "# ë²¡í„° ì°¨ì›(dimension) í™•ì¸\n",
        "dense_dim = dense_vecs.shape[1]\n",
        "\n",
        "# ì „ì²´ ê°œìˆ˜ì™€ ì°¨ì› ì¶œë ¥ (ì˜ˆ: (N, 1024) 1024)\n",
        "print(dense_vecs.shape, dense_dim)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Pinecone ì„œë²„ë¦¬ìŠ¤ ì¸ë±ìŠ¤ ìƒì„± & ì—…ì„œíŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "da02ff74",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dense_dim' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m pc = Pinecone(api_key=PINECONE_API_KEY)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m INDEX_NAME \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pc.list_indexes().names():\n\u001b[32m      6\u001b[39m     pc.create_index(\n\u001b[32m      7\u001b[39m         name=INDEX_NAME,\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         dimension=\u001b[38;5;28mint\u001b[39m(\u001b[43mdense_dim\u001b[49m),\n\u001b[32m      9\u001b[39m         metric=METRIC,\n\u001b[32m     10\u001b[39m         spec=ServerlessSpec(cloud=PC_CLOUD, region=PC_REGION)\n\u001b[32m     11\u001b[39m     )\n\u001b[32m     13\u001b[39m index = pc.Index(INDEX_NAME)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(index.describe_index_stats())\n",
            "\u001b[31mNameError\u001b[39m: name 'dense_dim' is not defined"
          ]
        }
      ],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=int(dense_dim),\n",
        "        metric=METRIC,\n",
        "        spec=ServerlessSpec(cloud=PC_CLOUD, region=PC_REGION)\n",
        "    )\n",
        "\n",
        "index = pc.Index(INDEX_NAME)\n",
        "print(index.describe_index_stats())\n",
        "\n",
        "\n",
        "batch = []\n",
        "for i in trange(len(corpus_df)):\n",
        "    _id = corpus_df.iloc[i][\"doc_id\"]\n",
        "    meta = {\"title\": ID2TITLE[_id], \"lang\":\"ko\", \"source\":\"miracl-ko-wiki\"}\n",
        "    batch.append({\n",
        "        \"id\": _id,\n",
        "        \"values\": dense_vecs[i].tolist(),\n",
        "        \"sparse_values\": sparse_vectors[i],\n",
        "        \"metadata\": meta\n",
        "    })\n",
        "    if len(batch) >= 200:\n",
        "        index.upsert(vectors=batch)\n",
        "        batch = []\n",
        "if batch:\n",
        "    index.upsert(vectors=batch)\n",
        "\n",
        "print(\"Upsert complete.\")\n",
        "print(index.describe_index_stats())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) ê²€ìƒ‰ í•¨ìˆ˜: Dense / Sparse / Weighted(Î±) / RRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "from collections import defaultdict\n",
        "\n",
        "def encode_query_dense(q: str):\n",
        "    return bge.encode([q], max_length=8192)[\"dense_vecs\"][0].astype(\"float32\")\n",
        "\n",
        "def encode_query_sparse(q: str):\n",
        "    return bm25.encode_queries([q])[0]\n",
        "\n",
        "def dense_only_search(query: str, top_k=20):\n",
        "    dv = encode_query_dense(query).tolist()\n",
        "    res = index.query(vector=dv, top_k=top_k, include_metadata=True)\n",
        "    return res.matches\n",
        "\n",
        "def sparse_only_search(query: str, top_k=20):\n",
        "    sv = encode_query_sparse(query)\n",
        "    res = index.query(sparse_vector=sv, top_k=top_k, include_metadata=True)\n",
        "    return res.matches\n",
        "\n",
        "def hybrid_weighted_search(query: str, top_k=20, alpha=ALPHA):\n",
        "    dv = encode_query_dense(query)\n",
        "    sv = encode_query_sparse(query)\n",
        "    dv = (dv * alpha).tolist()\n",
        "    sv_scaled = {\"indices\": sv[\"indices\"], \"values\": [v*(1.0-alpha) for v in sv[\"values\"]]}\n",
        "    res = index.query(vector=dv, sparse_vector=sv_scaled, top_k=top_k, include_metadata=True)\n",
        "    return res.matches\n",
        "\n",
        "def rrf_fusion(query: str, top_k=20, per_list_k=50, k_const=60):\n",
        "    dres = dense_only_search(query, top_k=per_list_k)\n",
        "    sres = sparse_only_search(query, top_k=per_list_k)\n",
        "    meta = {}\n",
        "    ranks = defaultdict(list)\n",
        "    for rlist in [dres, sres]:\n",
        "        for rank, m in enumerate(rlist, start=1):\n",
        "            meta[m.id] = m.metadata\n",
        "            ranks[m.id].append(rank)\n",
        "    scores = []\n",
        "    for _id, rks in ranks.items():\n",
        "        sc = sum(1.0 / (k_const + rk) for rk in rks)\n",
        "        scores.append((_id, sc))\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [{\"id\": _id, \"score\": sc, \"metadata\": meta.get(_id, {})} for _id, sc in scores[:top_k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Re-rank (Cross-Encoder): BGE Reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from FlagEmbedding import FlagReranker\n",
        "\n",
        "reranker = FlagReranker(\"BAAI/bge-reranker-v2-m3\", use_fp16=use_fp16)\n",
        "\n",
        "def rerank_ce(query: str, candidates: List[Dict], top_k=K):\n",
        "    pairs, id2meta = [], {}\n",
        "    for m in candidates:\n",
        "        if hasattr(m, \"id\"):\n",
        "            _id, meta = m.id, m.metadata\n",
        "        else:\n",
        "            _id, meta = m[\"id\"], m.get(\"metadata\", {})\n",
        "        title = meta.get(\"title\",\"\")\n",
        "        pairs.append([query, title])\n",
        "        id2meta[_id] = meta\n",
        "    scores = reranker.compute_score(pairs)\n",
        "    items = [{\"id\": _id, \"score\": float(sc), \"metadata\": id2meta[_id]} for (_id, sc) in zip(id2meta.keys(), scores)]\n",
        "    items.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    return items[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) ìƒì„±(Answer): Ollama + ê·¼ê±° ì»¨í…ìŠ¤íŠ¸ ì£¼ì…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_context(doc_ids: List[str], max_chars: int = 3000) -> str:\n",
        "    parts, total = [], 0\n",
        "    for did in doc_ids:\n",
        "        t = ID2TITLE.get(did, \"\")\n",
        "        x = ID2TEXT.get(did, \"\")\n",
        "        snippet = (x[:700] + \"...\") if len(x) > 700 else x\n",
        "        block = f\"[{did}] {t}\\n{snippet}\"\n",
        "        if total + len(block) > max_chars:\n",
        "            break\n",
        "        parts.append(block)\n",
        "        total += len(block)\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "def answer_with_ollama(query: str, topk_items: List[Dict], model: str = None):\n",
        "    model = model or os.getenv(\"OLLAMA_MODEL\", \"llama3.1:8b-instruct\")\n",
        "    doc_ids = [m[\"id\"] if isinstance(m, dict) else m.id for m in topk_items]\n",
        "    ctx = collect_context(doc_ids, max_chars=3000)\n",
        "\n",
        "    system = (\n",
        "        \"ë‹¹ì‹ ì€ í•œêµ­ì–´ RAG ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ 'ê·¼ê±° ì»¨í…ìŠ¤íŠ¸'ì— í¬í•¨ëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì—¬ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•˜ì„¸ìš”. \"\n",
        "        \"í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê³  ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ì„¸ìš”. \"\n",
        "        \"í•„ìš”í•˜ë©´ ë¬¸ì„œ IDë¡œ ê° ê·¼ê±°ë¥¼ í‘œê¸°í•˜ì„¸ìš”.\"\n",
        "    )\n",
        "    user = f\"ì§ˆë¬¸: {query}\\n\\n[ê·¼ê±° ì»¨í…ìŠ¤íŠ¸]\\n{ctx}\"\n",
        "    messages = [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
        "    text, dt = chat_ollama(model=model, messages=messages, stream=False)\n",
        "    return text, dt, ctx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) ë°ëª¨: ì§ˆì˜ â†’ Hybrid+CE â†’ Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_queries = [q for q in queries_ds[\"query\"][:10] if len(q) > 5][:3]\n",
        "\n",
        "for q in demo_queries:\n",
        "    print(\"=\"*100)\n",
        "    print(\"ì§ˆë¬¸:\", q)\n",
        "    cand = hybrid_weighted_search(q, top_k=N, alpha=ALPHA)\n",
        "    topk = rerank_ce(q, cand, top_k=K)\n",
        "    try:\n",
        "        answer, dt, ctx = answer_with_ollama(q, topk)\n",
        "        print(\"[ìƒì„± ì†Œìš”]\", f\"{dt:.2f}s\")\n",
        "        print(\"[ë‹µë³€]\\n\", answer)\n",
        "        print(\"\\n[ê·¼ê±° ì»¨í…ìŠ¤íŠ¸]\\n\", ctx[:1000], \"...\")\n",
        "    except Exception as e:\n",
        "        print(\"Ollama í˜¸ì¶œ ì‹¤íŒ¨:\", e)\n",
        "        print(\"â†’ OLLAMA_HOST ì„¤ì • ë° ì„œë²„ ì‹¤í–‰ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2df3f7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
