{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 고급 RAG 실습 (v2): .env + Pinecone + Ollama\n",
        "- 날짜: 2025-09-29\n",
        "- 이 노트북은 **.env**에서 환경변수를 불러와 **Pinecone**과 **Ollama**를 함께 사용하는 하이브리드 RAG 파이프라인을 제공합니다.\n",
        "- 구성: **MIRACL-ko 공개 데이터** → Dense(BGE-M3) + Sparse(BM25) → RRF/Weighted → **BGE Reranker** → **Ollama 생성**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) .env 템플릿 (로컬에 파일 생성)\n",
        "아래 내용을 프로젝트 루트의 `.env` 파일에 저장하세요.\n",
        "\n",
        "```\n",
        "PINECONE_API_KEY=pc_********************************\n",
        "PC_CLOUD=aws\n",
        "PC_REGION=us-east-1\n",
        "PC_INDEX_NAME=hybrid-miracl-ko\n",
        "\n",
        "# Ollama 기본 호스트 (로컬 실행 시)\n",
        "OLLAMA_HOST=http://localhost:11434\n",
        "# 선호 모델 예: llama3.1:8b-instruct, qwen2.5:7b-instruct 등\n",
        "OLLAMA_MODEL=llama3.1:8b-instruct\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # !pip -q install -U pip\n",
        "!pip -q install -U python-dotenv requests pinecone pinecone-text FlagEmbedding datasets rank_bm25 matplotlib tqdm\n",
        "# 선택: torch 최신 (환경에 따라 생략 가능)\n",
        "# !pip -q install -U torch --index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9f77f239",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: pinecone 7.3.0\n",
            "Uninstalling pinecone-7.3.0:\n",
            "  Successfully uninstalled pinecone-7.3.0\n",
            "\u001b[33mWARNING: Skipping pinecone-client as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping pinecone-plugin-inference as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pinecone\n",
            "  Using cached pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (2025.10.5)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (1.8.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: packaging<25.0,>=24.2 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.7)\n",
            "Requirement already satisfied: six>=1.5 in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Using cached pinecone-7.3.0-py3-none-any.whl (587 kB)\n",
            "Installing collected packages: pinecone\n",
            "Successfully installed pinecone-7.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y pinecone pinecone-client pinecone-plugin-inference\n",
        "!pip install -U pinecone\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) 환경 변수 로드 (.env) & 구성\n",
        "- `.env`를 읽어 **PINECONE_API_KEY**, **OLLAMA_HOST**, 기타 파라미터를 설정합니다.\n",
        "- α 프리셋 / N·k 템플릿도 환경 변수 또는 기본값으로 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone: hybrid-miracl-ko aws us-east-1 dotproduct\n",
            "Ollama: http://localhost:11434 llama3.1:8b-instruct\n",
            "Scale: 8000 30  | α= 0.5  N= 100  K= 5\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Pinecone\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
        "PC_CLOUD  = os.getenv(\"PC_CLOUD\", \"aws\")\n",
        "PC_REGION = os.getenv(\"PC_REGION\", \"us-east-1\")\n",
        "INDEX_NAME = os.getenv(\"PC_INDEX_NAME\", \"hybrid-miracl-ko\")\n",
        "METRIC = os.getenv(\"PC_METRIC\", \"dotproduct\")\n",
        "\n",
        "# Ollama\n",
        "OLLAMA_HOST  = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
        "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.1:8b-instruct\")\n",
        "\n",
        "# 실습 스케일\n",
        "CORPUS_MAX = int(os.getenv(\"CORPUS_MAX\", \"8000\"))\n",
        "N_QUERIES  = int(os.getenv(\"N_QUERIES\", \"30\"))\n",
        "\n",
        "# α 프리셋\n",
        "ALPHA_PRESETS = {\"balanced\":0.5, \"semantic_heavy\":0.7, \"keyword_heavy\":0.3}\n",
        "ALPHA = float(os.getenv(\"ALPHA\", ALPHA_PRESETS[\"balanced\"]))\n",
        "\n",
        "# N/k 템플릿\n",
        "TEMPLATES = {\n",
        "    \"speed\":   {\"N\": 50,  \"k\": 3},\n",
        "    \"balanced\":{\"N\": 100, \"k\": 5},\n",
        "    \"quality\": {\"N\": 200, \"k\": 10}\n",
        "}\n",
        "N = int(os.getenv(\"RETRIEVER_N\", TEMPLATES[\"balanced\"][\"N\"]))\n",
        "K = int(os.getenv(\"LLM_K\",       TEMPLATES[\"balanced\"][\"k\"]))\n",
        "\n",
        "print(\"Pinecone:\", INDEX_NAME, PC_CLOUD, PC_REGION, METRIC)\n",
        "print(\"Ollama:\", OLLAMA_HOST, OLLAMA_MODEL)\n",
        "print(\"Scale:\", CORPUS_MAX, N_QUERIES, \" | α=\", ALPHA, \" N=\", N, \" K=\", K)\n",
        "assert PINECONE_API_KEY, \"PINECONE_API_KEY is required (set in .env)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Ollama 채팅 함수 (요청하신 패턴 반영)\n",
        "- 아래 함수는 `.env`의 `OLLAMA_HOST`를 기본으로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json, time, requests\n",
        "from typing import List, Dict\n",
        "\n",
        "def chat_ollama(model: str, messages: List[Dict], stream: bool = False, host: str = None):\n",
        "    host = host or os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
        "    url = f\"{host}/api/chat\"\n",
        "    headers = {\"Content-Type\":\"application/json\"}\n",
        "    payload = {\"model\": model, \"messages\": messages, \"stream\": stream}\n",
        "    t0 = time.time()\n",
        "    resp = requests.post(url, headers=headers, data=json.dumps(payload), timeout=120)\n",
        "    dt = time.time() - t0\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    text = data.get(\"message\", {}).get(\"content\", \"\")\n",
        "    return text, dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) 공개 데이터 로드 (MIRACL-ko)\n",
        "- 쿼리/정답(dev), 코퍼스(train)을 로드합니다.\n",
        "- 코퍼스는 `CORPUS_MAX` 만큼만 샘플링합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['docid', 'title', 'text'],\n",
            "    num_rows: 500000\n",
            "})\n",
            "{'docid': '5#0', 'title': '지미 카터', 'text': '제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.'}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5#0</td>\n",
              "      <td>지미 카터</td>\n",
              "      <td>제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5#1</td>\n",
              "      <td>지미 카터</td>\n",
              "      <td>지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  doc_id  title                                               text\n",
              "0    5#0  지미 카터  제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미...\n",
              "1    5#1  지미 카터  지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 ..."
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "queries_ds = load_dataset(\"Cohere/miracl-ko-queries-22-12\", split=\"dev\")\n",
        "# MIRACL 한국어 코퍼스 샤드 (docs-0.jsonl.gz 등)\n",
        "url = \"https://huggingface.co/datasets/miracl/miracl-corpus/resolve/main/miracl-corpus-v1.0-ko/docs-0.jsonl.gz\"\n",
        "\n",
        "corpus_ds = load_dataset(\"json\", data_files=url, split=\"train\")\n",
        "\n",
        "print(corpus_ds)\n",
        "print(corpus_ds[0])\n",
        "\n",
        "corpus_ds = load_dataset(\n",
        "    \"json\",\n",
        "    data_files=\"https://huggingface.co/datasets/miracl/miracl-corpus/resolve/main/miracl-corpus-v1.0-ko/docs-*.jsonl.gz\",\n",
        "    split=\"train\"\n",
        ")\n",
        "\n",
        "\n",
        "if CORPUS_MAX and len(corpus_ds) > CORPUS_MAX:\n",
        "    corpus_ds = corpus_ds.select(range(CORPUS_MAX))\n",
        "\n",
        "corpus_df = corpus_ds.to_pandas().rename(columns={\"docid\":\"doc_id\"})\n",
        "corpus_df[\"doc_id\"] = corpus_df[\"doc_id\"].astype(str)\n",
        "\n",
        "# 검색 후 문서 텍스트를 회수하기 위해 lookup dict 준비\n",
        "ID2TEXT  = dict(zip(corpus_df[\"doc_id\"], corpus_df[\"text\"].fillna(\"\").astype(str)))\n",
        "ID2TITLE = dict(zip(corpus_df[\"doc_id\"], corpus_df[\"title\"].fillna(\"\").astype(str)))\n",
        "\n",
        "corpus_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2d7eae3e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(corpus_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Sparse (BM25) 인코더 적합"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8000/8000 [00:07<00:00, 1018.40it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(8000, 8000)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pinecone_text.sparse import BM25Encoder\n",
        "from tqdm import trange\n",
        "import numpy as np, pickle, os\n",
        "\n",
        "texts = (corpus_df[\"title\"].fillna(\"\") + \" \" + corpus_df[\"text\"].fillna(\"\")).tolist()\n",
        "bm25 = BM25Encoder()\n",
        "bm25.fit(texts)\n",
        "sparse_vectors = bm25.encode_documents(texts)\n",
        "\n",
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "with open(\"artifacts/bm25_encoder.pkl\",\"wb\") as f:\n",
        "    pickle.dump(bm25, f)\n",
        "\n",
        "len(sparse_vectors), len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Dense 임베딩: BGE-M3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8dd8dc25",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math, numpy as np\n",
        "\n",
        "CANDIDATE_TEXT_KEYS = (\"text\", \"content\", \"body\", \"doc\", \"passage\", \"sentence\")\n",
        "\n",
        "def to_text(x):\n",
        "    \"\"\"입력 x를 안전한 str로 변환(실패 시 None).\"\"\"\n",
        "    if x is None:\n",
        "        return None\n",
        "    # NaN 처리 (float/np.float 계열)\n",
        "    if isinstance(x, (float, np.floating)) and math.isnan(x):\n",
        "        return None\n",
        "    # bytes -> str\n",
        "    if isinstance(x, (bytes, bytearray)):\n",
        "        try:\n",
        "            return x.decode(\"utf-8\", errors=\"ignore\")\n",
        "        except Exception:\n",
        "            return None\n",
        "    # dict에서 텍스트 필드 추출\n",
        "    if isinstance(x, dict):\n",
        "        for k in CANDIDATE_TEXT_KEYS:\n",
        "            if k in x:\n",
        "                return to_text(x[k])\n",
        "        return None\n",
        "    # (토큰 리스트가 아니라) 문자열 조합 필요한 경우\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        # 이미 토큰화된 list[str]은 허용되지만 여기선 concat해 하나의 문장으로 만듦\n",
        "        if all(isinstance(t, str) for t in x):\n",
        "            return \" \".join(x)\n",
        "        # 그렇지 않으면 문자열화 시도\n",
        "        try:\n",
        "            return \" \".join(map(str, x))\n",
        "        except Exception:\n",
        "            return None\n",
        "    # 넘파이 스칼라/정수 등 → 문자열\n",
        "    if isinstance(x, (int, np.integer)) or isinstance(x, (np.floating,)):\n",
        "        return str(x)\n",
        "    # 그 외 객체 → 문자열화 시도\n",
        "    if not isinstance(x, str):\n",
        "        try:\n",
        "            x = str(x)\n",
        "        except Exception:\n",
        "            return None\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "12e945da",
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_texts = texts  # 기존 리스트/시리즈/제너레이터 등\n",
        "\n",
        "cleaned_texts = []\n",
        "bad_indices = []\n",
        "for i, x in enumerate(raw_texts):\n",
        "    s = to_text(x)\n",
        "    if s is None:\n",
        "        bad_indices.append(i)\n",
        "        continue\n",
        "    s = s.strip()\n",
        "    if not s:\n",
        "        bad_indices.append(i)\n",
        "        continue\n",
        "    cleaned_texts.append(s)\n",
        "\n",
        "if bad_indices:\n",
        "    print(f\"[warn] {len(bad_indices)}개의 레코드가 문자열이 아니거나 비어 있어 제외됨. 예시 인덱스: {bad_indices[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c27cb64",
      "metadata": {},
      "source": [
        "### 아래 임베딩 모델 로드 및 임베딩 문서 저장 시 메모리 부족이 뜰 수 있어서, Batch 32개씩 진행 (53분 소요), 따라서 더 빠르게 고사양으로 진행하고자 한다면, Batch를 64, 128로 진행해서 시간을 최적화할 것!\n",
        "\n",
        "### BGE-M3 모델을 사용하여 cleaned_texts를 배치 단위로 dense 임베딩을 생성합니다.\n",
        "각 배치에서 dense 벡터를 추출하고 L2 정규화하여 코사인 유사도를 안정화합니다.\n",
        "모든 배치 결과를 수직으로 결합하여 dense_vecs 배열을 만들고, float32로 변환합니다.\n",
        "벡터 차원을 확인하고, 전체 shape와 차원을 출력합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4ea2ddce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: safetensors in /home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages (0.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 171429.32it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     18\u001b[39m use_fp16 = use_cuda\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 🧠 BGE-M3 임베딩 모델 로드\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# - \"BAAI/bge-m3\" 허깅페이스 허브에서 받아옵니다(최초 1회 캐시 후 재사용).\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# - use_fp16: 위에서 결정한 half precision 사용\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - devices: 어떤 디바이스에 올릴지 지정 (예: \"cuda:0\" 또는 \"cpu\")\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# bge = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=use_fp16, devices=device)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m bge = \u001b[43mBGEM3FlagModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBAAI/bge-m3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# ✂️ 최대 토큰 길이: 512~1024 권장(길수록 더 긴 문맥을 커버하지만 속도/VRAM 증가)\u001b[39;00m\n\u001b[32m     33\u001b[39m MAX_LEN = \u001b[32m1024\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/FlagEmbedding/inference/embedder/encoder_only/m3.py:95\u001b[39m, in \u001b[36mM3Embedder.__init__\u001b[39m\u001b[34m(self, model_name_or_path, normalize_embeddings, use_fp16, query_instruction_for_retrieval, query_instruction_format, devices, pooling_method, trust_remote_code, cache_dir, colbert_dim, batch_size, query_max_length, passage_max_length, return_dense, return_sparse, return_colbert_vecs, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.pooling_method = pooling_method\n\u001b[32m     89\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = AutoTokenizer.from_pretrained(\n\u001b[32m     90\u001b[39m     model_name_or_path,\n\u001b[32m     91\u001b[39m     trust_remote_code=trust_remote_code,\n\u001b[32m     92\u001b[39m     cache_dir=cache_dir\n\u001b[32m     93\u001b[39m )\n\u001b[32m     94\u001b[39m \u001b[38;5;28mself\u001b[39m.model = EncoderOnlyEmbedderM3ModelForInference(\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[43mEncoderOnlyEmbedderM3Runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolbert_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolbert_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    101\u001b[39m     tokenizer=\u001b[38;5;28mself\u001b[39m.tokenizer,\n\u001b[32m    102\u001b[39m     sentence_pooling_method=pooling_method,\n\u001b[32m    103\u001b[39m     normalize_embeddings=normalize_embeddings\n\u001b[32m    104\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/FlagEmbedding/finetune/embedder/encoder_only/m3/runner.py:69\u001b[39m, in \u001b[36mEncoderOnlyEmbedderM3Runner.get_model\u001b[39m\u001b[34m(model_name_or_path, trust_remote_code, colbert_dim, cache_dir)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(model_name_or_path):\n\u001b[32m     63\u001b[39m     model_name_or_path = snapshot_download(\n\u001b[32m     64\u001b[39m         repo_id=model_name_or_path,\n\u001b[32m     65\u001b[39m         cache_dir=cache_folder,\n\u001b[32m     66\u001b[39m         ignore_patterns=[\u001b[33m'\u001b[39m\u001b[33mflax_model.msgpack\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrust_model.ot\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtf_model.h5\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     67\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m colbert_linear = torch.nn.Linear(\n\u001b[32m     75\u001b[39m     in_features=model.config.hidden_size,\n\u001b[32m     76\u001b[39m     out_features=model.config.hidden_size \u001b[38;5;28;01mif\u001b[39;00m colbert_dim <= \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m colbert_dim\n\u001b[32m     77\u001b[39m )\n\u001b[32m     78\u001b[39m sparse_linear = torch.nn.Linear(\n\u001b[32m     79\u001b[39m     in_features=model.config.hidden_size,\n\u001b[32m     80\u001b[39m     out_features=\u001b[32m1\u001b[39m\n\u001b[32m     81\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/modeling_utils.py:5051\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5041\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5042\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5044\u001b[39m     (\n\u001b[32m   5045\u001b[39m         model,\n\u001b[32m   5046\u001b[39m         missing_keys,\n\u001b[32m   5047\u001b[39m         unexpected_keys,\n\u001b[32m   5048\u001b[39m         mismatched_keys,\n\u001b[32m   5049\u001b[39m         offload_index,\n\u001b[32m   5050\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5051\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5065\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5066\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5067\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5068\u001b[39m model.tie_weights()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/modeling_utils.py:5319\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5316\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   5317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5318\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m5319\u001b[39m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m.keys()\n\u001b[32m   5320\u001b[39m     )\n\u001b[32m   5322\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   5323\u001b[39m prefix = model.base_model_prefix\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/modeling_utils.py:508\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch_env/lib/python3.11/site-packages/transformers/utils/import_utils.py:1647\u001b[39m, in \u001b[36mcheck_torch_load_is_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1645\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_torch_load_is_safe\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1646\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[33m\"\u001b[39m\u001b[33m2.6\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1648\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1649\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1650\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen loading files with safetensors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1651\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1652\u001b[39m         )\n",
            "\u001b[31mValueError\u001b[39m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
          ]
        }
      ],
      "source": [
        "import os, torch, numpy as np\n",
        "from tqdm import trange\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "\n",
        "# 🤫 토크나이저 멀티프로세스 경고 억제용 환경변수.\n",
        "# 병렬 토크나이징으로 인한 경고/출력 섞임을 피하고 싶을 때 False로 둡니다.\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# ⚙️ 사용 가능한 CUDA(GPU)가 있는지 확인\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# 장치 선택: GPU가 있으면 \"cuda:0\", 없으면 \"cpu\"\n",
        "device = \"cuda:0\" if use_cuda else \"cpu\"\n",
        "\n",
        "# FP16(half precision) 사용 여부: 보통 GPU가 있을 때만 켭니다.\n",
        "# - 장점: 메모리 절약, 속도 향상 가능\n",
        "# - 단점: 아주 드문 경우 수치 정밀도 이슈\n",
        "use_fp16 = use_cuda\n",
        "\n",
        "# 🧠 BGE-M3 임베딩 모델 로드\n",
        "# - \"BAAI/bge-m3\" 허깅페이스 허브에서 받아옵니다(최초 1회 캐시 후 재사용).\n",
        "# - use_fp16: 위에서 결정한 half precision 사용\n",
        "# - devices: 어떤 디바이스에 올릴지 지정 (예: \"cuda:0\" 또는 \"cpu\")\n",
        "# bge = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=use_fp16, devices=device)\n",
        "bge = BGEM3FlagModel(\n",
        "    \"BAAI/bge-m3\",\n",
        "    use_fp16=use_fp16,\n",
        "    devices=device,\n",
        "    use_safetensors=True\n",
        ")\n",
        "\n",
        "# ✂️ 최대 토큰 길이: 512~1024 권장(길수록 더 긴 문맥을 커버하지만 속도/VRAM 증가)\n",
        "MAX_LEN = 1024\n",
        "\n",
        "# 📦 배치 크기: 16~32부터 시작해서 VRAM/메모리에 맞춰 조정\n",
        "BATCH   = 32\n",
        "\n",
        "# 모든 문서의 dense 임베딩을 담아둘 리스트 (배치별로 쌓았다가 마지막에 합칩니다)\n",
        "dense_vecs = []\n",
        "\n",
        "# 🔒 추론 모드: 자동 미분 OFF → 메모리 절약 & 속도 향상\n",
        "with torch.inference_mode():\n",
        "    # trange: 진행상황(progress bar)을 보여주는 range\n",
        "    for i in trange(0, len(cleaned_texts), BATCH):\n",
        "        # 현재 배치 슬라이싱 (✅ 반드시 list[str] 형태여야 합니다)\n",
        "        batch = cleaned_texts[i:i+BATCH]\n",
        "\n",
        "        # ✅ 임베딩 추출\n",
        "        # - batch_size: 내부 처리 배치 크기 (보통 BATCH와 동일하게 둠)\n",
        "        # - max_length: 토큰 최대 길이(초과분은 모델 토크나이저에서 잘립니다)\n",
        "        # - return_dense: dense 임베딩(ANN, 벡터DB용) 반환\n",
        "        # - return_sparse: BM25 유사한 sparse(토큰 기반) 벡터 반환 여부 (여기선 미사용)\n",
        "        # - return_colbert_vecs: ColBERT 스타일 토큰 단위 벡터 반환 여부 (여기선 미사용)\n",
        "        out = bge.encode(\n",
        "            batch,\n",
        "            batch_size=BATCH,\n",
        "            max_length=MAX_LEN,\n",
        "            return_dense=True,\n",
        "            return_sparse=False,\n",
        "            return_colbert_vecs=False,\n",
        "        )\n",
        "\n",
        "        # 모델 출력에서 dense 임베딩(Numpy ndarray)을 꺼냅니다. shape: (batch, dim)\n",
        "        dense = out[\"dense_vecs\"]\n",
        "\n",
        "        # 🔄 L2 정규화: 각 벡터를 단위벡터로(normalize) 만들어 코사인 유사도 계산을 안정화\n",
        "        # - np.linalg.norm(..., axis=1, keepdims=True): 각 행(문서)별 L2 노름\n",
        "        # - 분모가 0인(edge) 경우가 드물지만 있을 수 있으니, 필요시 eps를 더해 방어코드 추가 가능\n",
        "        dense = dense / np.linalg.norm(dense, axis=1, keepdims=True)\n",
        "\n",
        "        # 배치 결과를 리스트에 쌓아둠\n",
        "        dense_vecs.append(dense)\n",
        "\n",
        "# 🔧 여러 배치로 쌓인 배열들을 한 번에 세로로 이어붙이기\n",
        "# - 결과 shape: (num_texts, dim)\n",
        "dense_vecs = np.vstack(dense_vecs).astype(\"float32\")  # 벡터DB 호환/메모리 절약 위해 float32 캐스팅\n",
        "\n",
        "# 벡터 차원(dimension) 확인\n",
        "dense_dim = dense_vecs.shape[1]\n",
        "\n",
        "# 전체 개수와 차원 출력 (예: (N, 1024) 1024)\n",
        "print(dense_vecs.shape, dense_dim)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Pinecone 서버리스 인덱스 생성 & 업서트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "da02ff74",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dense_dim' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m pc = Pinecone(api_key=PINECONE_API_KEY)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m INDEX_NAME \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pc.list_indexes().names():\n\u001b[32m      6\u001b[39m     pc.create_index(\n\u001b[32m      7\u001b[39m         name=INDEX_NAME,\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         dimension=\u001b[38;5;28mint\u001b[39m(\u001b[43mdense_dim\u001b[49m),\n\u001b[32m      9\u001b[39m         metric=METRIC,\n\u001b[32m     10\u001b[39m         spec=ServerlessSpec(cloud=PC_CLOUD, region=PC_REGION)\n\u001b[32m     11\u001b[39m     )\n\u001b[32m     13\u001b[39m index = pc.Index(INDEX_NAME)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(index.describe_index_stats())\n",
            "\u001b[31mNameError\u001b[39m: name 'dense_dim' is not defined"
          ]
        }
      ],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=int(dense_dim),\n",
        "        metric=METRIC,\n",
        "        spec=ServerlessSpec(cloud=PC_CLOUD, region=PC_REGION)\n",
        "    )\n",
        "\n",
        "index = pc.Index(INDEX_NAME)\n",
        "print(index.describe_index_stats())\n",
        "\n",
        "\n",
        "batch = []\n",
        "for i in trange(len(corpus_df)):\n",
        "    _id = corpus_df.iloc[i][\"doc_id\"]\n",
        "    meta = {\"title\": ID2TITLE[_id], \"lang\":\"ko\", \"source\":\"miracl-ko-wiki\"}\n",
        "    batch.append({\n",
        "        \"id\": _id,\n",
        "        \"values\": dense_vecs[i].tolist(),\n",
        "        \"sparse_values\": sparse_vectors[i],\n",
        "        \"metadata\": meta\n",
        "    })\n",
        "    if len(batch) >= 200:\n",
        "        index.upsert(vectors=batch)\n",
        "        batch = []\n",
        "if batch:\n",
        "    index.upsert(vectors=batch)\n",
        "\n",
        "print(\"Upsert complete.\")\n",
        "print(index.describe_index_stats())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) 검색 함수: Dense / Sparse / Weighted(α) / RRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "from collections import defaultdict\n",
        "\n",
        "def encode_query_dense(q: str):\n",
        "    return bge.encode([q], max_length=8192)[\"dense_vecs\"][0].astype(\"float32\")\n",
        "\n",
        "def encode_query_sparse(q: str):\n",
        "    return bm25.encode_queries([q])[0]\n",
        "\n",
        "def dense_only_search(query: str, top_k=20):\n",
        "    dv = encode_query_dense(query).tolist()\n",
        "    res = index.query(vector=dv, top_k=top_k, include_metadata=True)\n",
        "    return res.matches\n",
        "\n",
        "def sparse_only_search(query: str, top_k=20):\n",
        "    sv = encode_query_sparse(query)\n",
        "    res = index.query(sparse_vector=sv, top_k=top_k, include_metadata=True)\n",
        "    return res.matches\n",
        "\n",
        "def hybrid_weighted_search(query: str, top_k=20, alpha=ALPHA):\n",
        "    dv = encode_query_dense(query)\n",
        "    sv = encode_query_sparse(query)\n",
        "    dv = (dv * alpha).tolist()\n",
        "    sv_scaled = {\"indices\": sv[\"indices\"], \"values\": [v*(1.0-alpha) for v in sv[\"values\"]]}\n",
        "    res = index.query(vector=dv, sparse_vector=sv_scaled, top_k=top_k, include_metadata=True)\n",
        "    return res.matches\n",
        "\n",
        "def rrf_fusion(query: str, top_k=20, per_list_k=50, k_const=60):\n",
        "    dres = dense_only_search(query, top_k=per_list_k)\n",
        "    sres = sparse_only_search(query, top_k=per_list_k)\n",
        "    meta = {}\n",
        "    ranks = defaultdict(list)\n",
        "    for rlist in [dres, sres]:\n",
        "        for rank, m in enumerate(rlist, start=1):\n",
        "            meta[m.id] = m.metadata\n",
        "            ranks[m.id].append(rank)\n",
        "    scores = []\n",
        "    for _id, rks in ranks.items():\n",
        "        sc = sum(1.0 / (k_const + rk) for rk in rks)\n",
        "        scores.append((_id, sc))\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [{\"id\": _id, \"score\": sc, \"metadata\": meta.get(_id, {})} for _id, sc in scores[:top_k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Re-rank (Cross-Encoder): BGE Reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from FlagEmbedding import FlagReranker\n",
        "\n",
        "reranker = FlagReranker(\"BAAI/bge-reranker-v2-m3\", use_fp16=use_fp16)\n",
        "\n",
        "def rerank_ce(query: str, candidates: List[Dict], top_k=K):\n",
        "    pairs, id2meta = [], {}\n",
        "    for m in candidates:\n",
        "        if hasattr(m, \"id\"):\n",
        "            _id, meta = m.id, m.metadata\n",
        "        else:\n",
        "            _id, meta = m[\"id\"], m.get(\"metadata\", {})\n",
        "        title = meta.get(\"title\",\"\")\n",
        "        pairs.append([query, title])\n",
        "        id2meta[_id] = meta\n",
        "    scores = reranker.compute_score(pairs)\n",
        "    items = [{\"id\": _id, \"score\": float(sc), \"metadata\": id2meta[_id]} for (_id, sc) in zip(id2meta.keys(), scores)]\n",
        "    items.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    return items[:top_k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) 생성(Answer): Ollama + 근거 컨텍스트 주입"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_context(doc_ids: List[str], max_chars: int = 3000) -> str:\n",
        "    parts, total = [], 0\n",
        "    for did in doc_ids:\n",
        "        t = ID2TITLE.get(did, \"\")\n",
        "        x = ID2TEXT.get(did, \"\")\n",
        "        snippet = (x[:700] + \"...\") if len(x) > 700 else x\n",
        "        block = f\"[{did}] {t}\\n{snippet}\"\n",
        "        if total + len(block) > max_chars:\n",
        "            break\n",
        "        parts.append(block)\n",
        "        total += len(block)\n",
        "    return \"\\n\\n\".join(parts)\n",
        "\n",
        "def answer_with_ollama(query: str, topk_items: List[Dict], model: str = None):\n",
        "    model = model or os.getenv(\"OLLAMA_MODEL\", \"llama3.1:8b-instruct\")\n",
        "    doc_ids = [m[\"id\"] if isinstance(m, dict) else m.id for m in topk_items]\n",
        "    ctx = collect_context(doc_ids, max_chars=3000)\n",
        "\n",
        "    system = (\n",
        "        \"당신은 한국어 RAG 어시스턴트입니다. 아래 '근거 컨텍스트'에 포함된 내용만 사용하여 간결하고 정확하게 답하세요. \"\n",
        "        \"확실하지 않으면 모른다고 말하고 추가 정보를 요청하세요. \"\n",
        "        \"필요하면 문서 ID로 각 근거를 표기하세요.\"\n",
        "    )\n",
        "    user = f\"질문: {query}\\n\\n[근거 컨텍스트]\\n{ctx}\"\n",
        "    messages = [{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}]\n",
        "    text, dt = chat_ollama(model=model, messages=messages, stream=False)\n",
        "    return text, dt, ctx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) 데모: 질의 → Hybrid+CE → Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo_queries = [q for q in queries_ds[\"query\"][:10] if len(q) > 5][:3]\n",
        "\n",
        "for q in demo_queries:\n",
        "    print(\"=\"*100)\n",
        "    print(\"질문:\", q)\n",
        "    cand = hybrid_weighted_search(q, top_k=N, alpha=ALPHA)\n",
        "    topk = rerank_ce(q, cand, top_k=K)\n",
        "    try:\n",
        "        answer, dt, ctx = answer_with_ollama(q, topk)\n",
        "        print(\"[생성 소요]\", f\"{dt:.2f}s\")\n",
        "        print(\"[답변]\\n\", answer)\n",
        "        print(\"\\n[근거 컨텍스트]\\n\", ctx[:1000], \"...\")\n",
        "    except Exception as e:\n",
        "        print(\"Ollama 호출 실패:\", e)\n",
        "        print(\"→ OLLAMA_HOST 설정 및 서버 실행 여부를 확인하세요.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2df3f7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
