{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215b7785",
   "metadata": {},
   "source": [
    "# Assignment 02 - Part 2: Pinecone Indexing & Embeddings\n",
    "\n",
    "## 목표\n",
    "- BGE-M3 임베딩 모델 로드\n",
    "- Pinecone 인덱스 생성\n",
    "- 청크 데이터 임베딩 및 업로드\n",
    "- BM25 인덱스 생성 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ebe214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhc99/anaconda3/envs/torch_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Project initialized\n",
      "   Data: /home/dhc99/ajou-llmops-2025-2nd-semester/assignment02/datasets\n",
      "   Artifacts: /home/dhc99/ajou-llmops-2025-2nd-semester/assignment02/artifacts\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "# Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# 환경 변수\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 경로 설정\n",
    "PROJECT_ROOT = Path('/home/dhc99/ajou-llmops-2025-2nd-semester/assignment02')\n",
    "DATA_DIR = PROJECT_ROOT / 'datasets'\n",
    "CONFIG_DIR = PROJECT_ROOT / 'configs'\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"✅ Project initialized\")\n",
    "print(f\"   Data: {DATA_DIR}\")\n",
    "print(f\"   Artifacts: {ARTIFACTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa11c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Configuration:\n",
      "  - Embedding Model: BAAI/bge-m3\n",
      "  - Pinecone Index: rag-assignment-korquad\n",
      "  - Vector Dimension: 1024\n",
      "  - Metric: cosine\n"
     ]
    }
   ],
   "source": [
    "# 설정 로드\n",
    "with open(CONFIG_DIR / 'models.yaml', 'r', encoding='utf-8') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"📋 Configuration:\")\n",
    "print(f\"  - Embedding Model: {config['embedding']['model_name']}\")\n",
    "print(f\"  - Pinecone Index: {config['pinecone']['index_name']}\")\n",
    "print(f\"  - Vector Dimension: {config['pinecone']['dimension']}\")\n",
    "print(f\"  - Metric: {config['pinecone']['metric']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef668e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 641 chunks\n",
      "\n",
      "📊 Sample chunk:\n",
      "chunk_id                                  6566495-0-0_chunk_0\n",
      "text        1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로...\n",
      "title                                                 파우스트_서곡\n",
      "doc_id                                            6566495-0-0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 청크 데이터 로드\n",
    "chunks_df = pd.read_parquet(DATA_DIR / 'corpus_chunks.parquet')\n",
    "print(f\"✅ Loaded {len(chunks_df):,} chunks\")\n",
    "print(f\"\\n📊 Sample chunk:\")\n",
    "print(chunks_df.iloc[0][['chunk_id', 'text', 'title', 'doc_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc733a7",
   "metadata": {},
   "source": [
    "## 1. 임베딩 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c1e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading embedding model...\n",
      "   Trying with safetensors...\n",
      "   ⚠️  Safetensors failed: BAAI/bge-m3 does not appear to have a file named model.safetensors or model.safetensors.index.json and thus cannot be loaded with `safetensors`. Please make sure that the model has been saved with `safe_serialization=True` or do not set `use_safetensors=True`.\n",
      "   Trying alternative model: intfloat/multilingual-e5-large...\n",
      "   ✅ Loaded multilingual-e5-large!\n",
      "\n",
      "✅ Model loaded: intfloat/multilingual-e5-large\n",
      "   Max sequence length: 512\n",
      "   Embedding dimension: 1024\n",
      "\n",
      "✅ Test embedding shape: (1024,)\n",
      "   Dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "# BGE-M3 모델 로드 (safetensors 사용)\n",
    "print(\"📥 Loading embedding model...\")\n",
    "\n",
    "# 방법 1: safetensors 사용 (가장 안전)\n",
    "try:\n",
    "    print(\"   Trying with safetensors...\")\n",
    "    embedding_model = SentenceTransformer(\n",
    "        config['embedding']['model_name'],\n",
    "        device='cuda' if config['embedding']['device'] == 'cuda' else 'cpu',\n",
    "        model_kwargs={'use_safetensors': True}  # safetensors 강제 사용\n",
    "    )\n",
    "    print(\"   ✅ Loaded with safetensors!\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️  Safetensors failed: {e}\")\n",
    "    \n",
    "    # 방법 2: 대체 모델 사용 (multilingual-e5)\n",
    "    try:\n",
    "        print(\"   Trying alternative model: intfloat/multilingual-e5-large...\")\n",
    "        embedding_model = SentenceTransformer(\n",
    "            \"intfloat/multilingual-e5-large\",\n",
    "            device='cuda' if config['embedding']['device'] == 'cuda' else 'cpu'\n",
    "        )\n",
    "        print(\"   ✅ Loaded multilingual-e5-large!\")\n",
    "        # config 업데이트\n",
    "        config['embedding']['model_name'] = \"intfloat/multilingual-e5-large\"\n",
    "        config['pinecone']['dimension'] = 1024  # e5-large dimension\n",
    "    except Exception as e2:\n",
    "        print(f\"   ⚠️  Alternative model failed: {e2}\")\n",
    "        \n",
    "        # 방법 3: 더 작은 모델 (all-MiniLM-L6-v2)\n",
    "        print(\"   Trying smaller model: paraphrase-multilingual-MiniLM-L12-v2...\")\n",
    "        embedding_model = SentenceTransformer(\n",
    "            \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "            device='cuda' if config['embedding']['device'] == 'cuda' else 'cpu'\n",
    "        )\n",
    "        print(\"   ✅ Loaded paraphrase-multilingual-MiniLM-L12-v2!\")\n",
    "        # config 업데이트\n",
    "        config['embedding']['model_name'] = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        config['pinecone']['dimension'] = 384  # MiniLM dimension\n",
    "\n",
    "print(f\"\\n✅ Model loaded: {config['embedding']['model_name']}\")\n",
    "print(f\"   Max sequence length: {embedding_model.max_seq_length}\")\n",
    "print(f\"   Embedding dimension: {config['pinecone']['dimension']}\")\n",
    "\n",
    "# 테스트 임베딩\n",
    "test_text = \"이것은 테스트 문장입니다.\"\n",
    "test_embedding = embedding_model.encode(test_text, normalize_embeddings=config['embedding']['normalize'])\n",
    "print(f\"\\n✅ Test embedding shape: {test_embedding.shape}\")\n",
    "print(f\"   Dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0517f",
   "metadata": {},
   "source": [
    "## 2. Pinecone 초기화 및 인덱스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3158a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Existing indexes: ['rag-korquad-demo']\n",
      "\n",
      "🔧 Creating index 'rag-assignment-korquad'...\n",
      "✅ Index 'rag-assignment-korquad' created\n",
      "✅ Index is ready!\n",
      "\n",
      "📊 Index stats:\n",
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# Pinecone 초기화\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"❌ PINECONE_API_KEY not found in environment variables\")\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# 인덱스 이름\n",
    "index_name = config['pinecone']['index_name']\n",
    "\n",
    "# 기존 인덱스 확인\n",
    "existing_indexes = pc.list_indexes().names()\n",
    "print(f\"📋 Existing indexes: {existing_indexes}\")\n",
    "\n",
    "# 인덱스가 이미 존재하면 삭제 (선택적)\n",
    "if index_name in existing_indexes:\n",
    "    print(f\"⚠️  Index '{index_name}' already exists.\")\n",
    "    response = input(\"Delete and recreate? (yes/no): \")\n",
    "    if response.lower() == 'yes':\n",
    "        pc.delete_index(index_name)\n",
    "        print(f\"🗑️  Deleted index '{index_name}'\")\n",
    "        time.sleep(5)  # Wait for deletion\n",
    "\n",
    "# 새 인덱스 생성\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f\"\\n🔧 Creating index '{index_name}'...\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=config['pinecone']['dimension'],\n",
    "        metric=config['pinecone']['metric'],\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=config['pinecone']['cloud'],\n",
    "            region=config['pinecone']['region']\n",
    "        )\n",
    "    )\n",
    "    print(f\"✅ Index '{index_name}' created\")\n",
    "    \n",
    "    # Wait for index to be ready\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        print(\"   Waiting for index to be ready...\")\n",
    "        time.sleep(5)\n",
    "    print(\"✅ Index is ready!\")\n",
    "\n",
    "# 인덱스 연결\n",
    "index = pc.Index(index_name)\n",
    "print(f\"\\n📊 Index stats:\")\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a7053a",
   "metadata": {},
   "source": [
    "## 3. 임베딩 생성 및 Pinecone 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1604fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Creating embeddings for 641 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 21/21 [00:04<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings created: (641, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_embeddings_batch(texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    배치 단위로 임베딩 생성\n",
    "    \n",
    "    Args:\n",
    "        texts: 텍스트 리스트\n",
    "        batch_size: 배치 크기\n",
    "    \n",
    "    Returns:\n",
    "        임베딩 배열\n",
    "    \"\"\"\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        normalize_embeddings=config['embedding']['normalize'],\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# 전체 청크 임베딩 생성\n",
    "print(f\"\\n🔄 Creating embeddings for {len(chunks_df):,} chunks...\")\n",
    "all_texts = chunks_df['text'].tolist()\n",
    "all_embeddings = create_embeddings_batch(\n",
    "    all_texts,\n",
    "    batch_size=config['embedding']['batch_size']\n",
    ")\n",
    "\n",
    "print(f\"✅ Embeddings created: {all_embeddings.shape}\")\n",
    "\n",
    "# DataFrame에 임베딩 추가\n",
    "chunks_df['embedding'] = list(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a42d157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📤 Uploading vectors to Pinecone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to Pinecone: 100%|██████████| 7/7 [00:22<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded 641 vectors to Pinecone\n",
      "\n",
      "📊 Index stats after upload:\n",
      "   Total vectors: 641\n",
      "   Dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "def upsert_to_pinecone(index, chunks_df: pd.DataFrame, batch_size: int = 100):\n",
    "    \"\"\"\n",
    "    Pinecone에 벡터 업로드\n",
    "    \n",
    "    Args:\n",
    "        index: Pinecone 인덱스\n",
    "        chunks_df: 청크 데이터프레임\n",
    "        batch_size: 업로드 배치 크기\n",
    "    \"\"\"\n",
    "    total_chunks = len(chunks_df)\n",
    "    \n",
    "    for i in tqdm(range(0, total_chunks, batch_size), desc=\"Uploading to Pinecone\"):\n",
    "        batch_df = chunks_df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Pinecone 벡터 포맷 생성\n",
    "        vectors = []\n",
    "        for _, row in batch_df.iterrows():\n",
    "            metadata = {\n",
    "                'text': row['text'][:1000],  # Pinecone metadata limit\n",
    "                'doc_id': str(row['doc_id']),\n",
    "                'title': str(row['title'])[:100],\n",
    "                'language': row['language'],\n",
    "                'source': row['source'],\n",
    "                'chunk_index': int(row['chunk_index']),\n",
    "                'section': str(row['section'])\n",
    "            }\n",
    "            \n",
    "            vectors.append({\n",
    "                'id': str(row['chunk_id']),\n",
    "                'values': row['embedding'].tolist(),\n",
    "                'metadata': metadata\n",
    "            })\n",
    "        \n",
    "        # 업로드\n",
    "        index.upsert(vectors=vectors)\n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "    \n",
    "    print(f\"✅ Uploaded {total_chunks:,} vectors to Pinecone\")\n",
    "\n",
    "\n",
    "# Pinecone에 업로드\n",
    "print(\"\\n📤 Uploading vectors to Pinecone...\")\n",
    "upsert_to_pinecone(index, chunks_df, batch_size=100)\n",
    "\n",
    "# 인덱스 통계 확인\n",
    "time.sleep(5)  # Wait for stats to update\n",
    "stats = index.describe_index_stats()\n",
    "print(f\"\\n📊 Index stats after upload:\")\n",
    "print(f\"   Total vectors: {stats['total_vector_count']:,}\")\n",
    "print(f\"   Dimension: {stats['dimension']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d3985",
   "metadata": {},
   "source": [
    "## 4. BM25 인덱스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "893c7f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creating BM25 index with improved Korean tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 641/641 [00:00<00:00, 5007.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BM25 index created with 641 documents\n",
      "✅ BM25 index saved to: /home/dhc99/ajou-llmops-2025-2nd-semester/assignment02/artifacts/bm25_index.pkl\n",
      "   Size: 3.87 MB\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 토크나이제이션 (개선된 한국어 지원)\n",
    "import re\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    한국어 친화적 토크나이저\n",
    "    - 공백 분리\n",
    "    - 특수문자 제거\n",
    "    - N-gram 생성 (2-3 글자 단위)\n",
    "    \"\"\"\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 특수문자 제거 (한글, 영문, 숫자만 유지)\n",
    "    text = re.sub(r'[^가-힣a-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    # 공백 기반 토큰\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 추가: 2-3 글자 n-gram (한국어 검색 향상)\n",
    "    ngrams = []\n",
    "    for token in tokens:\n",
    "        if len(token) >= 2:\n",
    "            # 2-gram\n",
    "            for i in range(len(token) - 1):\n",
    "                ngrams.append(token[i:i+2])\n",
    "            # 3-gram\n",
    "            if len(token) >= 3:\n",
    "                for i in range(len(token) - 2):\n",
    "                    ngrams.append(token[i:i+3])\n",
    "    \n",
    "    # 토큰 + n-gram 결합\n",
    "    all_tokens = tokens + ngrams\n",
    "    \n",
    "    # 중복 제거 및 필터링 (너무 짧은 토큰 제외)\n",
    "    return [t for t in all_tokens if len(t) >= 1]\n",
    "\n",
    "print(\"🔄 Creating BM25 index with improved Korean tokenizer...\")\n",
    "\n",
    "# 모든 청크 토크나이즈\n",
    "tokenized_corpus = [tokenize(text) for text in tqdm(chunks_df['text'], desc=\"Tokenizing\")]\n",
    "\n",
    "# BM25 인덱스 생성 (파라미터 조정)\n",
    "bm25 = BM25Okapi(\n",
    "    tokenized_corpus,\n",
    "    k1=1.5,  # Term frequency saturation\n",
    "    b=0.75   # Length normalization\n",
    ")\n",
    "\n",
    "print(f\"✅ BM25 index created with {len(tokenized_corpus):,} documents\")\n",
    "\n",
    "# BM25 인덱스 저장\n",
    "bm25_file = ARTIFACTS_DIR / 'bm25_index.pkl'\n",
    "with open(bm25_file, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'bm25': bm25,\n",
    "        'tokenized_corpus': tokenized_corpus,\n",
    "        'chunk_ids': chunks_df['chunk_id'].tolist()\n",
    "    }, f)\n",
    "\n",
    "print(f\"✅ BM25 index saved to: {bm25_file}\")\n",
    "print(f\"   Size: {bm25_file.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4481e4f",
   "metadata": {},
   "source": [
    "## 5. 임베딩 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b5d1bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings saved to: /home/dhc99/ajou-llmops-2025-2nd-semester/assignment02/artifacts/embeddings.npy\n",
      "   Shape: (641, 1024)\n",
      "   Size: 2.50 MB\n",
      "\n",
      "✅ Chunks metadata saved to: /home/dhc99/ajou-llmops-2025-2nd-semester/assignment02/artifacts/chunks_metadata.parquet\n"
     ]
    }
   ],
   "source": [
    "# 임베딩이 포함된 청크 데이터 저장\n",
    "# (임베딩은 numpy array이므로 별도 처리)\n",
    "embeddings_file = ARTIFACTS_DIR / 'embeddings.npy'\n",
    "np.save(embeddings_file, all_embeddings)\n",
    "print(f\"✅ Embeddings saved to: {embeddings_file}\")\n",
    "print(f\"   Shape: {all_embeddings.shape}\")\n",
    "print(f\"   Size: {embeddings_file.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# 메타데이터만 저장 (임베딩 제외)\n",
    "chunks_metadata = chunks_df.drop(columns=['embedding']).copy()\n",
    "metadata_file = ARTIFACTS_DIR / 'chunks_metadata.parquet'\n",
    "chunks_metadata.to_parquet(metadata_file, index=False)\n",
    "print(f\"\\n✅ Chunks metadata saved to: {metadata_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95f94b",
   "metadata": {},
   "source": [
    "## 6. 검색 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a143417a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Test Query: 대한민국에서 최초로 출시된 스낵은?\n",
      "   Expected Answer: 새우깡\n",
      "\n",
      "📊 Dense Retrieval Results:\n",
      "================================================================================\n",
      "\n",
      "1. Score: 0.8713\n",
      "   ID: 6467462-0-1_chunk_1\n",
      "   Title: 농심그룹\n",
      "   Text: '농심'으로 바꾸고 라면시장에 본격적으로 뛰어든다. 이 과정에서 피를 나눈 두 형제는 의절했고, 결국 롯데그룹과 농심그룹으로 갈라지는 계기가 됐다. 1971년 12월, 대한민국 최초의 스낵인 '새우깡'을 출시했다. 짭짤하면서 고소하고 바삭한 식감의 새우깡은 독특한 이름과 함께 시장에서 돌풍을 일으키며 출시 3개월 만에 농심 매출을 2배 가까운 성장을 가져다...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Score: 0.8713\n",
      "   ID: 6484373-0-0_chunk_1\n",
      "   Title: 농심그룹\n",
      "   Text: '농심'으로 바꾸고 라면시장에 본격적으로 뛰어든다. 이 과정에서 피를 나눈 두 형제는 의절했고, 결국 롯데그룹과 농심그룹으로 갈라지는 계기가 됐다. 1971년 12월, 대한민국 최초의 스낵인 '새우깡'을 출시했다. 짭짤하면서 고소하고 바삭한 식감의 새우깡은 독특한 이름과 함께 시장에서 돌풍을 일으키며 출시 3개월 만에 농심 매출을 2배 가까운 성장을 가져다...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Score: 0.8713\n",
      "   ID: 6484373-0-1_chunk_1\n",
      "   Title: 농심그룹\n",
      "   Text: '농심'으로 바꾸고 라면시장에 본격적으로 뛰어든다. 이 과정에서 피를 나눈 두 형제는 의절했고, 결국 롯데그룹과 농심그룹으로 갈라지는 계기가 됐다. 1971년 12월, 대한민국 최초의 스낵인 '새우깡'을 출시했다. 짭짤하면서 고소하고 바삭한 식감의 새우깡은 독특한 이름과 함께 시장에서 돌풍을 일으키며 출시 3개월 만에 농심 매출을 2배 가까운 성장을 가져다...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. Score: 0.8713\n",
      "   ID: 6484373-0-2_chunk_1\n",
      "   Title: 농심그룹\n",
      "   Text: '농심'으로 바꾸고 라면시장에 본격적으로 뛰어든다. 이 과정에서 피를 나눈 두 형제는 의절했고, 결국 롯데그룹과 농심그룹으로 갈라지는 계기가 됐다. 1971년 12월, 대한민국 최초의 스낵인 '새우깡'을 출시했다. 짭짤하면서 고소하고 바삭한 식감의 새우깡은 독특한 이름과 함께 시장에서 돌풍을 일으키며 출시 3개월 만에 농심 매출을 2배 가까운 성장을 가져다...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. Score: 0.8713\n",
      "   ID: 6467462-0-0_chunk_1\n",
      "   Title: 농심그룹\n",
      "   Text: '농심'으로 바꾸고 라면시장에 본격적으로 뛰어든다. 이 과정에서 피를 나눈 두 형제는 의절했고, 결국 롯데그룹과 농심그룹으로 갈라지는 계기가 됐다. 1971년 12월, 대한민국 최초의 스낵인 '새우깡'을 출시했다. 짭짤하면서 고소하고 바삭한 식감의 새우깡은 독특한 이름과 함께 시장에서 돌풍을 일으키며 출시 3개월 만에 농심 매출을 2배 가까운 성장을 가져다...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Dense 검색 테스트 (데이터셋에 실제로 있는 질문 사용)\n",
    "test_query = \"대한민국에서 최초로 출시된 스낵은?\"\n",
    "print(f\"🔍 Test Query: {test_query}\")\n",
    "print(f\"   Expected Answer: 새우깡\\n\")\n",
    "\n",
    "# 쿼리 임베딩\n",
    "query_embedding = embedding_model.encode(\n",
    "    test_query,\n",
    "    normalize_embeddings=config['embedding']['normalize']\n",
    ")\n",
    "\n",
    "# Pinecone 검색\n",
    "results = index.query(\n",
    "    vector=query_embedding.tolist(),\n",
    "    top_k=5,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(\"📊 Dense Retrieval Results:\")\n",
    "print(\"=\"*80)\n",
    "for i, match in enumerate(results['matches'], 1):\n",
    "    print(f\"\\n{i}. Score: {match['score']:.4f}\")\n",
    "    print(f\"   ID: {match['id']}\")\n",
    "    print(f\"   Title: {match['metadata'].get('title', 'N/A')}\")\n",
    "    print(f\"   Text: {match['metadata']['text'][:200]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f85c28a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 BM25 Test Query: 대한민국에서 최초로 출시된 스낵은?\n",
      "   Expected Answer: 새우깡\n",
      "\n",
      "🔤 Tokenized Query: ['대한민국에서', '최초로', '출시된', '스낵은', '대한', '한민', '민국', '국에', '에서', '대한민', '한민국', '민국에', '국에서', '최초', '초로', '최초로', '출시', '시된', '출시된', '스낵']...\n",
      "\n",
      "📊 BM25 Retrieval Results:\n",
      "================================================================================\n",
      "\n",
      "1. Score: 28.4991\n",
      "   ID: 6484386-0-2_chunk_1\n",
      "   Title: 농심그룹\n",
      "   Text: '농심'으로 바꾸고 라면시장에 본격적으로 뛰어든다. 이 과정에서 피를 나눈 두 형제는 의절했고, 결국 롯데그룹과 농심그룹으로 갈라지는 계기가 됐다. 1971년 12월, 대한민국 최초의 스낵인 '새우깡'을 출시했다. 짭짤하면서 고소하고 바삭한 식감의 새우깡은 독특한 이름과 함께 시장에서 돌풍을 일으키며 출시 3개월 만에 농심 매출을 2배 가까운 성장을 가져다...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Score: 28.4991\n",
      "   ID: 6484386-0-1_chunk_1\n",
      "   Title: 농심그룹\n",
      "   Text: '농심'으로 바꾸고 라면시장에 본격적으로 뛰어든다. 이 과정에서 피를 나눈 두 형제는 의절했고, 결국 롯데그룹과 농심그룹으로 갈라지는 계기가 됐다. 1971년 12월, 대한민국 최초의 스낵인 '새우깡'을 출시했다. 짭짤하면서 고소하고 바삭한 식감의 새우깡은 독특한 이름과 함께 시장에서 돌풍을 일으키며 출시 3개월 만에 농심 매출을 2배 가까운 성장을 가져다...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Score: 28.4991\n",
      "   ID: 6484386-0-0_chunk_1\n",
      "   Title: 농심그룹\n",
      "   Text: '농심'으로 바꾸고 라면시장에 본격적으로 뛰어든다. 이 과정에서 피를 나눈 두 형제는 의절했고, 결국 롯데그룹과 농심그룹으로 갈라지는 계기가 됐다. 1971년 12월, 대한민국 최초의 스낵인 '새우깡'을 출시했다. 짭짤하면서 고소하고 바삭한 식감의 새우깡은 독특한 이름과 함께 시장에서 돌풍을 일으키며 출시 3개월 만에 농심 매출을 2배 가까운 성장을 가져다...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "4. Score: 28.4991\n",
      "   ID: 6484373-0-1_chunk_1\n",
      "   Title: 농심그룹\n",
      "   Text: '농심'으로 바꾸고 라면시장에 본격적으로 뛰어든다. 이 과정에서 피를 나눈 두 형제는 의절했고, 결국 롯데그룹과 농심그룹으로 갈라지는 계기가 됐다. 1971년 12월, 대한민국 최초의 스낵인 '새우깡'을 출시했다. 짭짤하면서 고소하고 바삭한 식감의 새우깡은 독특한 이름과 함께 시장에서 돌풍을 일으키며 출시 3개월 만에 농심 매출을 2배 가까운 성장을 가져다...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "5. Score: 28.4991\n",
      "   ID: 6467462-0-1_chunk_1\n",
      "   Title: 농심그룹\n",
      "   Text: '농심'으로 바꾸고 라면시장에 본격적으로 뛰어든다. 이 과정에서 피를 나눈 두 형제는 의절했고, 결국 롯데그룹과 농심그룹으로 갈라지는 계기가 됐다. 1971년 12월, 대한민국 최초의 스낵인 '새우깡'을 출시했다. 짭짤하면서 고소하고 바삭한 식감의 새우깡은 독특한 이름과 함께 시장에서 돌풍을 일으키며 출시 3개월 만에 농심 매출을 2배 가까운 성장을 가져다...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# BM25 검색 테스트\n",
    "print(f\"\\n🔍 BM25 Test Query: {test_query}\")\n",
    "print(f\"   Expected Answer: 새우깡\\n\")\n",
    "\n",
    "tokenized_query = tokenize(test_query)\n",
    "print(f\"🔤 Tokenized Query: {tokenized_query[:20]}...\")  # 처음 20개만 표시\n",
    "\n",
    "bm25_scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "# Top-K 결과\n",
    "top_k_indices = np.argsort(bm25_scores)[::-1][:5]\n",
    "\n",
    "print(\"\\n📊 BM25 Retrieval Results:\")\n",
    "print(\"=\"*80)\n",
    "for i, idx in enumerate(top_k_indices, 1):\n",
    "    chunk = chunks_df.iloc[idx]\n",
    "    print(f\"\\n{i}. Score: {bm25_scores[idx]:.4f}\")\n",
    "    print(f\"   ID: {chunk['chunk_id']}\")\n",
    "    print(f\"   Title: {chunk['title']}\")\n",
    "    print(f\"   Text: {chunk['text'][:200]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac8384d",
   "metadata": {},
   "source": [
    "## 7. Pinecone 프로젝트 공유 안내\n",
    "\n",
    "**중요: Pinecone 프로젝트에 강사를 초대해야 합니다!**\n",
    "\n",
    "### 단계:\n",
    "1. Pinecone 콘솔 접속: https://app.pinecone.io/\n",
    "2. 프로젝트 설정 → Members\n",
    "3. 강사 이메일 초대 (권한: Viewer 또는 Editor)\n",
    "4. 초대 스크린샷 캡처\n",
    "5. 제출물에 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80498853",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 Indexing Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n📊 Summary:\")\n",
    "print(f\"   Pinecone Index: {index_name}\")\n",
    "print(f\"   Total Vectors: {stats['total_vector_count']:,}\")\n",
    "print(f\"   Embedding Model: {config['embedding']['model_name']}\")\n",
    "print(f\"   BM25 Index: {bm25_file}\")\n",
    "print(f\"\\n📁 Artifacts:\")\n",
    "print(f\"   - {embeddings_file}\")\n",
    "print(f\"   - {bm25_file}\")\n",
    "print(f\"   - {metadata_file}\")\n",
    "print(\"\\n⚠️  Remember to invite instructor to Pinecone project!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
